{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries\n"
      ],
      "metadata": {
        "id": "16QzIa_zmFq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy step"
      ],
      "metadata": {
        "id": "TtxZNa0da4ny"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vYGJgwVtMIrb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a05f094-c141-4886-b63e-5e74ac6a7838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "import torch\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# ssl training data (numpy arrays)\n",
        "!cp -r /content/gdrive/MyDrive/smrt-foundation/ob007.memmap/ /content/\n",
        "# downstream methylation dataset (parquet, tabular)\n",
        "!cp -r /content/gdrive/MyDrive/smrt-foundation/pacbio_standard_train_1m.parquet /content/\n",
        "!cp -r /content/gdrive/MyDrive/smrt-foundation/pacbio_standard_test_1m.parquet /content/\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cpu = torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "!pip install \"vegafusion[embed]>=1.5.0\" \"vl-convert-python>=1.6.0\"\n",
        "sys.path.append('/content/gdrive/MyDrive/smrt-foundation')\n"
      ],
      "metadata": {
        "id": "FLn4nkjnV53P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48bbcba6-8936-4184-a26b-d3f5c7a611cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vegafusion>=1.5.0 (from vegafusion[embed]>=1.5.0)\n",
            "  Downloading vegafusion-2.0.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting vl-convert-python>=1.6.0\n",
            "  Downloading vl_convert_python-1.9.0.post1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting arro3-core (from vegafusion>=1.5.0->vegafusion[embed]>=1.5.0)\n",
            "  Downloading arro3_core-0.6.5-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (363 bytes)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from vegafusion>=1.5.0->vegafusion[embed]>=1.5.0) (25.0)\n",
            "Requirement already satisfied: narwhals>=1.42 in /usr/local/lib/python3.12/dist-packages (from vegafusion>=1.5.0->vegafusion[embed]>=1.5.0) (2.15.0)\n",
            "Downloading vegafusion-2.0.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vl_convert_python-1.9.0.post1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arro3_core-0.6.5-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vl-convert-python, arro3-core, vegafusion\n",
            "Successfully installed arro3-core-0.6.5 vegafusion-2.0.3 vl-convert-python-1.9.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glimpse ssl data"
      ],
      "metadata": {
        "id": "icfit_SKmJq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is our pretraining dataset\n",
        "# take a peak at one of its numpy shards\n",
        "# note how each slice of the array is a single-stranded sample for pretraining\n",
        "# columns are organized [seq, ipd, pw, padding_mask]\n",
        "\n",
        "import numpy as np\n",
        "x = torch.tensor(np.load('ob007.memmap/shard_00002.npy')).to(device)\n",
        "mask = ~x[...,-1].bool()\n",
        "print(x.shape)\n",
        "print(x[0,0:10])"
      ],
      "metadata": {
        "id": "PUg_Q9DXk3vI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8550d479-6eeb-46e8-bd83-25b6049f3791"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16384, 4096, 4])\n",
            "tensor([[ 2.0000, -0.0258,  0.7886,  0.0000],\n",
            "        [ 0.0000,  0.5142,  1.3975,  0.0000],\n",
            "        [ 0.0000, -1.3145,  0.3120,  0.0000],\n",
            "        [ 3.0000,  1.6426, -1.4004,  0.0000],\n",
            "        [ 0.0000, -0.5776, -0.5669,  0.0000],\n",
            "        [ 2.0000,  0.8359,  0.9258,  0.0000],\n",
            "        [ 1.0000, -0.7842, -0.0809,  0.0000],\n",
            "        [ 1.0000, -1.8984, -1.6172,  0.0000],\n",
            "        [ 1.0000, -0.8994,  0.1249,  0.0000],\n",
            "        [ 3.0000, -0.0928, -0.8608,  0.0000]], device='cuda:0',\n",
            "       dtype=torch.float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first column is seq, center two are features which should have mean 0, sd 1\n",
        "# last column is mask and should be 0's and 1's (mostly 0's)\n",
        "\n",
        "x.mean(dim=(0, 1)).round(decimals=2), x.std(dim=(0,1)).round(decimals=2)"
      ],
      "metadata": {
        "id": "VCwGImgEhTQp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1afd0b5f-6386-4dd3-8f63-5d2c8241adaf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 1.1904, -0.0100,  0.0000,  0.2000], device='cuda:0',\n",
              "        dtype=torch.float16),\n",
              " tensor([1.1904, 0.8999, 0.8901, 0.3999], device='cuda:0', dtype=torch.float16))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View the distribution of nucleotides in the flattened seq column\n",
        "# note that in the natural genome it is not a uniform distribution\n",
        "# what we see here after subsetting with the mask matches expectations\n",
        "\n",
        "import altair as alt\n",
        "alt.data_transformers.enable(\"vegafusion\")\n",
        "import polars as pl\n",
        "seq = x[:, :, 0][mask].flatten()\n",
        "seq_df = pl.DataFrame({'seq':seq.to(cpu)})\n",
        "alt.Chart(seq_df).mark_bar(width=70).encode(\n",
        "    alt.X('seq:Q'),\n",
        "    y='count()'\n",
        ")"
      ],
      "metadata": {
        "id": "OLpocjf2h7Dy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "12fa6a31-512d-4dba-d314-04fd29045d49"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "  #altair-viz-ba95789a88c24b53b11db21696cb02f4.vega-embed {\n",
              "    width: 100%;\n",
              "    display: flex;\n",
              "  }\n",
              "\n",
              "  #altair-viz-ba95789a88c24b53b11db21696cb02f4.vega-embed details,\n",
              "  #altair-viz-ba95789a88c24b53b11db21696cb02f4.vega-embed details summary {\n",
              "    position: relative;\n",
              "  }\n",
              "</style>\n",
              "<div id=\"altair-viz-ba95789a88c24b53b11db21696cb02f4\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-ba95789a88c24b53b11db21696cb02f4\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-ba95789a88c24b53b11db21696cb02f4\");\n",
              "    }\n",
              "\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      let deps = [\"vega-embed\"];\n",
              "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"$schema\": \"https://vega.github.io/schema/vega/v5.json\", \"data\": [{\"name\": \"source_0\", \"values\": [{\"__count\": 12216124, \"seq\": 2.0}, {\"__count\": 14462097, \"seq\": 0.0}, {\"__count\": 14462097, \"seq\": 3.0}, {\"__count\": 12216124, \"seq\": 1.0}]}, {\"name\": \"source_0_x_domain_seq\", \"values\": [{\"min\": 0.0, \"max\": 3.0}]}, {\"name\": \"source_0_y_domain___count\", \"values\": [{\"min\": 12216124, \"max\": 14462097}]}], \"marks\": [{\"type\": \"rect\", \"name\": \"marks\", \"from\": {\"data\": \"source_0\"}, \"encode\": {\"update\": {\"xc\": {\"field\": \"seq\", \"scale\": \"x\"}, \"y\": {\"field\": \"__count\", \"scale\": \"y\"}, \"width\": {\"value\": 70}, \"y2\": {\"value\": 0, \"scale\": \"y\"}, \"fill\": {\"value\": \"#4c78a8\"}}}, \"style\": [\"bar\"]}], \"scales\": [{\"name\": \"x\", \"type\": \"linear\", \"domain\": [{\"signal\": \"(data(\\\"source_0_x_domain_seq\\\")[0] || {}).min\"}, {\"signal\": \"(data(\\\"source_0_x_domain_seq\\\")[0] || {}).max\"}], \"range\": [0, {\"signal\": \"width\"}], \"nice\": true, \"zero\": false, \"padding\": 5}, {\"name\": \"y\", \"type\": \"linear\", \"domain\": [{\"signal\": \"(data(\\\"source_0_y_domain___count\\\")[0] || {}).min\"}, {\"signal\": \"(data(\\\"source_0_y_domain___count\\\")[0] || {}).max\"}], \"range\": [{\"signal\": \"height\"}, 0], \"zero\": true, \"nice\": true}], \"axes\": [{\"scale\": \"x\", \"maxExtent\": 0, \"ticks\": false, \"gridScale\": \"y\", \"labels\": false, \"orient\": \"bottom\", \"domain\": false, \"grid\": true, \"minExtent\": 0, \"aria\": false, \"zindex\": 0, \"tickCount\": {\"signal\": \"ceil(width/40)\"}}, {\"scale\": \"y\", \"tickCount\": {\"signal\": \"ceil(height/40)\"}, \"domain\": false, \"ticks\": false, \"zindex\": 0, \"grid\": true, \"minExtent\": 0, \"maxExtent\": 0, \"aria\": false, \"gridScale\": \"x\", \"orient\": \"left\", \"labels\": false}, {\"scale\": \"x\", \"labelOverlap\": true, \"zindex\": 0, \"title\": \"seq\", \"tickCount\": {\"signal\": \"ceil(width/40)\"}, \"labelFlush\": true, \"orient\": \"bottom\", \"grid\": false}, {\"scale\": \"y\", \"orient\": \"left\", \"tickCount\": {\"signal\": \"ceil(height/40)\"}, \"title\": \"Count of Records\", \"grid\": false, \"zindex\": 0, \"labelOverlap\": true}], \"padding\": 5, \"background\": \"white\", \"width\": 300, \"style\": \"cell\", \"height\": 300}, {\"mode\": \"vega\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and here we can see the proportions of each nucleotide in tabular format\n",
        "seq = x[:, :, 0][mask.to(cpu)].flatten()\n",
        "seq_df = pl.DataFrame({'seq':seq.to(cpu)})\n",
        "seq_df[\"seq\"].value_counts(sort=True, normalize=True)"
      ],
      "metadata": {
        "id": "jZW27r0HkV8W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "0e87226e-7223-4a39-fe41-f3e801587ac8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (4, 2)\n",
              "┌─────┬────────────┐\n",
              "│ seq ┆ proportion │\n",
              "│ --- ┆ ---        │\n",
              "│ f32 ┆ f64        │\n",
              "╞═════╪════════════╡\n",
              "│ 0.0 ┆ 0.271047   │\n",
              "│ 3.0 ┆ 0.271047   │\n",
              "│ 2.0 ┆ 0.228953   │\n",
              "│ 1.0 ┆ 0.228953   │\n",
              "└─────┴────────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (4, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>seq</th><th>proportion</th></tr><tr><td>f32</td><td>f64</td></tr></thead><tbody><tr><td>0.0</td><td>0.271047</td></tr><tr><td>3.0</td><td>0.271047</td></tr><tr><td>2.0</td><td>0.228953</td></tr><tr><td>1.0</td><td>0.228953</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check that the long transformation doesn't corrupt the floats\n",
        "# if it did, we might see an excess of 0 (A)\n",
        "seq = x[:, :, 0].long()[mask.to(cpu)].flatten()\n",
        "seq_df = pl.DataFrame({'seq':seq.to(cpu)})\n",
        "seq_df[\"seq\"].value_counts(sort=True, normalize=True)"
      ],
      "metadata": {
        "id": "JpWNfSFXwdko",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "f9a048fd-9129-4446-e9eb-4eac2cd37e93"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (4, 2)\n",
              "┌─────┬────────────┐\n",
              "│ seq ┆ proportion │\n",
              "│ --- ┆ ---        │\n",
              "│ i64 ┆ f64        │\n",
              "╞═════╪════════════╡\n",
              "│ 0   ┆ 0.271047   │\n",
              "│ 3   ┆ 0.271047   │\n",
              "│ 2   ┆ 0.228953   │\n",
              "│ 1   ┆ 0.228953   │\n",
              "└─────┴────────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (4, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>seq</th><th>proportion</th></tr><tr><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>0.271047</td></tr><tr><td>3</td><td>0.271047</td></tr><tr><td>2</td><td>0.228953</td></tr><tr><td>1</td><td>0.228953</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# plot the histograms of the two features\n",
        "cont_df = pl.DataFrame({'ipd':x[...,1][mask].flatten().to(cpu),\n",
        "                        'pw':x[...,2][mask].flatten().to(cpu)})\n",
        "alt.Chart(cont_df.unpivot()).mark_bar().encode(\n",
        "      alt.X('value:Q').title('normalized zmw frames'),\n",
        "      alt.Y('count():Q').scale(type='linear').title('count'),\n",
        "  ).properties(\n",
        "      width=400,\n",
        "      height=400,\n",
        "  ).facet(\n",
        "      column='variable:N'\n",
        "  ).properties(\n",
        "      title=\"Memmap Kinetics Distributions\"\n",
        "  )"
      ],
      "metadata": {
        "id": "kUWzekMmsIjW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "448b9a00-def9-4743-ed9b-49f75423676c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "  #altair-viz-d003075242e14dfe91cee30b055f90f9.vega-embed {\n",
              "    width: 100%;\n",
              "    display: flex;\n",
              "  }\n",
              "\n",
              "  #altair-viz-d003075242e14dfe91cee30b055f90f9.vega-embed details,\n",
              "  #altair-viz-d003075242e14dfe91cee30b055f90f9.vega-embed details summary {\n",
              "    position: relative;\n",
              "  }\n",
              "</style>\n",
              "<div id=\"altair-viz-d003075242e14dfe91cee30b055f90f9\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-d003075242e14dfe91cee30b055f90f9\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-d003075242e14dfe91cee30b055f90f9\");\n",
              "    }\n",
              "\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      let deps = [\"vega-embed\"];\n",
              "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"$schema\": \"https://vega.github.io/schema/vega/v5.json\", \"data\": [{\"name\": \"source_0\", \"values\": [{\"value\": -0.026, \"variable\": \"ipd\", \"__count\": 658403}, {\"value\": 0.514, \"variable\": \"ipd\", \"__count\": 319739}, {\"value\": -1.314, \"variable\": \"ipd\", \"__count\": 752651}, {\"value\": 1.643, \"variable\": \"ipd\", \"__count\": 86385}, {\"value\": -0.578, \"variable\": \"ipd\", \"__count\": 1146301}, {\"value\": 0.836, \"variable\": \"ipd\", \"__count\": 192591}, {\"value\": -0.784, \"variable\": \"ipd\", \"__count\": 1134425}, {\"value\": -1.898, \"variable\": \"ipd\", \"__count\": 246249}, {\"value\": -0.899, \"variable\": \"ipd\", \"__count\": 902415}, {\"value\": -0.093, \"variable\": \"ipd\", \"__count\": 845351}, {\"value\": 0.038, \"variable\": \"ipd\", \"__count\": 734624}, {\"value\": -0.485, \"variable\": \"ipd\", \"__count\": 940683}, {\"value\": -1.677, \"variable\": \"ipd\", \"__count\": 466190}, {\"value\": 0.1, \"variable\": \"ipd\", \"__count\": 568551}, {\"value\": -1.025, \"variable\": \"ipd\", \"__count\": 1003266}, {\"value\": -0.237, \"variable\": \"ipd\", \"__count\": 963144}, {\"value\": -0.397, \"variable\": \"ipd\", \"__count\": 1073049}, {\"value\": 0.559, \"variable\": \"ipd\", \"__count\": 365915}, {\"value\": -0.163, \"variable\": \"ipd\", \"__count\": 756081}, {\"value\": -2.162, \"variable\": \"ipd\", \"__count\": 241847}, {\"value\": 0.421, \"variable\": \"ipd\", \"__count\": 367655}, {\"value\": 0.322, \"variable\": \"ipd\", \"__count\": 424237}, {\"value\": 0.724, \"variable\": \"ipd\", \"__count\": 285313}, {\"value\": 0.158, \"variable\": \"ipd\", \"__count\": 636035}, {\"value\": -1.484, \"variable\": \"ipd\", \"__count\": 473018}, {\"value\": -0.677, \"variable\": \"ipd\", \"__count\": 971315}, {\"value\": 0.215, \"variable\": \"ipd\", \"__count\": 490049}, {\"value\": 0.27, \"variable\": \"ipd\", \"__count\": 550953}, {\"value\": 0.469, \"variable\": \"ipd\", \"__count\": 418105}, {\"value\": 1.444, \"variable\": \"ipd\", \"__count\": 83584}, {\"value\": 1.468, \"variable\": \"ipd\", \"__count\": 63444}, {\"value\": 1.214, \"variable\": \"ipd\", \"__count\": 100980}, {\"value\": -1.162, \"variable\": \"ipd\", \"__count\": 722385}, {\"value\": 0.602, \"variable\": \"ipd\", \"__count\": 280310}, {\"value\": 0.644, \"variable\": \"ipd\", \"__count\": 321501}, {\"value\": 0.974, \"variable\": \"ipd\", \"__count\": 151994}, {\"value\": -2.898, \"variable\": \"ipd\", \"__count\": 98905}, {\"value\": -0.315, \"variable\": \"ipd\", \"__count\": 854387}, {\"value\": 0.799, \"variable\": \"ipd\", \"__count\": 252749}, {\"value\": 0.872, \"variable\": \"ipd\", \"__count\": 225336}, {\"value\": 0.906, \"variable\": \"ipd\", \"__count\": 171746}, {\"value\": 1.889, \"variable\": \"ipd\", \"__count\": 37691}, {\"value\": 1.037, \"variable\": \"ipd\", \"__count\": 136947}, {\"value\": 0.373, \"variable\": \"ipd\", \"__count\": 479043}, {\"value\": 0.684, \"variable\": \"ipd\", \"__count\": 245772}, {\"value\": 1.558, \"variable\": \"ipd\", \"__count\": 117446}, {\"value\": 1.269, \"variable\": \"ipd\", \"__count\": 91102}, {\"value\": 2.852, \"variable\": \"ipd\", \"__count\": 4462}, {\"value\": 1.157, \"variable\": \"ipd\", \"__count\": 111113}, {\"value\": -3.482, \"variable\": \"ipd\", \"__count\": 27666}, {\"value\": 1.068, \"variable\": \"ipd\", \"__count\": 162247}, {\"value\": 1.684, \"variable\": \"ipd\", \"__count\": 75190}, {\"value\": -4.48, \"variable\": \"ipd\", \"__count\": 19305}, {\"value\": 1.347, \"variable\": \"ipd\", \"__count\": 99769}, {\"value\": 1.129, \"variable\": \"ipd\", \"__count\": 145875}, {\"value\": 1.006, \"variable\": \"ipd\", \"__count\": 180835}, {\"value\": -2.482, \"variable\": \"ipd\", \"__count\": 101017}, {\"value\": 1.187, \"variable\": \"ipd\", \"__count\": 132018}, {\"value\": 1.703, \"variable\": \"ipd\", \"__count\": 70754}, {\"value\": 1.78, \"variable\": \"ipd\", \"__count\": 54180}, {\"value\": 0.762, \"variable\": \"ipd\", \"__count\": 216901}, {\"value\": 1.622, \"variable\": \"ipd\", \"__count\": 93718}, {\"value\": 0.94, \"variable\": \"ipd\", \"__count\": 200324}, {\"value\": 1.321, \"variable\": \"ipd\", \"__count\": 82473}, {\"value\": 2.307, \"variable\": \"ipd\", \"__count\": 10775}, {\"value\": 1.241, \"variable\": \"ipd\", \"__count\": 119859}, {\"value\": 2.082, \"variable\": \"ipd\", \"__count\": 20601}, {\"value\": 1.491, \"variable\": \"ipd\", \"__count\": 76806}, {\"value\": 2.254, \"variable\": \"ipd\", \"__count\": 12512}, {\"value\": 1.536, \"variable\": \"ipd\", \"__count\": 70303}, {\"value\": 1.421, \"variable\": \"ipd\", \"__count\": 68833}, {\"value\": 1.396, \"variable\": \"ipd\", \"__count\": 90746}, {\"value\": 2.186, \"variable\": \"ipd\", \"__count\": 15099}, {\"value\": 2.098, \"variable\": \"ipd\", \"__count\": 36535}, {\"value\": 1.601, \"variable\": \"ipd\", \"__count\": 100585}, {\"value\": 1.973, \"variable\": \"ipd\", \"__count\": 28798}, {\"value\": 2.621, \"variable\": \"ipd\", \"__count\": 7191}, {\"value\": 1.579, \"variable\": \"ipd\", \"__count\": 108915}, {\"value\": 1.989, \"variable\": \"ipd\", \"__count\": 50000}, {\"value\": 2.068, \"variable\": \"ipd\", \"__count\": 21508}, {\"value\": 2.535, \"variable\": \"ipd\", \"__count\": 18951}, {\"value\": 1.854, \"variable\": \"ipd\", \"__count\": 42433}, {\"value\": 1.871, \"variable\": \"ipd\", \"__count\": 39841}, {\"value\": 1.742, \"variable\": \"ipd\", \"__count\": 61677}, {\"value\": 3.141, \"variable\": \"ipd\", \"__count\": 2120}, {\"value\": 1.295, \"variable\": \"ipd\", \"__count\": 109654}, {\"value\": 2.199, \"variable\": \"ipd\", \"__count\": 27270}, {\"value\": 2.143, \"variable\": \"ipd\", \"__count\": 17460}, {\"value\": 1.956, \"variable\": \"ipd\", \"__count\": 30628}, {\"value\": 1.817, \"variable\": \"ipd\", \"__count\": 47778}, {\"value\": 1.099, \"variable\": \"ipd\", \"__count\": 123148}, {\"value\": 2.053, \"variable\": \"ipd\", \"__count\": 22474}, {\"value\": 1.799, \"variable\": \"ipd\", \"__count\": 50383}, {\"value\": 3.148, \"variable\": \"ipd\", \"__count\": 1467}, {\"value\": 2.113, \"variable\": \"ipd\", \"__count\": 19040}, {\"value\": 1.835, \"variable\": \"ipd\", \"__count\": 44864}, {\"value\": 2.334, \"variable\": \"ipd\", \"__count\": 9773}, {\"value\": 1.762, \"variable\": \"ipd\", \"__count\": 57593}, {\"value\": 2.021, \"variable\": \"ipd\", \"__count\": 24919}, {\"value\": 2.383, \"variable\": \"ipd\", \"__count\": 24014}, {\"value\": 2.395, \"variable\": \"ipd\", \"__count\": 8538}, {\"value\": 1.923, \"variable\": \"ipd\", \"__count\": 33930}, {\"value\": 2.844, \"variable\": \"ipd\", \"__count\": 4373}, {\"value\": 2.172, \"variable\": \"ipd\", \"__count\": 16079}, {\"value\": 2.006, \"variable\": \"ipd\", \"__count\": 26359}, {\"value\": 1.939, \"variable\": \"ipd\", \"__count\": 31932}, {\"value\": 2.227, \"variable\": \"ipd\", \"__count\": 13528}, {\"value\": 2.037, \"variable\": \"ipd\", \"__count\": 23783}, {\"value\": 2.514, \"variable\": \"ipd\", \"__count\": 6040}, {\"value\": 1.723, \"variable\": \"ipd\", \"__count\": 65769}, {\"value\": 2.816, \"variable\": \"ipd\", \"__count\": 2723}, {\"value\": 2.578, \"variable\": \"ipd\", \"__count\": 9123}, {\"value\": 1.905, \"variable\": \"ipd\", \"__count\": 35558}, {\"value\": 2.963, \"variable\": \"ipd\", \"__count\": 4245}, {\"value\": 2.281, \"variable\": \"ipd\", \"__count\": 11349}, {\"value\": 1.663, \"variable\": \"ipd\", \"__count\": 81249}, {\"value\": 2.643, \"variable\": \"ipd\", \"__count\": 6392}, {\"value\": 2.408, \"variable\": \"ipd\", \"__count\": 8315}, {\"value\": 2.77, \"variable\": \"ipd\", \"__count\": 3499}, {\"value\": 2.502, \"variable\": \"ipd\", \"__count\": 6207}, {\"value\": 2.357, \"variable\": \"ipd\", \"__count\": 9426}, {\"value\": 2.557, \"variable\": \"ipd\", \"__count\": 10610}, {\"value\": 2.631, \"variable\": \"ipd\", \"__count\": 6816}, {\"value\": 2.547, \"variable\": \"ipd\", \"__count\": 11141}, {\"value\": 2.807, \"variable\": \"ipd\", \"__count\": 2847}, {\"value\": 2.443, \"variable\": \"ipd\", \"__count\": 7583}, {\"value\": 2.455, \"variable\": \"ipd\", \"__count\": 17483}, {\"value\": 2.127, \"variable\": \"ipd\", \"__count\": 18049}, {\"value\": 2.49, \"variable\": \"ipd\", \"__count\": 6627}, {\"value\": 2.467, \"variable\": \"ipd\", \"__count\": 16637}, {\"value\": 3.352, \"variable\": \"ipd\", \"__count\": 99}, {\"value\": 2.896, \"variable\": \"ipd\", \"__count\": 3402}, {\"value\": 3.164, \"variable\": \"ipd\", \"__count\": 985}, {\"value\": 2.611, \"variable\": \"ipd\", \"__count\": 7565}, {\"value\": 3.082, \"variable\": \"ipd\", \"__count\": 725}, {\"value\": 2.523, \"variable\": \"ipd\", \"__count\": 8339}, {\"value\": 2.346, \"variable\": \"ipd\", \"__count\": 9712}, {\"value\": 2.879, \"variable\": \"ipd\", \"__count\": 2494}, {\"value\": 2.42, \"variable\": \"ipd\", \"__count\": 8062}, {\"value\": 2.723, \"variable\": \"ipd\", \"__count\": 7954}, {\"value\": 2.703, \"variable\": \"ipd\", \"__count\": 5161}, {\"value\": 2.59, \"variable\": \"ipd\", \"__count\": 8577}, {\"value\": 2.713, \"variable\": \"ipd\", \"__count\": 4825}, {\"value\": 2.479, \"variable\": \"ipd\", \"__count\": 6710}, {\"value\": 2.6, \"variable\": \"ipd\", \"__count\": 7975}, {\"value\": 3.105, \"variable\": \"ipd\", \"__count\": 1043}, {\"value\": 2.568, \"variable\": \"ipd\", \"__count\": 9686}, {\"value\": 2.779, \"variable\": \"ipd\", \"__count\": 3248}, {\"value\": 2.922, \"variable\": \"ipd\", \"__count\": 1735}, {\"value\": 2.93, \"variable\": \"ipd\", \"__count\": 1713}, {\"value\": 3.219, \"variable\": \"ipd\", \"__count\": 524}, {\"value\": 2.76, \"variable\": \"ipd\", \"__count\": 3563}, {\"value\": 1.514, \"variable\": \"ipd\", \"__count\": 58694}, {\"value\": 3.059, \"variable\": \"ipd\", \"__count\": 1469}, {\"value\": 3.004, \"variable\": \"ipd\", \"__count\": 2311}, {\"value\": 2.268, \"variable\": \"ipd\", \"__count\": 12135}, {\"value\": 2.732, \"variable\": \"ipd\", \"__count\": 7642}, {\"value\": 2.213, \"variable\": \"ipd\", \"__count\": 14290}, {\"value\": 2.682, \"variable\": \"ipd\", \"__count\": 9909}, {\"value\": 2.672, \"variable\": \"ipd\", \"__count\": 9963}, {\"value\": 2.24, \"variable\": \"ipd\", \"__count\": 13214}, {\"value\": 1.184, \"variable\": \"ipd\", \"__count\": 116946}, {\"value\": -0.627, \"variable\": \"ipd\", \"__count\": 1084172}, {\"value\": 0.292, \"variable\": \"ipd\", \"__count\": 480245}, {\"value\": -0.453, \"variable\": \"ipd\", \"__count\": 1027738}, {\"value\": 0.085, \"variable\": \"ipd\", \"__count\": 627813}, {\"value\": -0.298, \"variable\": \"ipd\", \"__count\": 930022}, {\"value\": -0.537, \"variable\": \"ipd\", \"__count\": 894468}, {\"value\": 0.904, \"variable\": \"ipd\", \"__count\": 189977}, {\"value\": 2.137, \"variable\": \"ipd\", \"__count\": 15054}, {\"value\": -0.827, \"variable\": \"ipd\", \"__count\": 1070433}, {\"value\": 0.385, \"variable\": \"ipd\", \"__count\": 421556}, {\"value\": -0.158, \"variable\": \"ipd\", \"__count\": 823288}, {\"value\": -1.689, \"variable\": \"ipd\", \"__count\": 439246}, {\"value\": -1.339, \"variable\": \"ipd\", \"__count\": 707053}, {\"value\": 0.028, \"variable\": \"ipd\", \"__count\": 559020}, {\"value\": -1.904, \"variable\": \"ipd\", \"__count\": 231820}, {\"value\": -0.938, \"variable\": \"ipd\", \"__count\": 845053}, {\"value\": 1.051, \"variable\": \"ipd\", \"__count\": 119921}, {\"value\": -1.06, \"variable\": \"ipd\", \"__count\": 942639}, {\"value\": 0.631, \"variable\": \"ipd\", \"__count\": 292363}, {\"value\": -0.723, \"variable\": \"ipd\", \"__count\": 914924}, {\"value\": -0.031, \"variable\": \"ipd\", \"__count\": 720614}, {\"value\": 0.192, \"variable\": \"ipd\", \"__count\": 548313}, {\"value\": 1.132, \"variable\": \"ipd\", \"__count\": 129145}, {\"value\": 1.52, \"variable\": \"ipd\", \"__count\": 98268}, {\"value\": -2.871, \"variable\": \"ipd\", \"__count\": 95674}, {\"value\": 0.429, \"variable\": \"ipd\", \"__count\": 324608}, {\"value\": -0.093, \"variable\": \"ipd\", \"__count\": 639573}, {\"value\": 0.243, \"variable\": \"ipd\", \"__count\": 423574}, {\"value\": -0.226, \"variable\": \"ipd\", \"__count\": 730103}, {\"value\": 1.417, \"variable\": \"ipd\", \"__count\": 77359}, {\"value\": 1.671, \"variable\": \"ipd\", \"__count\": 58938}, {\"value\": 0.472, \"variable\": \"ipd\", \"__count\": 372702}, {\"value\": 0.774, \"variable\": \"ipd\", \"__count\": 234296}, {\"value\": 1.158, \"variable\": \"ipd\", \"__count\": 99361}, {\"value\": -1.503, \"variable\": \"ipd\", \"__count\": 444170}, {\"value\": 2.248, \"variable\": \"ipd\", \"__count\": 11039}, {\"value\": 0.14, \"variable\": \"ipd\", \"__count\": 486957}, {\"value\": -2.158, \"variable\": \"ipd\", \"__count\": 228671}, {\"value\": -0.373, \"variable\": \"ipd\", \"__count\": 819957}, {\"value\": 0.514, \"variable\": \"ipd\", \"__count\": 286596}, {\"value\": 1.459, \"variable\": \"ipd\", \"__count\": 122155}, {\"value\": 1.209, \"variable\": \"ipd\", \"__count\": 90249}, {\"value\": 0.593, \"variable\": \"ipd\", \"__count\": 253170}, {\"value\": -2.469, \"variable\": \"ipd\", \"__count\": 97353}, {\"value\": -1.192, \"variable\": \"ipd\", \"__count\": 676111}, {\"value\": 0.705, \"variable\": \"ipd\", \"__count\": 261471}, {\"value\": 1.479, \"variable\": \"ipd\", \"__count\": 113743}, {\"value\": 0.554, \"variable\": \"ipd\", \"__count\": 329368}, {\"value\": 1.258, \"variable\": \"ipd\", \"__count\": 82929}, {\"value\": 0.808, \"variable\": \"ipd\", \"__count\": 179696}, {\"value\": 1.328, \"variable\": \"ipd\", \"__count\": 91127}, {\"value\": 1.823, \"variable\": \"ipd\", \"__count\": 37011}, {\"value\": 0.668, \"variable\": \"ipd\", \"__count\": 225384}, {\"value\": 1.5, \"variable\": \"ipd\", \"__count\": 105919}, {\"value\": 0.74, \"variable\": \"ipd\", \"__count\": 200376}, {\"value\": 0.965, \"variable\": \"ipd\", \"__count\": 171010}, {\"value\": 0.873, \"variable\": \"ipd\", \"__count\": 160679}, {\"value\": 1.396, \"variable\": \"ipd\", \"__count\": 64621}, {\"value\": 1.022, \"variable\": \"ipd\", \"__count\": 155685}, {\"value\": 2.031, \"variable\": \"ipd\", \"__count\": 20260}, {\"value\": 1.56, \"variable\": \"ipd\", \"__count\": 86225}, {\"value\": 0.339, \"variable\": \"ipd\", \"__count\": 369873}, {\"value\": 1.9, \"variable\": \"ipd\", \"__count\": 29517}, {\"value\": 1.281, \"variable\": \"ipd\", \"__count\": 98495}, {\"value\": 0.935, \"variable\": \"ipd\", \"__count\": 145846}, {\"value\": 0.841, \"variable\": \"ipd\", \"__count\": 210884}, {\"value\": 1.078, \"variable\": \"ipd\", \"__count\": 141068}, {\"value\": 0.994, \"variable\": \"ipd\", \"__count\": 131991}, {\"value\": 2.602, \"variable\": \"ipd\", \"__count\": 5290}, {\"value\": 1.791, \"variable\": \"ipd\", \"__count\": 41075}, {\"value\": 1.916, \"variable\": \"ipd\", \"__count\": 28002}, {\"value\": 1.635, \"variable\": \"ipd\", \"__count\": 66224}, {\"value\": 1.839, \"variable\": \"ipd\", \"__count\": 35210}, {\"value\": 1.578, \"variable\": \"ipd\", \"__count\": 80393}, {\"value\": 2.045, \"variable\": \"ipd\", \"__count\": 19220}, {\"value\": 1.724, \"variable\": \"ipd\", \"__count\": 50195}, {\"value\": 2.018, \"variable\": \"ipd\", \"__count\": 20705}, {\"value\": 1.438, \"variable\": \"ipd\", \"__count\": 131614}, {\"value\": 1.807, \"variable\": \"ipd\", \"__count\": 38962}, {\"value\": 1.305, \"variable\": \"ipd\", \"__count\": 75974}, {\"value\": 1.351, \"variable\": \"ipd\", \"__count\": 70046}, {\"value\": 1.105, \"variable\": \"ipd\", \"__count\": 108114}, {\"value\": 1.652, \"variable\": \"ipd\", \"__count\": 62983}, {\"value\": 1.87, \"variable\": \"ipd\", \"__count\": 31654}, {\"value\": 2.426, \"variable\": \"ipd\", \"__count\": 12392}, {\"value\": 1.233, \"variable\": \"ipd\", \"__count\": 107225}, {\"value\": -3.436, \"variable\": \"ipd\", \"__count\": 28083}, {\"value\": 1.373, \"variable\": \"ipd\", \"__count\": 84182}, {\"value\": 3.16, \"variable\": \"ipd\", \"__count\": 210}, {\"value\": 1.688, \"variable\": \"ipd\", \"__count\": 55714}, {\"value\": 1.598, \"variable\": \"ipd\", \"__count\": 75826}, {\"value\": 1.854, \"variable\": \"ipd\", \"__count\": 33365}, {\"value\": 2.004, \"variable\": \"ipd\", \"__count\": 21872}, {\"value\": 2.371, \"variable\": \"ipd\", \"__count\": 20042}, {\"value\": 2.072, \"variable\": \"ipd\", \"__count\": 18054}, {\"value\": 1.931, \"variable\": \"ipd\", \"__count\": 26823}, {\"value\": 1.706, \"variable\": \"ipd\", \"__count\": 52776}, {\"value\": 1.758, \"variable\": \"ipd\", \"__count\": 44686}, {\"value\": 1.774, \"variable\": \"ipd\", \"__count\": 43406}, {\"value\": 2.086, \"variable\": \"ipd\", \"__count\": 17357}, {\"value\": 2.328, \"variable\": \"ipd\", \"__count\": 8924}, {\"value\": 1.886, \"variable\": \"ipd\", \"__count\": 30549}, {\"value\": 1.74, \"variable\": \"ipd\", \"__count\": 47816}, {\"value\": 1.616, \"variable\": \"ipd\", \"__count\": 71819}, {\"value\": 2.393, \"variable\": \"ipd\", \"__count\": 14489}, {\"value\": 2.111, \"variable\": \"ipd\", \"__count\": 16039}, {\"value\": 2.26, \"variable\": \"ipd\", \"__count\": 10896}, {\"value\": 2.74, \"variable\": \"ipd\", \"__count\": 3082}, {\"value\": 2.316, \"variable\": \"ipd\", \"__count\": 9043}, {\"value\": 2.506, \"variable\": \"ipd\", \"__count\": 8551}, {\"value\": 2.836, \"variable\": \"ipd\", \"__count\": 1685}, {\"value\": 2.715, \"variable\": \"ipd\", \"__count\": 3669}, {\"value\": 2.867, \"variable\": \"ipd\", \"__count\": 1436}, {\"value\": 2.176, \"variable\": \"ipd\", \"__count\": 13559}, {\"value\": 1.54, \"variable\": \"ipd\", \"__count\": 92085}, {\"value\": 1.96, \"variable\": \"ipd\", \"__count\": 24443}, {\"value\": 2.654, \"variable\": \"ipd\", \"__count\": 3811}, {\"value\": 2.627, \"variable\": \"ipd\", \"__count\": 4582}, {\"value\": 2.271, \"variable\": \"ipd\", \"__count\": 10614}, {\"value\": 2.236, \"variable\": \"ipd\", \"__count\": 11481}, {\"value\": 2.295, \"variable\": \"ipd\", \"__count\": 21077}, {\"value\": 2.805, \"variable\": \"ipd\", \"__count\": 2097}, {\"value\": 3.051, \"variable\": \"ipd\", \"__count\": 1517}, {\"value\": 2.926, \"variable\": \"ipd\", \"__count\": 1116}, {\"value\": 2.125, \"variable\": \"ipd\", \"__count\": 15655}, {\"value\": 1.945, \"variable\": \"ipd\", \"__count\": 25733}, {\"value\": 2.781, \"variable\": \"ipd\", \"__count\": 2347}, {\"value\": 2.516, \"variable\": \"ipd\", \"__count\": 8479}, {\"value\": 2.525, \"variable\": \"ipd\", \"__count\": 8114}, {\"value\": 2.496, \"variable\": \"ipd\", \"__count\": 8375}, {\"value\": 2.592, \"variable\": \"ipd\", \"__count\": 5565}, {\"value\": 2.211, \"variable\": \"ipd\", \"__count\": 12089}, {\"value\": 2.305, \"variable\": \"ipd\", \"__count\": 9441}, {\"value\": 2.445, \"variable\": \"ipd\", \"__count\": 10790}, {\"value\": 2.697, \"variable\": \"ipd\", \"__count\": 4132}, {\"value\": 2.15, \"variable\": \"ipd\", \"__count\": 14323}, {\"value\": 2.572, \"variable\": \"ipd\", \"__count\": 6064}, {\"value\": 2.35, \"variable\": \"ipd\", \"__count\": 8364}, {\"value\": 2.609, \"variable\": \"ipd\", \"__count\": 4982}, {\"value\": 2.225, \"variable\": \"ipd\", \"__count\": 11836}, {\"value\": -4.402, \"variable\": \"ipd\", \"__count\": 22762}, {\"value\": 2.582, \"variable\": \"ipd\", \"__count\": 5590}, {\"value\": 2.748, \"variable\": \"ipd\", \"__count\": 2838}, {\"value\": 2.689, \"variable\": \"ipd\", \"__count\": 4128}, {\"value\": 2.486, \"variable\": \"ipd\", \"__count\": 8606}, {\"value\": 2.875, \"variable\": \"ipd\", \"__count\": 1430}, {\"value\": 2.773, \"variable\": \"ipd\", \"__count\": 2447}, {\"value\": 2.797, \"variable\": \"ipd\", \"__count\": 2051}, {\"value\": 2.947, \"variable\": \"ipd\", \"__count\": 3450}, {\"value\": 2.162, \"variable\": \"ipd\", \"__count\": 13847}, {\"value\": 2.889, \"variable\": \"ipd\", \"__count\": 1279}, {\"value\": 2.969, \"variable\": \"ipd\", \"__count\": 2326}, {\"value\": 2.99, \"variable\": \"ipd\", \"__count\": 1450}, {\"value\": 2.941, \"variable\": \"ipd\", \"__count\": 2009}, {\"value\": 2.283, \"variable\": \"ipd\", \"__count\": 10188}, {\"value\": 2.414, \"variable\": \"ipd\", \"__count\": 13036}, {\"value\": 2.436, \"variable\": \"ipd\", \"__count\": 11673}, {\"value\": 2.555, \"variable\": \"ipd\", \"__count\": 6693}, {\"value\": 2.664, \"variable\": \"ipd\", \"__count\": 3708}, {\"value\": 2.404, \"variable\": \"ipd\", \"__count\": 13831}, {\"value\": 2.828, \"variable\": \"ipd\", \"__count\": 1849}, {\"value\": 2.188, \"variable\": \"ipd\", \"__count\": 12825}, {\"value\": 1.372, \"variable\": \"ipd\", \"__count\": 75384}, {\"value\": 2.32, \"variable\": \"ipd\", \"__count\": 10456}, {\"value\": 2.652, \"variable\": \"ipd\", \"__count\": 6320}, {\"value\": 2.693, \"variable\": \"ipd\", \"__count\": 5522}, {\"value\": 2.799, \"variable\": \"ipd\", \"__count\": 2903}, {\"value\": 2.75, \"variable\": \"ipd\", \"__count\": 3810}, {\"value\": 3.09, \"variable\": \"ipd\", \"__count\": 1205}, {\"value\": 2.834, \"variable\": \"ipd\", \"__count\": 2489}, {\"value\": 2.861, \"variable\": \"ipd\", \"__count\": 2751}, {\"value\": 2.869, \"variable\": \"ipd\", \"__count\": 2677}, {\"value\": 3.074, \"variable\": \"ipd\", \"__count\": 734}, {\"value\": 3.012, \"variable\": \"ipd\", \"__count\": 1039}, {\"value\": 3.02, \"variable\": \"ipd\", \"__count\": 1001}, {\"value\": 2.156, \"variable\": \"ipd\", \"__count\": 16479}, {\"value\": 2.432, \"variable\": \"ipd\", \"__count\": 7691}, {\"value\": 3.098, \"variable\": \"ipd\", \"__count\": 1241}, {\"value\": 3.135, \"variable\": \"ipd\", \"__count\": 2622}, {\"value\": 2.939, \"variable\": \"ipd\", \"__count\": 1594}, {\"value\": 3.035, \"variable\": \"ipd\", \"__count\": 961}, {\"value\": 3.254, \"variable\": \"ipd\", \"__count\": 389}, {\"value\": 2.988, \"variable\": \"ipd\", \"__count\": 1231}, {\"value\": 3.191, \"variable\": \"ipd\", \"__count\": 600}, {\"value\": 2.059, \"variable\": \"ipd\", \"__count\": 18516}, {\"value\": 1.975, \"variable\": \"ipd\", \"__count\": 23568}, {\"value\": 2.707, \"variable\": \"ipd\", \"__count\": 3951}, {\"value\": 2.361, \"variable\": \"ipd\", \"__count\": 8055}, {\"value\": 2.82, \"variable\": \"ipd\", \"__count\": 1876}, {\"value\": 2.766, \"variable\": \"ipd\", \"__count\": 2641}, {\"value\": 2.563, \"variable\": \"ipd\", \"__count\": 6415}, {\"value\": 3.064, \"variable\": \"ipd\", \"__count\": 564}, {\"value\": 2.904, \"variable\": \"ipd\", \"__count\": 3244}, {\"value\": 3.156, \"variable\": \"ipd\", \"__count\": 1203}, {\"value\": 2.662, \"variable\": \"ipd\", \"__count\": 6413}, {\"value\": 2.826, \"variable\": \"ipd\", \"__count\": 2495}, {\"value\": 3.127, \"variable\": \"ipd\", \"__count\": 1282}, {\"value\": 2.914, \"variable\": \"ipd\", \"__count\": 1860}, {\"value\": 3.199, \"variable\": \"ipd\", \"__count\": 575}, {\"value\": 3.123, \"variable\": \"ipd\", \"__count\": 298}, {\"value\": 2.859, \"variable\": \"ipd\", \"__count\": 1473}, {\"value\": 2.646, \"variable\": \"ipd\", \"__count\": 4142}, {\"value\": 3.01, \"variable\": \"ipd\", \"__count\": 1113}, {\"value\": 3.037, \"variable\": \"ipd\", \"__count\": 776}, {\"value\": 3.129, \"variable\": \"ipd\", \"__count\": 340}, {\"value\": 3.031, \"variable\": \"ipd\", \"__count\": 849}, {\"value\": 2.934, \"variable\": \"ipd\", \"__count\": 1502}, {\"value\": 2.477, \"variable\": \"ipd\", \"__count\": 9248}, {\"value\": 2.34, \"variable\": \"ipd\", \"__count\": 8727}, {\"value\": 2.912, \"variable\": \"ipd\", \"__count\": 1117}, {\"value\": 2.887, \"variable\": \"ipd\", \"__count\": 2380}, {\"value\": 3.119, \"variable\": \"ipd\", \"__count\": 1194}, {\"value\": 3.17, \"variable\": \"ipd\", \"__count\": 883}, {\"value\": 3.027, \"variable\": \"ipd\", \"__count\": 940}, {\"value\": 2.973, \"variable\": \"ipd\", \"__count\": 1290}, {\"value\": 3.111, \"variable\": \"ipd\", \"__count\": 1234}, {\"value\": 2.955, \"variable\": \"ipd\", \"__count\": 3295}, {\"value\": 3.066, \"variable\": \"ipd\", \"__count\": 713}, {\"value\": 2.742, \"variable\": \"ipd\", \"__count\": 4042}, {\"value\": 2.789, \"variable\": \"ipd\", \"__count\": 5258}, {\"value\": 2.92, \"variable\": \"ipd\", \"__count\": 1119}, {\"value\": 3.023, \"variable\": \"ipd\", \"__count\": 876}, {\"value\": 2.545, \"variable\": \"ipd\", \"__count\": 7332}, {\"value\": 3.045, \"variable\": \"ipd\", \"__count\": 696}, {\"value\": 3.166, \"variable\": \"ipd\", \"__count\": 226}, {\"value\": 2.813, \"variable\": \"ipd\", \"__count\": 1913}, {\"value\": 2.637, \"variable\": \"ipd\", \"__count\": 4272}, {\"value\": 2.619, \"variable\": \"ipd\", \"__count\": 4711}, {\"value\": 2.883, \"variable\": \"ipd\", \"__count\": 1320}, {\"value\": 2.982, \"variable\": \"ipd\", \"__count\": 1712}, {\"value\": 3.018, \"variable\": \"ipd\", \"__count\": 958}, {\"value\": 2.996, \"variable\": \"ipd\", \"__count\": 2412}, {\"value\": 2.977, \"variable\": \"ipd\", \"__count\": 1998}, {\"value\": 3.227, \"variable\": \"ipd\", \"__count\": 542}, {\"value\": 3.512, \"variable\": \"ipd\", \"__count\": 429}, {\"value\": 3.232, \"variable\": \"ipd\", \"__count\": 344}, {\"value\": 2.98, \"variable\": \"ipd\", \"__count\": 1252}, {\"value\": 3.268, \"variable\": \"ipd\", \"__count\": 297}, {\"value\": 3.213, \"variable\": \"ipd\", \"__count\": 650}, {\"value\": 3.186, \"variable\": \"ipd\", \"__count\": 683}, {\"value\": 3.24, \"variable\": \"ipd\", \"__count\": 355}, {\"value\": 3.506, \"variable\": \"ipd\", \"__count\": 88}, {\"value\": 3.246, \"variable\": \"ipd\", \"__count\": 329}, {\"value\": 3.26, \"variable\": \"ipd\", \"__count\": 370}, {\"value\": 3.461, \"variable\": \"ipd\", \"__count\": 29}, {\"value\": 3.346, \"variable\": \"ipd\", \"__count\": 131}, {\"value\": 3.313, \"variable\": \"ipd\", \"__count\": 154}, {\"value\": 3.178, \"variable\": \"ipd\", \"__count\": 973}, {\"value\": 3.473, \"variable\": \"ipd\", \"__count\": 23}, {\"value\": 3.281, \"variable\": \"ipd\", \"__count\": 276}, {\"value\": 3.369, \"variable\": \"ipd\", \"__count\": 88}, {\"value\": 3.043, \"variable\": \"ipd\", \"__count\": 880}, {\"value\": 3.146, \"variable\": \"ipd\", \"__count\": 286}, {\"value\": 3.395, \"variable\": \"ipd\", \"__count\": 62}, {\"value\": 3.326, \"variable\": \"ipd\", \"__count\": 150}, {\"value\": 3.205, \"variable\": \"ipd\", \"__count\": 492}, {\"value\": 3.078, \"variable\": \"ipd\", \"__count\": 500}, {\"value\": 2.756, \"variable\": \"ipd\", \"__count\": 2692}, {\"value\": 3.07, \"variable\": \"ipd\", \"__count\": 580}, {\"value\": 3.443, \"variable\": \"ipd\", \"__count\": 31}, {\"value\": 3.301, \"variable\": \"ipd\", \"__count\": 172}, {\"value\": 3.363, \"variable\": \"ipd\", \"__count\": 110}, {\"value\": 3.412, \"variable\": \"ipd\", \"__count\": 50}, {\"value\": 3.287, \"variable\": \"ipd\", \"__count\": 224}, {\"value\": 3.338, \"variable\": \"ipd\", \"__count\": 126}, {\"value\": 3.328, \"variable\": \"ipd\", \"__count\": 690}, {\"value\": 3.357, \"variable\": \"ipd\", \"__count\": 90}, {\"value\": 3.184, \"variable\": \"ipd\", \"__count\": 170}, {\"value\": 3.104, \"variable\": \"ipd\", \"__count\": 449}, {\"value\": 3.115, \"variable\": \"ipd\", \"__count\": 378}, {\"value\": 3.23, \"variable\": \"ipd\", \"__count\": 87}, {\"value\": 3.109, \"variable\": \"ipd\", \"__count\": 419}, {\"value\": 3.299, \"variable\": \"ipd\", \"__count\": 44}, {\"value\": 3.307, \"variable\": \"ipd\", \"__count\": 182}, {\"value\": 3.172, \"variable\": \"ipd\", \"__count\": 207}, {\"value\": 3.201, \"variable\": \"ipd\", \"__count\": 141}, {\"value\": 3.283, \"variable\": \"ipd\", \"__count\": 62}, {\"value\": 3.189, \"variable\": \"ipd\", \"__count\": 146}, {\"value\": 3.311, \"variable\": \"ipd\", \"__count\": 50}, {\"value\": 3.322, \"variable\": \"ipd\", \"__count\": 171}, {\"value\": 3.084, \"variable\": \"ipd\", \"__count\": 473}, {\"value\": 3.289, \"variable\": \"ipd\", \"__count\": 65}, {\"value\": 3.248, \"variable\": \"ipd\", \"__count\": 79}, {\"value\": 3.242, \"variable\": \"ipd\", \"__count\": 88}, {\"value\": 3.195, \"variable\": \"ipd\", \"__count\": 147}, {\"value\": 3.295, \"variable\": \"ipd\", \"__count\": 53}, {\"value\": 3.207, \"variable\": \"ipd\", \"__count\": 112}, {\"value\": 3.438, \"variable\": \"ipd\", \"__count\": 34}, {\"value\": 3.154, \"variable\": \"ipd\", \"__count\": 257}, {\"value\": 3.271, \"variable\": \"ipd\", \"__count\": 68}, {\"value\": 3.266, \"variable\": \"ipd\", \"__count\": 62}, {\"value\": 3.277, \"variable\": \"ipd\", \"__count\": 63}, {\"value\": 3.5, \"variable\": \"ipd\", \"__count\": 31}, {\"value\": 3.389, \"variable\": \"ipd\", \"__count\": 56}, {\"value\": 3.293, \"variable\": \"ipd\", \"__count\": 217}, {\"value\": 3.273, \"variable\": \"ipd\", \"__count\": 317}, {\"value\": 3.383, \"variable\": \"ipd\", \"__count\": 71}, {\"value\": 3.4, \"variable\": \"ipd\", \"__count\": 58}, {\"value\": 3.377, \"variable\": \"ipd\", \"__count\": 65}, {\"value\": 3.32, \"variable\": \"ipd\", \"__count\": 145}, {\"value\": 3.332, \"variable\": \"ipd\", \"__count\": 140}, {\"value\": 3.426, \"variable\": \"ipd\", \"__count\": 47}, {\"value\": 3.316, \"variable\": \"ipd\", \"__count\": 57}, {\"value\": 3.236, \"variable\": \"ipd\", \"__count\": 90}, {\"value\": 3.418, \"variable\": \"ipd\", \"__count\": 46}, {\"value\": 3.449, \"variable\": \"ipd\", \"__count\": 33}, {\"value\": 3.494, \"variable\": \"ipd\", \"__count\": 26}, {\"value\": 3.406, \"variable\": \"ipd\", \"__count\": 79}, {\"value\": 3.484, \"variable\": \"ipd\", \"__count\": 22}, {\"value\": 3.455, \"variable\": \"ipd\", \"__count\": 34}, {\"value\": 3.305, \"variable\": \"ipd\", \"__count\": 58}, {\"value\": 3.467, \"variable\": \"ipd\", \"__count\": 42}, {\"value\": 3.432, \"variable\": \"ipd\", \"__count\": 39}, {\"value\": 3.479, \"variable\": \"ipd\", \"__count\": 28}, {\"value\": 3.488, \"variable\": \"ipd\", \"__count\": 30}, {\"value\": 0.789, \"variable\": \"pw\", \"__count\": 547418}, {\"value\": 1.397, \"variable\": \"pw\", \"__count\": 211856}, {\"value\": 0.312, \"variable\": \"pw\", \"__count\": 952528}, {\"value\": -1.4, \"variable\": \"pw\", \"__count\": 747916}, {\"value\": -0.567, \"variable\": \"pw\", \"__count\": 1481282}, {\"value\": 0.926, \"variable\": \"pw\", \"__count\": 451544}, {\"value\": -0.081, \"variable\": \"pw\", \"__count\": 1274864}, {\"value\": -1.617, \"variable\": \"pw\", \"__count\": 699616}, {\"value\": 0.125, \"variable\": \"pw\", \"__count\": 1116210}, {\"value\": -0.861, \"variable\": \"pw\", \"__count\": 1439295}, {\"value\": 0.221, \"variable\": \"pw\", \"__count\": 859126}, {\"value\": 0.716, \"variable\": \"pw\", \"__count\": 496491}, {\"value\": 1.054, \"variable\": \"pw\", \"__count\": 372729}, {\"value\": -0.708, \"variable\": \"pw\", \"__count\": 1220885}, {\"value\": 1.175, \"variable\": \"pw\", \"__count\": 307644}, {\"value\": 0.858, \"variable\": \"pw\", \"__count\": 409291}, {\"value\": 0.642, \"variable\": \"pw\", \"__count\": 662091}, {\"value\": -1.025, \"variable\": \"pw\", \"__count\": 1096695}, {\"value\": 0.399, \"variable\": \"pw\", \"__count\": 724917}, {\"value\": -0.31, \"variable\": \"pw\", \"__count\": 1409194}, {\"value\": 1.233, \"variable\": \"pw\", \"__count\": 227196}, {\"value\": -0.434, \"variable\": \"pw\", \"__count\": 1204511}, {\"value\": 0.564, \"variable\": \"pw\", \"__count\": 602543}, {\"value\": -0.192, \"variable\": \"pw\", \"__count\": 1114657}, {\"value\": 1.824, \"variable\": \"pw\", \"__count\": 76120}, {\"value\": 0.025, \"variable\": \"pw\", \"__count\": 994422}, {\"value\": -1.859, \"variable\": \"pw\", \"__count\": 333650}, {\"value\": 2.383, \"variable\": \"pw\", \"__count\": 29455}, {\"value\": 2.479, \"variable\": \"pw\", \"__count\": 40886}, {\"value\": 2.178, \"variable\": \"pw\", \"__count\": 46124}, {\"value\": 0.483, \"variable\": \"pw\", \"__count\": 799168}, {\"value\": 1.949, \"variable\": \"pw\", \"__count\": 74465}, {\"value\": 2.742, \"variable\": \"pw\", \"__count\": 12837}, {\"value\": 3.139, \"variable\": \"pw\", \"__count\": 2723}, {\"value\": 1.344, \"variable\": \"pw\", \"__count\": 187581}, {\"value\": -1.204, \"variable\": \"pw\", \"__count\": 1180163}, {\"value\": 0.991, \"variable\": \"pw\", \"__count\": 335574}, {\"value\": 1.646, \"variable\": \"pw\", \"__count\": 107227}, {\"value\": 2.283, \"variable\": \"pw\", \"__count\": 28924}, {\"value\": -2.133, \"variable\": \"pw\", \"__count\": 274768}, {\"value\": 2.316, \"variable\": \"pw\", \"__count\": 34448}, {\"value\": 1.115, \"variable\": \"pw\", \"__count\": 275240}, {\"value\": 2.629, \"variable\": \"pw\", \"__count\": 21122}, {\"value\": 1.5, \"variable\": \"pw\", \"__count\": 176443}, {\"value\": 1.599, \"variable\": \"pw\", \"__count\": 147185}, {\"value\": 2.6, \"variable\": \"pw\", \"__count\": 24107}, {\"value\": 2.104, \"variable\": \"pw\", \"__count\": 54944}, {\"value\": 1.449, \"variable\": \"pw\", \"__count\": 154967}, {\"value\": 1.867, \"variable\": \"pw\", \"__count\": 87907}, {\"value\": 1.289, \"variable\": \"pw\", \"__count\": 254101}, {\"value\": 2.213, \"variable\": \"pw\", \"__count\": 33725}, {\"value\": 1.55, \"variable\": \"pw\", \"__count\": 129037}, {\"value\": 1.691, \"variable\": \"pw\", \"__count\": 123068}, {\"value\": -2.826, \"variable\": \"pw\", \"__count\": 58612}, {\"value\": 2.541, \"variable\": \"pw\", \"__count\": 30925}, {\"value\": 3.709, \"variable\": \"pw\", \"__count\": 440}, {\"value\": 1.781, \"variable\": \"pw\", \"__count\": 103646}, {\"value\": 1.737, \"variable\": \"pw\", \"__count\": 90394}, {\"value\": 2.027, \"variable\": \"pw\", \"__count\": 63505}, {\"value\": 2.51, \"variable\": \"pw\", \"__count\": 35542}, {\"value\": 2.066, \"variable\": \"pw\", \"__count\": 46037}, {\"value\": 2.715, \"variable\": \"pw\", \"__count\": 14414}, {\"value\": 3.023, \"variable\": \"pw\", \"__count\": 4214}, {\"value\": 2.926, \"variable\": \"pw\", \"__count\": 6139}, {\"value\": 2.875, \"variable\": \"pw\", \"__count\": 7557}, {\"value\": 2.686, \"variable\": \"pw\", \"__count\": 16128}, {\"value\": 2.416, \"variable\": \"pw\", \"__count\": 21399}, {\"value\": 2.822, \"variable\": \"pw\", \"__count\": 9326}, {\"value\": 2.951, \"variable\": \"pw\", \"__count\": 5638}, {\"value\": 2.57, \"variable\": \"pw\", \"__count\": 26969}, {\"value\": 2.447, \"variable\": \"pw\", \"__count\": 25784}, {\"value\": 2.248, \"variable\": \"pw\", \"__count\": 39510}, {\"value\": -2.451, \"variable\": \"pw\", \"__count\": 87453}, {\"value\": 2.352, \"variable\": \"pw\", \"__count\": 24735}, {\"value\": 3.793, \"variable\": \"pw\", \"__count\": 351}, {\"value\": 3.117, \"variable\": \"pw\", \"__count\": 3045}, {\"value\": 2.797, \"variable\": \"pw\", \"__count\": 10325}, {\"value\": -3.875, \"variable\": \"pw\", \"__count\": 1421}, {\"value\": 2.141, \"variable\": \"pw\", \"__count\": 39140}, {\"value\": 3.271, \"variable\": \"pw\", \"__count\": 1737}, {\"value\": 1.908, \"variable\": \"pw\", \"__count\": 64381}, {\"value\": 3.842, \"variable\": \"pw\", \"__count\": 301}, {\"value\": 2.658, \"variable\": \"pw\", \"__count\": 18505}, {\"value\": 3.07, \"variable\": \"pw\", \"__count\": 3497}, {\"value\": 3.621, \"variable\": \"pw\", \"__count\": 600}, {\"value\": 3.674, \"variable\": \"pw\", \"__count\": 549}, {\"value\": 3.604, \"variable\": \"pw\", \"__count\": 576}, {\"value\": 4.039, \"variable\": \"pw\", \"__count\": 486}, {\"value\": 4.207, \"variable\": \"pw\", \"__count\": 72}, {\"value\": 4.223, \"variable\": \"pw\", \"__count\": 160}, {\"value\": 3.656, \"variable\": \"pw\", \"__count\": 544}, {\"value\": 2.77, \"variable\": \"pw\", \"__count\": 11699}, {\"value\": 3.094, \"variable\": \"pw\", \"__count\": 3225}, {\"value\": 3.889, \"variable\": \"pw\", \"__count\": 1088}, {\"value\": 3.691, \"variable\": \"pw\", \"__count\": 523}, {\"value\": 3.0, \"variable\": \"pw\", \"__count\": 4558}, {\"value\": 3.434, \"variable\": \"pw\", \"__count\": 1060}, {\"value\": 2.85, \"variable\": \"pw\", \"__count\": 8320}, {\"value\": -3.285, \"variable\": \"pw\", \"__count\": 7414}, {\"value\": 3.162, \"variable\": \"pw\", \"__count\": 2492}, {\"value\": 3.184, \"variable\": \"pw\", \"__count\": 2392}, {\"value\": 3.207, \"variable\": \"pw\", \"__count\": 2173}, {\"value\": 1.989, \"variable\": \"pw\", \"__count\": 54382}, {\"value\": 3.584, \"variable\": \"pw\", \"__count\": 584}, {\"value\": 3.047, \"variable\": \"pw\", \"__count\": 3717}, {\"value\": 3.529, \"variable\": \"pw\", \"__count\": 740}, {\"value\": 2.9, \"variable\": \"pw\", \"__count\": 6710}, {\"value\": 3.354, \"variable\": \"pw\", \"__count\": 1353}, {\"value\": 3.291, \"variable\": \"pw\", \"__count\": 1551}, {\"value\": 0.73, \"variable\": \"pw\", \"__count\": 493162}, {\"value\": -1.192, \"variable\": \"pw\", \"__count\": 1197298}, {\"value\": 0.655, \"variable\": \"pw\", \"__count\": 657649}, {\"value\": 1.304, \"variable\": \"pw\", \"__count\": 250037}, {\"value\": 1.005, \"variable\": \"pw\", \"__count\": 330945}, {\"value\": 0.234, \"variable\": \"pw\", \"__count\": 855594}, {\"value\": 1.189, \"variable\": \"pw\", \"__count\": 302018}, {\"value\": 1.612, \"variable\": \"pw\", \"__count\": 143940}, {\"value\": 0.413, \"variable\": \"pw\", \"__count\": 721238}, {\"value\": -0.421, \"variable\": \"pw\", \"__count\": 1213999}, {\"value\": -0.068, \"variable\": \"pw\", \"__count\": 1277141}, {\"value\": 0.138, \"variable\": \"pw\", \"__count\": 1115414}, {\"value\": 0.802, \"variable\": \"pw\", \"__count\": 542286}, {\"value\": -0.849, \"variable\": \"pw\", \"__count\": 1460802}, {\"value\": 0.578, \"variable\": \"pw\", \"__count\": 596573}, {\"value\": -1.013, \"variable\": \"pw\", \"__count\": 1114901}, {\"value\": -0.297, \"variable\": \"pw\", \"__count\": 1416757}, {\"value\": -0.554, \"variable\": \"pw\", \"__count\": 1491630}, {\"value\": -1.388, \"variable\": \"pw\", \"__count\": 763322}, {\"value\": 1.515, \"variable\": \"pw\", \"__count\": 172682}, {\"value\": 0.497, \"variable\": \"pw\", \"__count\": 792312}, {\"value\": 2.555, \"variable\": \"pw\", \"__count\": 29943}, {\"value\": -0.696, \"variable\": \"pw\", \"__count\": 1234485}, {\"value\": 0.939, \"variable\": \"pw\", \"__count\": 446213}, {\"value\": -2.121, \"variable\": \"pw\", \"__count\": 277851}, {\"value\": -1.848, \"variable\": \"pw\", \"__count\": 340031}, {\"value\": 1.706, \"variable\": \"pw\", \"__count\": 120804}, {\"value\": 0.872, \"variable\": \"pw\", \"__count\": 403402}, {\"value\": 1.751, \"variable\": \"pw\", \"__count\": 88094}, {\"value\": 2.264, \"variable\": \"pw\", \"__count\": 38604}, {\"value\": 0.325, \"variable\": \"pw\", \"__count\": 947176}, {\"value\": 1.66, \"variable\": \"pw\", \"__count\": 105774}, {\"value\": 1.464, \"variable\": \"pw\", \"__count\": 152611}, {\"value\": 2.398, \"variable\": \"pw\", \"__count\": 28568}, {\"value\": -0.179, \"variable\": \"pw\", \"__count\": 1120250}, {\"value\": -1.604, \"variable\": \"pw\", \"__count\": 707983}, {\"value\": 3.063, \"variable\": \"pw\", \"__count\": 3680}, {\"value\": 2.119, \"variable\": \"pw\", \"__count\": 52399}, {\"value\": 1.411, \"variable\": \"pw\", \"__count\": 207706}, {\"value\": 2.645, \"variable\": \"pw\", \"__count\": 20153}, {\"value\": 1.247, \"variable\": \"pw\", \"__count\": 222677}, {\"value\": -2.814, \"variable\": \"pw\", \"__count\": 60617}, {\"value\": 2.494, \"variable\": \"pw\", \"__count\": 39387}, {\"value\": 1.796, \"variable\": \"pw\", \"__count\": 102499}, {\"value\": 1.881, \"variable\": \"pw\", \"__count\": 85891}, {\"value\": 3.199, \"variable\": \"pw\", \"__count\": 2206}, {\"value\": 0.038, \"variable\": \"pw\", \"__count\": 994007}, {\"value\": 1.068, \"variable\": \"pw\", \"__count\": 367050}, {\"value\": 2.729, \"variable\": \"pw\", \"__count\": 13836}, {\"value\": -2.439, \"variable\": \"pw\", \"__count\": 89845}, {\"value\": 1.129, \"variable\": \"pw\", \"__count\": 270539}, {\"value\": 2.156, \"variable\": \"pw\", \"__count\": 38113}, {\"value\": 1.839, \"variable\": \"pw\", \"__count\": 73903}, {\"value\": 2.297, \"variable\": \"pw\", \"__count\": 27694}, {\"value\": 1.564, \"variable\": \"pw\", \"__count\": 125882}, {\"value\": 1.358, \"variable\": \"pw\", \"__count\": 184012}, {\"value\": 2.004, \"variable\": \"pw\", \"__count\": 52779}, {\"value\": 2.229, \"variable\": \"pw\", \"__count\": 32641}, {\"value\": 2.674, \"variable\": \"pw\", \"__count\": 17797}, {\"value\": 2.43, \"variable\": \"pw\", \"__count\": 20619}, {\"value\": 2.043, \"variable\": \"pw\", \"__count\": 61417}, {\"value\": 2.811, \"variable\": \"pw\", \"__count\": 9984}, {\"value\": 2.758, \"variable\": \"pw\", \"__count\": 12402}, {\"value\": 2.463, \"variable\": \"pw\", \"__count\": 24806}, {\"value\": 1.964, \"variable\": \"pw\", \"__count\": 72592}, {\"value\": 2.08, \"variable\": \"pw\", \"__count\": 44920}, {\"value\": 1.923, \"variable\": \"pw\", \"__count\": 62754}, {\"value\": 2.332, \"variable\": \"pw\", \"__count\": 33173}, {\"value\": 3.285, \"variable\": \"pw\", \"__count\": 1740}, {\"value\": 2.783, \"variable\": \"pw\", \"__count\": 11222}, {\"value\": 2.701, \"variable\": \"pw\", \"__count\": 15621}, {\"value\": 3.109, \"variable\": \"pw\", \"__count\": 3139}, {\"value\": 2.525, \"variable\": \"pw\", \"__count\": 34121}, {\"value\": 2.615, \"variable\": \"pw\", \"__count\": 22827}, {\"value\": 2.586, \"variable\": \"pw\", \"__count\": 26272}, {\"value\": 2.889, \"variable\": \"pw\", \"__count\": 7075}, {\"value\": 2.191, \"variable\": \"pw\", \"__count\": 45226}, {\"value\": 2.365, \"variable\": \"pw\", \"__count\": 23943}, {\"value\": 2.965, \"variable\": \"pw\", \"__count\": 5234}, {\"value\": 3.775, \"variable\": \"pw\", \"__count\": 695}, {\"value\": 3.654, \"variable\": \"pw\", \"__count\": 510}, {\"value\": 3.725, \"variable\": \"pw\", \"__count\": 880}, {\"value\": 3.178, \"variable\": \"pw\", \"__count\": 2426}, {\"value\": 2.863, \"variable\": \"pw\", \"__count\": 8140}, {\"value\": 3.35, \"variable\": \"pw\", \"__count\": 1335}, {\"value\": 3.039, \"variable\": \"pw\", \"__count\": 4006}, {\"value\": 3.086, \"variable\": \"pw\", \"__count\": 3349}, {\"value\": 2.916, \"variable\": \"pw\", \"__count\": 6478}, {\"value\": 2.838, \"variable\": \"pw\", \"__count\": 8955}, {\"value\": 3.904, \"variable\": \"pw\", \"__count\": 995}, {\"value\": -3.273, \"variable\": \"pw\", \"__count\": 8929}, {\"value\": 3.41, \"variable\": \"pw\", \"__count\": 1105}, {\"value\": 2.99, \"variable\": \"pw\", \"__count\": 4881}, {\"value\": 3.488, \"variable\": \"pw\", \"__count\": 866}, {\"value\": 3.221, \"variable\": \"pw\", \"__count\": 2078}, {\"value\": 3.508, \"variable\": \"pw\", \"__count\": 772}, {\"value\": 3.525, \"variable\": \"pw\", \"__count\": 809}, {\"value\": 2.939, \"variable\": \"pw\", \"__count\": 5940}, {\"value\": 3.74, \"variable\": \"pw\", \"__count\": 401}, {\"value\": 4.07, \"variable\": \"pw\", \"__count\": 544}, {\"value\": 3.25, \"variable\": \"pw\", \"__count\": 1847}, {\"value\": 3.936, \"variable\": \"pw\", \"__count\": 791}, {\"value\": 3.453, \"variable\": \"pw\", \"__count\": 966}, {\"value\": 3.549, \"variable\": \"pw\", \"__count\": 693}, {\"value\": 4.012, \"variable\": \"pw\", \"__count\": 499}, {\"value\": 4.141, \"variable\": \"pw\", \"__count\": 233}, {\"value\": 3.313, \"variable\": \"pw\", \"__count\": 1490}, {\"value\": 3.51, \"variable\": \"pw\", \"__count\": 821}, {\"value\": 3.965, \"variable\": \"pw\", \"__count\": 318}, {\"value\": 3.395, \"variable\": \"pw\", \"__count\": 1181}, {\"value\": 2.975, \"variable\": \"pw\", \"__count\": 5020}, {\"value\": 3.873, \"variable\": \"pw\", \"__count\": 980}, {\"value\": 4.375, \"variable\": \"pw\", \"__count\": 37}, {\"value\": 4.387, \"variable\": \"pw\", \"__count\": 34}, {\"value\": 3.707, \"variable\": \"pw\", \"__count\": 501}, {\"value\": 3.266, \"variable\": \"pw\", \"__count\": 1864}, {\"value\": 3.996, \"variable\": \"pw\", \"__count\": 516}, {\"value\": 3.133, \"variable\": \"pw\", \"__count\": 2894}, {\"value\": 3.369, \"variable\": \"pw\", \"__count\": 1264}, {\"value\": 3.389, \"variable\": \"pw\", \"__count\": 1165}, {\"value\": 3.545, \"variable\": \"pw\", \"__count\": 719}, {\"value\": 3.375, \"variable\": \"pw\", \"__count\": 1216}, {\"value\": 4.125, \"variable\": \"pw\", \"__count\": 138}, {\"value\": 3.566, \"variable\": \"pw\", \"__count\": 704}, {\"value\": 3.92, \"variable\": \"pw\", \"__count\": 877}, {\"value\": 3.857, \"variable\": \"pw\", \"__count\": 693}, {\"value\": 3.334, \"variable\": \"pw\", \"__count\": 1463}, {\"value\": 3.229, \"variable\": \"pw\", \"__count\": 1933}, {\"value\": 3.307, \"variable\": \"pw\", \"__count\": 1607}, {\"value\": 4.156, \"variable\": \"pw\", \"__count\": 102}, {\"value\": 3.563, \"variable\": \"pw\", \"__count\": 742}, {\"value\": 3.014, \"variable\": \"pw\", \"__count\": 4415}, {\"value\": 3.43, \"variable\": \"pw\", \"__count\": 1016}, {\"value\": -6.125, \"variable\": \"pw\", \"__count\": 1352}, {\"value\": 3.809, \"variable\": \"pw\", \"__count\": 643}, {\"value\": 3.76, \"variable\": \"pw\", \"__count\": 333}, {\"value\": 3.619, \"variable\": \"pw\", \"__count\": 543}, {\"value\": 3.6, \"variable\": \"pw\", \"__count\": 587}, {\"value\": 3.242, \"variable\": \"pw\", \"__count\": 1931}, {\"value\": 3.637, \"variable\": \"pw\", \"__count\": 568}, {\"value\": 3.98, \"variable\": \"pw\", \"__count\": 597}, {\"value\": 3.154, \"variable\": \"pw\", \"__count\": 2699}, {\"value\": 3.414, \"variable\": \"pw\", \"__count\": 1121}, {\"value\": 4.684, \"variable\": \"pw\", \"__count\": 20}, {\"value\": 4.055, \"variable\": \"pw\", \"__count\": 491}, {\"value\": 4.719, \"variable\": \"pw\", \"__count\": 28}, {\"value\": 4.113, \"variable\": \"pw\", \"__count\": 353}, {\"value\": -3.865, \"variable\": \"pw\", \"__count\": 1705}, {\"value\": 3.469, \"variable\": \"pw\", \"__count\": 959}, {\"value\": 4.43, \"variable\": \"pw\", \"__count\": 25}, {\"value\": 3.492, \"variable\": \"pw\", \"__count\": 857}, {\"value\": 3.328, \"variable\": \"pw\", \"__count\": 1434}, {\"value\": 3.951, \"variable\": \"pw\", \"__count\": 651}, {\"value\": 4.441, \"variable\": \"pw\", \"__count\": 29}, {\"value\": 4.168, \"variable\": \"pw\", \"__count\": 227}, {\"value\": 4.363, \"variable\": \"pw\", \"__count\": 52}, {\"value\": 3.639, \"variable\": \"pw\", \"__count\": 546}, {\"value\": 4.734, \"variable\": \"pw\", \"__count\": 30}, {\"value\": 4.086, \"variable\": \"pw\", \"__count\": 264}, {\"value\": 4.316, \"variable\": \"pw\", \"__count\": 70}, {\"value\": 4.484, \"variable\": \"pw\", \"__count\": 9}, {\"value\": 4.438, \"variable\": \"pw\", \"__count\": 31}, {\"value\": 4.531, \"variable\": \"pw\", \"__count\": 18}, {\"value\": 4.184, \"variable\": \"pw\", \"__count\": 97}, {\"value\": 3.672, \"variable\": \"pw\", \"__count\": 533}, {\"value\": 3.758, \"variable\": \"pw\", \"__count\": 412}, {\"value\": 3.791, \"variable\": \"pw\", \"__count\": 330}, {\"value\": 4.129, \"variable\": \"pw\", \"__count\": 175}, {\"value\": 4.246, \"variable\": \"pw\", \"__count\": 82}, {\"value\": 4.285, \"variable\": \"pw\", \"__count\": 70}, {\"value\": 4.352, \"variable\": \"pw\", \"__count\": 65}, {\"value\": 4.082, \"variable\": \"pw\", \"__count\": 191}, {\"value\": 3.689, \"variable\": \"pw\", \"__count\": 466}, {\"value\": 3.582, \"variable\": \"pw\", \"__count\": 622}, {\"value\": 3.449, \"variable\": \"pw\", \"__count\": 986}, {\"value\": 3.824, \"variable\": \"pw\", \"__count\": 599}, {\"value\": 3.473, \"variable\": \"pw\", \"__count\": 899}, {\"value\": 4.023, \"variable\": \"pw\", \"__count\": 254}, {\"value\": 4.402, \"variable\": \"pw\", \"__count\": 36}, {\"value\": 4.355, \"variable\": \"pw\", \"__count\": 82}, {\"value\": 4.027, \"variable\": \"pw\", \"__count\": 239}, {\"value\": 4.238, \"variable\": \"pw\", \"__count\": 70}, {\"value\": 4.34, \"variable\": \"pw\", \"__count\": 191}, {\"value\": 4.414, \"variable\": \"pw\", \"__count\": 50}, {\"value\": 4.273, \"variable\": \"pw\", \"__count\": 80}, {\"value\": 4.324, \"variable\": \"pw\", \"__count\": 134}, {\"value\": 4.18, \"variable\": \"pw\", \"__count\": 91}, {\"value\": 4.262, \"variable\": \"pw\", \"__count\": 157}, {\"value\": 3.742, \"variable\": \"pw\", \"__count\": 382}, {\"value\": 3.84, \"variable\": \"pw\", \"__count\": 270}, {\"value\": 4.098, \"variable\": \"pw\", \"__count\": 401}, {\"value\": 4.301, \"variable\": \"pw\", \"__count\": 145}, {\"value\": 4.313, \"variable\": \"pw\", \"__count\": 105}, {\"value\": 4.398, \"variable\": \"pw\", \"__count\": 33}, {\"value\": 4.742, \"variable\": \"pw\", \"__count\": 69}, {\"value\": 4.582, \"variable\": \"pw\", \"__count\": 19}, {\"value\": 4.488, \"variable\": \"pw\", \"__count\": 17}, {\"value\": 3.967, \"variable\": \"pw\", \"__count\": 329}, {\"value\": 4.703, \"variable\": \"pw\", \"__count\": 20}, {\"value\": 4.727, \"variable\": \"pw\", \"__count\": 54}, {\"value\": -4.699, \"variable\": \"pw\", \"__count\": 28}, {\"value\": 4.711, \"variable\": \"pw\", \"__count\": 27}, {\"value\": 4.152, \"variable\": \"pw\", \"__count\": 95}, {\"value\": 4.195, \"variable\": \"pw\", \"__count\": 182}, {\"value\": 4.234, \"variable\": \"pw\", \"__count\": 83}, {\"value\": 4.465, \"variable\": \"pw\", \"__count\": 23}, {\"value\": 4.211, \"variable\": \"pw\", \"__count\": 115}, {\"value\": 4.473, \"variable\": \"pw\", \"__count\": 18}, {\"value\": 4.328, \"variable\": \"pw\", \"__count\": 87}, {\"value\": 4.25, \"variable\": \"pw\", \"__count\": 82}, {\"value\": 4.461, \"variable\": \"pw\", \"__count\": 18}, {\"value\": 4.555, \"variable\": \"pw\", \"__count\": 15}, {\"value\": 4.289, \"variable\": \"pw\", \"__count\": 60}, {\"value\": 4.367, \"variable\": \"pw\", \"__count\": 70}, {\"value\": 4.574, \"variable\": \"pw\", \"__count\": 11}, {\"value\": 4.738, \"variable\": \"pw\", \"__count\": 37}, {\"value\": 5.258, \"variable\": \"pw\", \"__count\": 3}, {\"value\": 4.613, \"variable\": \"pw\", \"__count\": 13}, {\"value\": 4.707, \"variable\": \"pw\", \"__count\": 28}, {\"value\": 4.5, \"variable\": \"pw\", \"__count\": 14}, {\"value\": 5.176, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.918, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 4.945, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 4.602, \"variable\": \"pw\", \"__count\": 6}, {\"value\": 4.477, \"variable\": \"pw\", \"__count\": 27}, {\"value\": 4.758, \"variable\": \"pw\", \"__count\": 5}, {\"value\": 4.723, \"variable\": \"pw\", \"__count\": 23}, {\"value\": 4.277, \"variable\": \"pw\", \"__count\": 56}, {\"value\": 4.52, \"variable\": \"pw\", \"__count\": 20}, {\"value\": 4.625, \"variable\": \"pw\", \"__count\": 14}, {\"value\": -6.133, \"variable\": \"pw\", \"__count\": 113}, {\"value\": 4.379, \"variable\": \"pw\", \"__count\": 45}, {\"value\": 4.801, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 5.266, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.586, \"variable\": \"pw\", \"__count\": 8}, {\"value\": 4.695, \"variable\": \"pw\", \"__count\": 28}, {\"value\": 4.754, \"variable\": \"pw\", \"__count\": 22}, {\"value\": 4.875, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.676, \"variable\": \"pw\", \"__count\": 11}, {\"value\": 4.543, \"variable\": \"pw\", \"__count\": 10}, {\"value\": 4.512, \"variable\": \"pw\", \"__count\": 16}, {\"value\": 4.57, \"variable\": \"pw\", \"__count\": 16}, {\"value\": 4.598, \"variable\": \"pw\", \"__count\": 18}, {\"value\": 4.453, \"variable\": \"pw\", \"__count\": 18}, {\"value\": 5.133, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 4.559, \"variable\": \"pw\", \"__count\": 6}, {\"value\": 4.426, \"variable\": \"pw\", \"__count\": 22}, {\"value\": 4.496, \"variable\": \"pw\", \"__count\": 12}, {\"value\": 4.652, \"variable\": \"pw\", \"__count\": 12}, {\"value\": 4.766, \"variable\": \"pw\", \"__count\": 13}, {\"value\": 4.609, \"variable\": \"pw\", \"__count\": 12}, {\"value\": 4.449, \"variable\": \"pw\", \"__count\": 23}, {\"value\": 4.691, \"variable\": \"pw\", \"__count\": 5}, {\"value\": 4.621, \"variable\": \"pw\", \"__count\": 10}, {\"value\": 4.633, \"variable\": \"pw\", \"__count\": 11}, {\"value\": 4.637, \"variable\": \"pw\", \"__count\": 13}, {\"value\": 4.391, \"variable\": \"pw\", \"__count\": 26}, {\"value\": 5.016, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.797, \"variable\": \"pw\", \"__count\": 7}, {\"value\": 4.836, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 5.148, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 5.242, \"variable\": \"pw\", \"__count\": 3}, {\"value\": 5.191, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.941, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 4.844, \"variable\": \"pw\", \"__count\": 4}, {\"value\": 4.98, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.535, \"variable\": \"pw\", \"__count\": 17}, {\"value\": 4.566, \"variable\": \"pw\", \"__count\": 12}, {\"value\": -4.711, \"variable\": \"pw\", \"__count\": 12}, {\"value\": 5.09, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.523, \"variable\": \"pw\", \"__count\": 13}, {\"value\": 5.184, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 4.641, \"variable\": \"pw\", \"__count\": 9}, {\"value\": 4.75, \"variable\": \"pw\", \"__count\": 13}, {\"value\": 4.785, \"variable\": \"pw\", \"__count\": 10}, {\"value\": 4.855, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 4.824, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.68, \"variable\": \"pw\", \"__count\": 7}, {\"value\": 4.883, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.508, \"variable\": \"pw\", \"__count\": 10}, {\"value\": 4.648, \"variable\": \"pw\", \"__count\": 7}, {\"value\": 4.664, \"variable\": \"pw\", \"__count\": 9}, {\"value\": 4.805, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.773, \"variable\": \"pw\", \"__count\": 7}, {\"value\": 4.668, \"variable\": \"pw\", \"__count\": 10}, {\"value\": 4.902, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 5.012, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 4.656, \"variable\": \"pw\", \"__count\": 6}, {\"value\": 4.777, \"variable\": \"pw\", \"__count\": 3}, {\"value\": 4.828, \"variable\": \"pw\", \"__count\": 3}, {\"value\": 4.82, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.547, \"variable\": \"pw\", \"__count\": 12}, {\"value\": 4.594, \"variable\": \"pw\", \"__count\": 12}, {\"value\": 4.961, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.938, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 5.074, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 4.914, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 5.281, \"variable\": \"pw\", \"__count\": 3}, {\"value\": 4.953, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.867, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 5.234, \"variable\": \"pw\", \"__count\": 2}, {\"value\": 4.879, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 5.219, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 4.789, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 5.227, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 5.035, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 5.125, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 5.02, \"variable\": \"pw\", \"__count\": 1}, {\"value\": 4.77, \"variable\": \"pw\", \"__count\": 1}]}, {\"name\": \"column_domain\", \"values\": [{\"variable\": \"ipd\", \"count\": 477}, {\"variable\": \"pw\", \"count\": 418}]}, {\"name\": \"source_0_x_domain_value\", \"values\": [{\"min\": -6.133, \"max\": 5.281}]}, {\"name\": \"source_0_y_domain___count\", \"values\": [{\"min\": 1, \"max\": 1491630}]}], \"signals\": [{\"name\": \"child_width\", \"value\": 400}, {\"name\": \"child_height\", \"value\": 400}], \"marks\": [{\"type\": \"group\", \"name\": \"column-title\", \"title\": {\"text\": \"variable\", \"offset\": 10, \"style\": \"guide-title\"}, \"role\": \"column-title\"}, {\"type\": \"group\", \"name\": \"row_header\", \"encode\": {\"update\": {\"height\": {\"signal\": \"child_height\"}}}, \"axes\": [{\"scale\": \"y\", \"tickCount\": {\"signal\": \"ceil(child_height/40)\"}, \"title\": \"count\", \"grid\": false, \"orient\": \"left\", \"labelOverlap\": true, \"zindex\": 0}], \"role\": \"row-header\"}, {\"type\": \"group\", \"name\": \"column_header\", \"from\": {\"data\": \"column_domain\"}, \"sort\": {\"field\": \"datum[\\\"variable\\\"]\", \"order\": \"ascending\"}, \"encode\": {\"update\": {\"width\": {\"signal\": \"child_width\"}}}, \"title\": {\"text\": {\"signal\": \"isValid(parent[\\\"variable\\\"]) ? parent[\\\"variable\\\"] : \\\"\\\"+parent[\\\"variable\\\"]\"}, \"style\": \"guide-label\", \"offset\": 10, \"frame\": \"group\"}, \"role\": \"column-header\"}, {\"type\": \"group\", \"name\": \"column_footer\", \"from\": {\"data\": \"column_domain\"}, \"sort\": {\"field\": \"datum[\\\"variable\\\"]\", \"order\": \"ascending\"}, \"encode\": {\"update\": {\"width\": {\"signal\": \"child_width\"}}}, \"axes\": [{\"scale\": \"x\", \"title\": \"normalized zmw frames\", \"labelFlush\": true, \"labelOverlap\": true, \"orient\": \"bottom\", \"zindex\": 0, \"tickCount\": {\"signal\": \"ceil(child_width/40)\"}, \"grid\": false}], \"role\": \"column-footer\"}, {\"type\": \"group\", \"name\": \"cell\", \"from\": {\"facet\": {\"data\": \"source_0\", \"name\": \"facet\", \"groupby\": [\"variable\"]}}, \"sort\": {\"field\": [\"datum[\\\"variable\\\"]\"], \"order\": [\"ascending\"]}, \"encode\": {\"update\": {\"width\": {\"signal\": \"child_width\"}, \"height\": {\"signal\": \"child_height\"}}}, \"marks\": [{\"type\": \"rect\", \"name\": \"child_marks\", \"from\": {\"data\": \"facet\"}, \"encode\": {\"update\": {\"fill\": {\"value\": \"#4c78a8\"}, \"width\": {\"value\": 5}, \"xc\": {\"field\": \"value\", \"scale\": \"x\"}, \"y\": {\"field\": \"__count\", \"scale\": \"y\"}, \"y2\": {\"value\": 0, \"scale\": \"y\"}}}, \"style\": [\"bar\"]}], \"axes\": [{\"scale\": \"x\", \"maxExtent\": 0, \"minExtent\": 0, \"aria\": false, \"grid\": true, \"tickCount\": {\"signal\": \"ceil(child_width/40)\"}, \"ticks\": false, \"labels\": false, \"zindex\": 0, \"domain\": false, \"orient\": \"bottom\", \"gridScale\": \"y\"}, {\"scale\": \"y\", \"zindex\": 0, \"grid\": true, \"gridScale\": \"x\", \"tickCount\": {\"signal\": \"ceil(child_height/40)\"}, \"domain\": false, \"minExtent\": 0, \"orient\": \"left\", \"aria\": false, \"ticks\": false, \"maxExtent\": 0, \"labels\": false}], \"style\": \"cell\"}], \"scales\": [{\"name\": \"x\", \"type\": \"linear\", \"domain\": [{\"signal\": \"(data(\\\"source_0_x_domain_value\\\")[0] || {}).min\"}, {\"signal\": \"(data(\\\"source_0_x_domain_value\\\")[0] || {}).max\"}], \"range\": [0, {\"signal\": \"child_width\"}], \"nice\": true, \"padding\": 5, \"zero\": false}, {\"name\": \"y\", \"type\": \"linear\", \"domain\": [{\"signal\": \"(data(\\\"source_0_y_domain___count\\\")[0] || {}).min\"}, {\"signal\": \"(data(\\\"source_0_y_domain___count\\\")[0] || {}).max\"}], \"range\": [{\"signal\": \"child_height\"}, 0], \"zero\": true, \"nice\": true}], \"title\": {\"text\": \"Memmap Kinetics Distributions\", \"anchor\": \"start\"}, \"layout\": {\"padding\": 20, \"offset\": {\"columnTitle\": 10}, \"columns\": {\"signal\": \"length(data('column_domain'))\"}, \"bounds\": \"full\", \"align\": \"all\"}, \"padding\": 5, \"background\": \"white\"}, {\"mode\": \"vega\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.FacetChart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glimpse downstream data"
      ],
      "metadata": {
        "id": "gBcyQeb_mOfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our current downstream dataset. Let's look at the first 10 samples. Note how each row is both a forward and reverse sample, and the features are not normalized"
      ],
      "metadata": {
        "id": "CDAkB0hPm5N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_train = pl.read_parquet('pacbio_standard_train_1m.parquet')\n",
        "df_train.head(10)"
      ],
      "metadata": {
        "id": "wI65JvaZTqS2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "c4d86dab-cce9-4779-d2c1-87bc81242866"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (10, 10)\n",
              "┌────────────┬────────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬───────┐\n",
              "│ read_name  ┆ cg_pos ┆ seq        ┆ qual       ┆ … ┆ fp         ┆ ri         ┆ rp         ┆ label │\n",
              "│ ---        ┆ ---    ┆ ---        ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---   │\n",
              "│ str        ┆ i64    ┆ str        ┆ list[u8]   ┆   ┆ list[u16]  ┆ list[u16]  ┆ list[u16]  ┆ i32   │\n",
              "╞════════════╪════════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪═══════╡\n",
              "│ m64168_200 ┆ 3058   ┆ GATGTCCTGG ┆ [60, 67, … ┆ … ┆ [7, 19, …  ┆ [10, 10, … ┆ [34, 39, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ GGATTCGGGG ┆ 69]        ┆   ┆ 23]        ┆ 5]         ┆ 33]        ┆       │\n",
              "│ /48169889/ ┆        ┆ GCATAACTGC ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 8167   ┆ TCTCCACGTT ┆ [93, 73, … ┆ … ┆ [20, 46, … ┆ [48, 70, … ┆ [20, 16, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ GGCCACGCTG ┆ 93]        ┆   ┆ 27]        ┆ 20]        ┆ 51]        ┆       │\n",
              "│ /45943110/ ┆        ┆ GTCTCGAACT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 1413   ┆ AATTTCTTGA ┆ [93, 93, … ┆ … ┆ [21, 9, …  ┆ [16, 17, … ┆ [16, 23, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ AGAGACGAAA ┆ 93]        ┆   ┆ 13]        ┆ 13]        ┆ 22]        ┆       │\n",
              "│ /50332760/ ┆        ┆ GTCTGTGGGT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 4708   ┆ CAACCCACTG ┆ [93, 82, … ┆ … ┆ [23, 14, … ┆ [9, 21, …  ┆ [21, 13, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ CCAAGCGCTT ┆ 93]        ┆   ┆ 34]        ┆ 31]        ┆ 34]        ┆       │\n",
              "│ /177537981 ┆        ┆ CCTGCCACCT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 5695   ┆ CCTCCCTACC ┆ [13, 58, … ┆ … ┆ [12, 34, … ┆ [17, 13, … ┆ [24, 12, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ GAAAACGGGG ┆ 53]        ┆   ┆ 19]        ┆ 20]        ┆ 43]        ┆       │\n",
              "│ /49154585/ ┆        ┆ ATCGTGTGAA ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 6320   ┆ ATGCAATCAA ┆ [93, 93, … ┆ … ┆ [25, 12, … ┆ [28, 18, … ┆ [14, 25, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ CCTAACGTAA ┆ 93]        ┆   ┆ 15]        ┆ 21]        ┆ 21]        ┆       │\n",
              "│ /163316698 ┆        ┆ GTGCTCTCAC ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 2902   ┆ CACCATGCCT ┆ [93, 93, … ┆ … ┆ [23, 11, … ┆ [21, 13, … ┆ [39, 34, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ GGCCACGAGA ┆ 93]        ┆   ┆ 17]        ┆ 23]        ┆ 26]        ┆       │\n",
              "│ /4260355/c ┆        ┆ CCCCATCTCA ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 5186   ┆ GGCAAGGCCC ┆ [93, 93, … ┆ … ┆ [19, 24, … ┆ [23, 12, … ┆ [17, 29, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ AGGCACGTGG ┆ 93]        ┆   ┆ 29]        ┆ 25]        ┆ 19]        ┆       │\n",
              "│ /71829932/ ┆        ┆ TGCATCTGAA ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 6311   ┆ GAGGGTGGGG ┆ [93, 93, … ┆ … ┆ [32, 38, … ┆ [28, 9, …  ┆ [27, 22, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ GTTAGCGAGT ┆ 93]        ┆   ┆ 17]        ┆ 13]        ┆ 39]        ┆       │\n",
              "│ /50529600/ ┆        ┆ GATAGTGTGG ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 3405   ┆ GCTGGAGTGC ┆ [93, 60, … ┆ … ┆ [28, 50, … ┆ [20, 26, … ┆ [13, 30, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ AGTGACGTGA ┆ 88]        ┆   ┆ 13]        ┆ 32]        ┆ 10]        ┆       │\n",
              "│ /2360276/c ┆        ┆ TCTCGGCTCA ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "└────────────┴────────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴───────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (10, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>read_name</th><th>cg_pos</th><th>seq</th><th>qual</th><th>np</th><th>fi</th><th>fp</th><th>ri</th><th>rp</th><th>label</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>list[u8]</td><td>u8</td><td>list[u16]</td><td>list[u16]</td><td>list[u16]</td><td>list[u16]</td><td>i32</td></tr></thead><tbody><tr><td>&quot;m64168_200820_000733/48169889/…</td><td>3058</td><td>&quot;GATGTCCTGGGGATTCGGGGGCATAACTGC…</td><td>[60, 67, … 69]</td><td>8</td><td>[15, 29, … 35]</td><td>[7, 19, … 23]</td><td>[10, 10, … 5]</td><td>[34, 39, … 33]</td><td>0</td></tr><tr><td>&quot;m64168_200820_000733/45943110/…</td><td>8167</td><td>&quot;TCTCCACGTTGGCCACGCTGGTCTCGAACT…</td><td>[93, 73, … 93]</td><td>13</td><td>[33, 18, … 26]</td><td>[20, 46, … 27]</td><td>[48, 70, … 20]</td><td>[20, 16, … 51]</td><td>0</td></tr><tr><td>&quot;m64168_200823_191315/50332760/…</td><td>1413</td><td>&quot;AATTTCTTGAAGAGACGAAAGTCTGTGGGT…</td><td>[93, 93, … 93]</td><td>32</td><td>[32, 12, … 18]</td><td>[21, 9, … 13]</td><td>[16, 17, … 13]</td><td>[16, 23, … 22]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/177537981…</td><td>4708</td><td>&quot;CAACCCACTGCCAAGCGCTTCCTGCCACCT…</td><td>[93, 82, … 93]</td><td>9</td><td>[13, 19, … 19]</td><td>[23, 14, … 34]</td><td>[9, 21, … 31]</td><td>[21, 13, … 34]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/49154585/…</td><td>5695</td><td>&quot;CCTCCCTACCGAAAACGGGGATCGTGTGAA…</td><td>[13, 58, … 53]</td><td>3</td><td>[6, 35, … 10]</td><td>[12, 34, … 19]</td><td>[17, 13, … 20]</td><td>[24, 12, … 43]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/163316698…</td><td>6320</td><td>&quot;ATGCAATCAACCTAACGTAAGTGCTCTCAC…</td><td>[93, 93, … 93]</td><td>24</td><td>[38, 51, … 76]</td><td>[25, 12, … 15]</td><td>[28, 18, … 21]</td><td>[14, 25, … 21]</td><td>1</td></tr><tr><td>&quot;m64168_200820_000733/4260355/c…</td><td>2902</td><td>&quot;CACCATGCCTGGCCACGAGACCCCATCTCA…</td><td>[93, 93, … 93]</td><td>28</td><td>[11, 8, … 10]</td><td>[23, 11, … 17]</td><td>[21, 13, … 23]</td><td>[39, 34, … 26]</td><td>0</td></tr><tr><td>&quot;m64168_200823_191315/71829932/…</td><td>5186</td><td>&quot;GGCAAGGCCCAGGCACGTGGTGCATCTGAA…</td><td>[93, 93, … 93]</td><td>22</td><td>[28, 30, … 18]</td><td>[19, 24, … 29]</td><td>[23, 12, … 25]</td><td>[17, 29, … 19]</td><td>1</td></tr><tr><td>&quot;m64168_200820_000733/50529600/…</td><td>6311</td><td>&quot;GAGGGTGGGGGTTAGCGAGTGATAGTGTGG…</td><td>[93, 93, … 93]</td><td>15</td><td>[13, 8, … 12]</td><td>[32, 38, … 17]</td><td>[28, 9, … 13]</td><td>[27, 22, … 39]</td><td>0</td></tr><tr><td>&quot;m64168_200820_000733/2360276/c…</td><td>3405</td><td>&quot;GCTGGAGTGCAGTGACGTGATCTCGGCTCA…</td><td>[93, 60, … 88]</td><td>6</td><td>[26, 32, … 21]</td><td>[28, 50, … 13]</td><td>[20, 26, … 32]</td><td>[13, 30, … 10]</td><td>0</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_val = pl.read_parquet('pacbio_standard_test_1m.parquet')\n",
        "df_val.head(10)"
      ],
      "metadata": {
        "id": "-LyR85Nn2T7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "4373f76e-c1e5-437a-fb1f-3eb624d6e834"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (10, 10)\n",
              "┌────────────┬────────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬───────┐\n",
              "│ read_name  ┆ cg_pos ┆ seq        ┆ qual       ┆ … ┆ fp         ┆ ri         ┆ rp         ┆ label │\n",
              "│ ---        ┆ ---    ┆ ---        ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---   │\n",
              "│ str        ┆ i64    ┆ str        ┆ list[u8]   ┆   ┆ list[u16]  ┆ list[u16]  ┆ list[u16]  ┆ i32   │\n",
              "╞════════════╪════════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪═══════╡\n",
              "│ m64168_200 ┆ 3630   ┆ GGAGTCTCAC ┆ [93, 93, … ┆ … ┆ [43, 41, … ┆ [26, 15, … ┆ [41, 25, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ TCTGTCGCCC ┆ 93]        ┆   ┆ 15]        ┆ 11]        ┆ 22]        ┆       │\n",
              "│ /78053961/ ┆        ┆ AGGCTGGAGC ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 5646   ┆ TGCAGCAACA ┆ [93, 93, … ┆ … ┆ [9, 25, …  ┆ [52, 71, … ┆ [20, 20, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ CATGACGCAT ┆ 93]        ┆   ┆ 18]        ┆ 14]        ┆ 9]         ┆       │\n",
              "│ /151587048 ┆        ┆ TCTAAAATGT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 4396   ┆ ACATTTTTAA ┆ [93, 93, … ┆ … ┆ [21, 24, … ┆ [23, 25, … ┆ [18, 13, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ GTTGCCGTCT ┆ 93]        ┆   ┆ 19]        ┆ 37]        ┆ 21]        ┆       │\n",
              "│ /139657571 ┆        ┆ CTAGGACAAA ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 7207   ┆ GCAGTGGCAT ┆ [93, 93, … ┆ … ┆ [26, 13, … ┆ [15, 22, … ┆ [23, 17, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ GATCTCGGCT ┆ 93]        ┆   ┆ 36]        ┆ 14]        ┆ 21]        ┆       │\n",
              "│ /18940076/ ┆        ┆ CACTGCAACC ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 5185   ┆ GTCTCCAGCA ┆ [93, 93, … ┆ … ┆ [13, 30, … ┆ [27, 8, …  ┆ [26, 13, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ CCCAGCGCTC ┆ 93]        ┆   ┆ 41]        ┆ 16]        ┆ 15]        ┆       │\n",
              "│ /76875948/ ┆        ┆ CCACAAGCCT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 2983   ┆ AGTTCTTGCC ┆ [93, 93, … ┆ … ┆ [13, 14, … ┆ [24, 30, … ┆ [23, 16, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ TAGCTCGACC ┆ 93]        ┆   ┆ 30]        ┆ 11]        ┆ 15]        ┆       │\n",
              "│ /170133346 ┆        ┆ TCAGTCCCGT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 5830   ┆ GGGCGCGGTG ┆ [31, 93, … ┆ … ┆ [22, 13, … ┆ [23, 31, … ┆ [18, 15, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ GCTCACGCCT ┆ 93]        ┆   ┆ 13]        ┆ 56]        ┆ 26]        ┆       │\n",
              "│ /148045839 ┆        ┆ GTAATCCCAG ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 267    ┆ TAAGTTTCTA ┆ [93, 93, … ┆ … ┆ [19, 31, … ┆ [17, 17, … ┆ [40, 33, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ GTAACCGTAT ┆ 93]        ┆   ┆ 40]        ┆ 23]        ┆ 22]        ┆       │\n",
              "│ /18220356/ ┆        ┆ TAAAAAGTAA ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 4642   ┆ GAGGTTGTGG ┆ [93, 93, … ┆ … ┆ [20, 19, … ┆ [19, 13, … ┆ [31, 21, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ TAAGCCGAGA ┆ 93]        ┆   ┆ 18]        ┆ 14]        ┆ 23]        ┆       │\n",
              "│ /6751166/c ┆        ┆ TCGCGCCATT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 399    ┆ CTGGGTGTGG ┆ [93, 93, … ┆ … ┆ [7, 9, …   ┆ [23, 16, … ┆ [45, 10, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ TGGCACGTGC ┆ 93]        ┆   ┆ 14]        ┆ 13]        ┆ 11]        ┆       │\n",
              "│ /45418455/ ┆        ┆ CTGTAATCTC ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "└────────────┴────────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴───────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (10, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>read_name</th><th>cg_pos</th><th>seq</th><th>qual</th><th>np</th><th>fi</th><th>fp</th><th>ri</th><th>rp</th><th>label</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>list[u8]</td><td>u8</td><td>list[u16]</td><td>list[u16]</td><td>list[u16]</td><td>list[u16]</td><td>i32</td></tr></thead><tbody><tr><td>&quot;m64168_200823_191315/78053961/…</td><td>3630</td><td>&quot;GGAGTCTCACTCTGTCGCCCAGGCTGGAGC…</td><td>[93, 93, … 93]</td><td>34</td><td>[20, 22, … 17]</td><td>[43, 41, … 15]</td><td>[26, 15, … 11]</td><td>[41, 25, … 22]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/151587048…</td><td>5646</td><td>&quot;TGCAGCAACACATGACGCATTCTAAAATGT…</td><td>[93, 93, … 93]</td><td>14</td><td>[26, 37, … 19]</td><td>[9, 25, … 18]</td><td>[52, 71, … 14]</td><td>[20, 20, … 9]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/139657571…</td><td>4396</td><td>&quot;ACATTTTTAAGTTGCCGTCTCTAGGACAAA…</td><td>[93, 93, … 93]</td><td>18</td><td>[20, 62, … 22]</td><td>[21, 24, … 19]</td><td>[23, 25, … 37]</td><td>[18, 13, … 21]</td><td>1</td></tr><tr><td>&quot;m64168_200820_000733/18940076/…</td><td>7207</td><td>&quot;GCAGTGGCATGATCTCGGCTCACTGCAACC…</td><td>[93, 93, … 93]</td><td>27</td><td>[9, 14, … 11]</td><td>[26, 13, … 36]</td><td>[15, 22, … 14]</td><td>[23, 17, … 21]</td><td>0</td></tr><tr><td>&quot;m64168_200823_191315/76875948/…</td><td>5185</td><td>&quot;GTCTCCAGCACCCAGCGCTCCCACAAGCCT…</td><td>[93, 93, … 93]</td><td>17</td><td>[10, 29, … 39]</td><td>[13, 30, … 41]</td><td>[27, 8, … 16]</td><td>[26, 13, … 15]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/170133346…</td><td>2983</td><td>&quot;AGTTCTTGCCTAGCTCGACCTCAGTCCCGT…</td><td>[93, 93, … 93]</td><td>26</td><td>[13, 15, … 28]</td><td>[13, 14, … 30]</td><td>[24, 30, … 11]</td><td>[23, 16, … 15]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/148045839…</td><td>5830</td><td>&quot;GGGCGCGGTGGCTCACGCCTGTAATCCCAG…</td><td>[31, 93, … 93]</td><td>22</td><td>[18, 14, … 11]</td><td>[22, 13, … 13]</td><td>[23, 31, … 56]</td><td>[18, 15, … 26]</td><td>1</td></tr><tr><td>&quot;m64168_200820_000733/18220356/…</td><td>267</td><td>&quot;TAAGTTTCTAGTAACCGTATTAAAAAGTAA…</td><td>[93, 93, … 93]</td><td>22</td><td>[16, 18, … 46]</td><td>[19, 31, … 40]</td><td>[17, 17, … 23]</td><td>[40, 33, … 22]</td><td>0</td></tr><tr><td>&quot;m64168_200820_000733/6751166/c…</td><td>4642</td><td>&quot;GAGGTTGTGGTAAGCCGAGATCGCGCCATT…</td><td>[93, 93, … 93]</td><td>20</td><td>[17, 16, … 30]</td><td>[20, 19, … 18]</td><td>[19, 13, … 14]</td><td>[31, 21, … 23]</td><td>0</td></tr><tr><td>&quot;m64168_200820_000733/45418455/…</td><td>399</td><td>&quot;CTGGGTGTGGTGGCACGTGCCTGTAATCTC…</td><td>[93, 93, … 93]</td><td>10</td><td>[28, 43, … 6]</td><td>[7, 9, … 14]</td><td>[23, 16, … 13]</td><td>[45, 10, … 11]</td><td>0</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SSL Dataset Class"
      ],
      "metadata": {
        "id": "tf09N6FHa-VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each \"shard\" is a 512 MB numpy array, and so with this truncated datset we have around 15 GB of data. Based on the test below, it looks like we can transfer that at a rate well over 1 GB/s to the machine."
      ],
      "metadata": {
        "id": "uMi5jNA54V08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import glob\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class ShardedMemmapDataset(Dataset):\n",
        "    def __init__(self, data_dir, cache_size=100):\n",
        "        expanded_dir = os.path.expandvars(data_dir)\n",
        "        self.shard_paths = sorted(glob.glob(os.path.join(expanded_dir, \"*.npy\")))\n",
        "        first_shard = np.load(self.shard_paths[0], mmap_mode='r')\n",
        "        self.shard_size = first_shard.shape[0]\n",
        "        last_shard = np.load(self.shard_paths[-1], mmap_mode='r')\n",
        "        self.total_len = ((len(self.shard_paths) - 1) * self.shard_size) + last_shard.shape[0]\n",
        "        self.cache_size = cache_size\n",
        "        self.memmaps = OrderedDict()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        shard_idx = idx // self.shard_size\n",
        "        local_idx = idx % self.shard_size\n",
        "        if shard_idx not in self.memmaps:\n",
        "            if len(self.memmaps) >= self.cache_size:\n",
        "                self.memmaps.popitem(last=False)\n",
        "            self.memmaps[shard_idx] = np.load(self.shard_paths[shard_idx], mmap_mode='r')\n",
        "        else:\n",
        "            self.memmaps.move_to_end(shard_idx)\n",
        "        return torch.from_numpy(np.array(self.memmaps[shard_idx][local_idx])).bfloat16()"
      ],
      "metadata": {
        "id": "4-3Bear63rpT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# ssl_ds = ShardedMemmapDataset(\"ob007.memmap/\")\n",
        "# ssl_dl = DataLoader(ssl_ds, batch_size=256, num_workers=4, pin_memory=True, prefetch_factor=2, shuffle=True)\n",
        "\n",
        "# for batch in iter(tqdm(ssl_dl)):\n",
        "#   x = batch.to(device)\n"
      ],
      "metadata": {
        "id": "yMo2TJ8J3vIp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "3UiN1hteGGfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SEQ_LEN = 4096\n",
        "BATCH_SIZE = 64\n",
        "D_MODEL = 128\n",
        "\n",
        "ssl_ds = ShardedMemmapDataset(\"ob007.memmap/\")\n",
        "ssl_dl = DataLoader(ssl_ds, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, prefetch_factor=2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "iqXxINcbGGKK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Smrt2Vec"
      ],
      "metadata": {
        "id": "gwsQpOMcbHbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataflow Plan\n",
        "### Embed data: [B, T, C] -> [B, T, d_model] = E\n",
        "Since we have 1 categorical channel and 2 continuous channels we'll use a hybrid embedding. The nucleotide channel gets an embedding table, and the 2 continuous kinetics channels get a single linear projection with a GeLU nonlinearity. Note the continuous channels are normalized across the genome to have 0 mean, unit variance. GeLU is important for this since it allows negative values...\n",
        "\n",
        "### Extract features: [B, T, d_model], Pad -> [B, T', d_model], [B, T', 1] = Z, Pad'\n",
        "Separate out the padding channel. Runn a CNN over the sequence to generate a new sequence with features. Calculate the new padding mask based on the the CNN downsampling stride.\n",
        "\n",
        "### Mask random indices: [B, T', d_model], Pad' -> [B, T', d_model] = Z_masked, Mask_idx\n",
        "We mask the output of the CNN at randomly sampled indices (say 5 percent of them) and then replace a window (say 5 indices) starting at that index with the learnable padding vector (d_model) such that the sequence length remains the same as the output of the CNN.\n",
        "\n",
        "### Positional encoding: [B, T', d_model] -> [B, T', d_model]\n",
        "Only add the positional encoding at this point since the CNN and addition of masking vectors would overwrite its information otherwise\n",
        "\n",
        "### Transformer block: [B, T', d_model], Pad' -> [B, T', d_model]\n",
        "Run through a series of transformer blocks to get contextualized embeddings\n",
        "\n",
        "### Compute contrastive loss: [B, T', d_model], Mask_idx -> Loss\n",
        "Using the masked indices, use each C_t from the transformer output to predict the latent embedding. We will use an MLP for this transformation, and a separate one for the targets, and I suspect a smaller space than d_model will perform better (say 32 instead of 128). Score the prediction with infoNCE, so how much more similar is the predicted embedding vector to the true target at the position (which we retained) in comparison to a set of randomly sampled indices from the batch.\n"
      ],
      "metadata": {
        "id": "FpuXfCr9y9GA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building Blocks"
      ],
      "metadata": {
        "id": "xJRcCFExbU-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=4096):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.bfloat16).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, d_model, expansion=4):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(d_model, d_model * expansion)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(d_model * expansion, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class SmrtEmbedding(nn.Module):\n",
        "  def __init__(self, d_model, n_nucleotides=5, n_continuous=2):\n",
        "    super().__init__()\n",
        "    self.nuc_embed = nn.Embedding(n_nucleotides, d_model//2)\n",
        "    self.kin_embed = nn.Linear(n_continuous, d_model//2, dtype=torch.bfloat16)\n",
        "    self.layernorm = nn.LayerNorm(d_model)\n",
        "    self.d_model = d_model\n",
        "  def forward(self, x_nuc, x_kin, is_padding):\n",
        "    scale = math.sqrt(self.d_model)\n",
        "    seq_emb = self.nuc_embed(x_nuc.int())*scale\n",
        "    kin_emb = self.kin_embed(x_kin)*scale\n",
        "    x = torch.concat((seq_emb,kin_emb),dim=-1)\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "class BidirectionalSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_head=4, max_len=4096):\n",
        "        super().__init__()\n",
        "        assert d_model % n_head == 0\n",
        "        self.n_head = n_head\n",
        "        self.head_dim = d_model // n_head\n",
        "        # produces qkv, so we output 3*d_model\n",
        "        self.c_attn = nn.Linear(d_model, 3 * d_model, bias=False)\n",
        "        self.c_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x, x_pad, pad_val=1):\n",
        "        B, T, C = x.size()\n",
        "        # use one big matmul and split\n",
        "        qkv = self.c_attn(x).view(B, T, 3, self.n_head, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4) # -> (3, B, n_head, T, head_dim)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2] # -> 3 x (B, n_head, T, head_dim)\n",
        "\n",
        "        # F.scaled_dot_product_attention expects the padding mask s.t.:\n",
        "        # --- True: Attend, False: Ignore ---\n",
        "        # We are committing to the fact that our mask is True for padded\n",
        "        # sequences, so we need to invert it here\n",
        "        # Also, we want to broadcast across the head and query dims\n",
        "        # Given alignment right to left, we need to reshape to match B,H,T,T\n",
        "        attn_mask = ~x_pad.view(B, 1, 1, T)\n",
        "        output = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            attn_mask=attn_mask,\n",
        "            dropout_p=0.0 if not self.training else 0.05,\n",
        "            is_causal=False # since we attend to everything outside the att_mask\n",
        "        )\n",
        "\n",
        "        output = output.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.c_proj(output)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_head, max_len):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = BidirectionalSelfAttention(d_model, n_head, max_len)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.mlp = MLP(d_model)\n",
        "\n",
        "    def forward(self, x, x_pad): # includes unscaled residuals\n",
        "        x = x + self.attn(self.ln1(x), x_pad)\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
        "    super(ResBlock, self).__init__()\n",
        "\n",
        "    self.padding = (kernel_size - 1) // 2\n",
        "    self.kernel_size = kernel_size\n",
        "\n",
        "    self.bn1 = nn.BatchNorm1d(in_channels)\n",
        "    self.conv1 = nn.Conv1d(in_channels=in_channels,\n",
        "                           out_channels=out_channels,\n",
        "                           kernel_size=kernel_size,\n",
        "                           stride=stride,\n",
        "                           padding=self.padding,\n",
        "                           bias=False)\n",
        "    self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "    self.conv2 = nn.Conv1d(in_channels=out_channels,\n",
        "                           out_channels=out_channels,\n",
        "                           kernel_size=kernel_size,\n",
        "                           stride=1,\n",
        "                           padding=self.padding,\n",
        "                           bias=False)\n",
        "\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.stride = stride\n",
        "    # projection residual\n",
        "    if any([in_channels != out_channels, stride != 1]):\n",
        "      self.residual = nn.Sequential(\n",
        "          nn.Conv1d(in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=1, stride=stride,\n",
        "                    bias=False)\n",
        "          )\n",
        "    # identity residual\n",
        "    else:\n",
        "      self.residual = nn.Sequential()\n",
        "  def _resize_mask(self, mask, pad_val=1):\n",
        "    if mask.dtype == torch.bool:\n",
        "      mask = mask.float()\n",
        "    if pad_val == 0:\n",
        "      mask = F.max_pool1d(mask,\n",
        "                          kernel_size=self.kernel_size,\n",
        "                          stride=self.stride,\n",
        "                          padding=self.padding)\n",
        "    elif pad_val == 1:\n",
        "      mask = 1 - F.max_pool1d(1 - mask,\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              stride=self.stride,\n",
        "                              padding=self.padding)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid pad value: Pad value must be 0 or 1\")\n",
        "    return mask.bool()\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    out = self.relu(self.bn1(x))\n",
        "    out = self.conv1(out)\n",
        "    out = self.relu(self.bn2(out))\n",
        "    out = self.conv2(out)\n",
        "    out += self.residual(x)\n",
        "    mask = self._resize_mask(mask)\n",
        "    return out, mask\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, d_model, max_len, dropout_p):\n",
        "    super().__init__()\n",
        "    self.max_len = max_len\n",
        "    self.in_channels = d_model\n",
        "    # extractor\n",
        "    self.extractor = nn.ModuleList([\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=7),            # (B, C, T)   -> (B, C, T)\n",
        "\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T)   -> (B, C, T)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T)   -> (B, C, T)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T)   -> (B, C, T)\n",
        "\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3, stride=2),  # (B, C, T)   -> (B, C, T/2)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/2)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/2)\n",
        "\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3, stride=1),  # (B, C, T/2) -> (B, C, T/4)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/4)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/4)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/4)\n",
        "          ])\n",
        "    self.dropout = nn.Dropout(p=dropout_p)\n",
        "    # calculate fc layer input with dummy passthrough\n",
        "    self.output_shapes = self._get_output_shape()\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for block in self.extractor:\n",
        "      x, mask= block(x,mask)\n",
        "    return x, mask\n",
        "\n",
        "  def _get_output_shape(self):\n",
        "      \"\"\"\n",
        "      Returns output shapes for the data and mask\n",
        "      \"\"\"\n",
        "      dummy_x = torch.randn(1, self.in_channels, self.max_len)\n",
        "      dummy_mask = torch.randn(1, self.max_len)\n",
        "\n",
        "      # get outputshapes\n",
        "      output, mask = self.forward(dummy_x, dummy_mask)\n",
        "      return output.shape, mask.shape\n"
      ],
      "metadata": {
        "id": "h2DpvvAbBTgP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Model Classes"
      ],
      "metadata": {
        "id": "FznAftiOasie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Encoder Class\n",
        "\n",
        "class SmrtEncoder(nn.Module):\n",
        "  def __init__(self, d_model=128, n_layers=4, n_head=4, max_len=4096, dropout_p=0.01):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embed = SmrtEmbedding(d_model)\n",
        "    self.pe = PositionalEncoding(d_model, max_len=max_len)\n",
        "    self.downsample = CNN(d_model, max_len=max_len, dropout_p=dropout_p)\n",
        "    self.layer_norm_target = nn.LayerNorm(d_model)\n",
        "    self.blocks = nn.ModuleList([\n",
        "        TransformerBlock(d_model=d_model, n_head=n_head, max_len=max_len) for _ in range(n_layers)\n",
        "        ])\n",
        "  def get_latents(self, x):\n",
        "    \"\"\"\n",
        "    Runs [x -> Embedding -> CNN -> out] stack (for training)\n",
        "    Returns:\n",
        "      z (downsampled latents with PE)\n",
        "      z_pad (dowsampled padding mask)\n",
        "      targets (latents without PE)\n",
        "    \"\"\"\n",
        "    # separate into features and padding\n",
        "    x_nuc = x[...,0]\n",
        "    x_kin = x[...,1:3]\n",
        "    x_pad = x[...,3]\n",
        "    # generate hybrid embedding\n",
        "    x = self.embed(x_nuc, x_kin, x_pad)\n",
        "    # featurize the emmbeddings (cnn expect BCT)\n",
        "    z, z_pad = self.downsample(x.permute(0,2,1), x_pad)\n",
        "    # permute back to BTC\n",
        "    z = z.permute(0,2,1)\n",
        "    targets = self.layer_norm_target(z.clone())\n",
        "    return z, z_pad, targets\n",
        "\n",
        "  def add_pe(self, z):\n",
        "      return self.pe(z)\n",
        "\n",
        "  def forward_transformer(self, z, z_pad):\n",
        "    \"\"\"\n",
        "    Runs the transformer blocks on the downsampled latents\n",
        "    Returns:\n",
        "      c (context aware latents)\n",
        "    \"\"\"\n",
        "    c = z\n",
        "    for block in self.blocks:\n",
        "      c = block(c, z_pad)\n",
        "    return c\n",
        "  def forward(self, x):\n",
        "    z, z_pad, _ = self.get_latents(x)\n",
        "    z = self.add_pe(z)\n",
        "    c = self.forward_transformer(z, z_pad)\n",
        "    return c\n",
        "\n",
        "### Main Model\n",
        "class Smrt2Vec(nn.Module):\n",
        "  def __init__(self, d_model=128, n_layers=4, n_head=4, max_len=4096):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.encoder = SmrtEncoder(d_model, n_layers, n_head, max_len)\n",
        "\n",
        "    # components specific to pretraining\n",
        "    self.mask_vec = nn.Parameter(torch.randn(d_model))\n",
        "    self.project =  nn.Sequential(\n",
        "        nn.Linear(d_model, d_model),\n",
        "        nn.GELU(), # avoid negative values being ignored with ReLU\n",
        "        nn.Linear(d_model, d_model)\n",
        "        )\n",
        "  def apply_mask(self, x_emb, pad, prob=0.05, size=6):\n",
        "    B, T, C = x_emb.shape\n",
        "    mask_idx_centers = (torch.rand(B, T, device=x_emb.device) < prob) & ~(pad.bool())\n",
        "    mask_idx_full = F.max_pool1d(\n",
        "        mask_idx_centers.bfloat16(),\n",
        "        kernel_size=size, stride=1, # hyperparameter here...\n",
        "        padding=size//2\n",
        "      ).bool()[:, :T] & (~pad.bool())\n",
        "    x_masked = x_emb.clone()\n",
        "    x_masked[mask_idx_full] = self.mask_vec.to(dtype=x_emb.dtype, device=x_emb.device)\n",
        "    return x_masked, mask_idx_full\n",
        "  def forward(self, x):\n",
        "    # dowsampled latents with pe (no transormer block yet)\n",
        "    z, z_pad, targets = self.encoder.get_latents(x)\n",
        "    # mask indices for loss\n",
        "    z_masked, z_masked_bool = self.apply_mask(z, z_pad)\n",
        "    z_masked_pe = self.encoder.add_pe(z_masked)\n",
        "    # run through transformer\n",
        "    c = self.encoder.forward_transformer(z_masked_pe, z_pad)\n",
        "    # project\n",
        "    c_proj = self.project(c)\n",
        "    return c_proj, targets.detach(), z_masked_bool # projected transformer output, detached unmasked downsampled latents (not transfomer applied), boolean matrix of where the targets are\n",
        "\n",
        "### Loss\n",
        "\n",
        "class InfoNCELoss(nn.Module):\n",
        "  def __init__(self, temperature=0.1):\n",
        "    super().__init__()\n",
        "    self.cross_entropy = nn.CrossEntropyLoss()\n",
        "    self.temperature = temperature\n",
        "  def forward(self, c_proj, targets, mask_idx):\n",
        "    # gather the predictions and truth vectors\n",
        "    preds = c_proj[mask_idx]\n",
        "    truth = targets[mask_idx]\n",
        "    # normalize for cosine similarity\n",
        "    # last dim (embedding dim)\n",
        "    preds = F.normalize(preds, dim=-1)\n",
        "    truth = F.normalize(truth, dim=-1)\n",
        "    # print(truth.shape,preds.shape)\n",
        "    logits = torch.mm(preds, truth.permute(1,0)) / self.temperature\n",
        "    labels = torch.arange(truth.shape[0], device=truth.device)\n",
        "    loss = self.cross_entropy(logits, labels)\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0-czBfcm7dmS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model forward pass check"
      ],
      "metadata": {
        "id": "1gOoZWXbbjoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "batch = next(iter(ssl_dl)).to(device)\n",
        "model = Smrt2Vec().to(device)\n",
        "c_proj, targets, mask = model(batch)\n",
        "print((c_proj.shape, targets.shape, mask.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bhhVe6vIkfn",
        "outputId": "adfd2d68-456d-4ace-b14d-b025485d3973"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(torch.Size([64, 2048, 128]), torch.Size([64, 2048, 128]), torch.Size([64, 2048]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = InfoNCELoss()\n",
        "loss(c_proj, targets, mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpflfHgtWAvT",
        "outputId": "2e007dd3-4cb9-407f-e91c-a6c101357f6f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.5058, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Smrt2Vec().to(device)\n",
        "model.train()\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"trainable params: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSQE0CHcCv-1",
        "outputId": "ab031350-fc9a-4745-8c41-1917676591b8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2059648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Training"
      ],
      "metadata": {
        "id": "DMIzPGZYekWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.02)\n",
        "criterion = InfoNCELoss(temperature=0.1).to(device)\n",
        "EPOCHS = 2\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=6e-4,\n",
        "    total_steps=len(ssl_dl) * EPOCHS,\n",
        "    pct_start=0.05\n",
        ")\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    progress_bar = tqdm(ssl_dl, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for i, batch in enumerate(progress_bar):\n",
        "        batch = batch.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "            c_proj, targets, mask_idx = model(batch)\n",
        "            loss = criterion(c_proj, targets, mask_idx)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            progress_bar.set_postfix(\n",
        "                loss=f\"{loss.item():.4f}\",\n",
        "                lr=f\"{scheduler.get_last_lr()[0]:.6f}\"\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fok1kxgAyvP5",
        "outputId": "fa845eeb-c004-4cc2-8429-81ce254bed53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:  51%|█████     | 4077/7976 [13:59<13:24,  4.84it/s, loss=4.2953, lr=0.000533]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downstream Task"
      ],
      "metadata": {
        "id": "2PamTn7KcQkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "Honestly this feels a bit funky, and I'm debating whether to make a new preprocessing script that produces numpy arrays like the SSL dataset. This parquet style dataset is inherited from the CNN a while back and is much more difficult to work with, I find. Also much more difficult to get good bandwidth."
      ],
      "metadata": {
        "id": "1kBnYhgnk9yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import pyarrow.parquet as pq\n",
        "from pathlib import Path\n",
        "from torch.utils.data import IterableDataset\n",
        "def compute_log_normalization_stats(df, features, epsilon=1):\n",
        "    means = {col: (df[col].explode() + epsilon).log().mean() for col in features}\n",
        "    stds = {col: (df[col].explode() + epsilon).log().explode().std() for col in features}\n",
        "    return means, stds\n",
        "\n",
        "class MethylIterableDataset(IterableDataset):\n",
        "    def __init__(self, data_path, means, stds, context, restrict_row_groups=0, single_strand=False, inference=False):\n",
        "        super().__init__()\n",
        "        self.data_path = Path(data_path)\n",
        "        self.means, self.stds = means, stds\n",
        "        self.context = context\n",
        "        self.single_strand = single_strand\n",
        "        self.inference = inference\n",
        "        self.restrict = restrict_row_groups\n",
        "\n",
        "        self.kin_feats = ['fi', 'fp', 'ri', 'rp']\n",
        "        self.vocab = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4}\n",
        "        self.comp_map = torch.tensor([3, 2, 1, 0, 4], dtype=torch.long)\n",
        "\n",
        "        try:\n",
        "            meta = pq.read_metadata(self.data_path)\n",
        "            self.n_groups = meta.num_row_groups\n",
        "            use_groups = min(self.restrict, self.n_groups) if self.restrict else self.n_groups\n",
        "\n",
        "            # fast row count\n",
        "            n_rows = sum(meta.row_group(i).num_rows for i in range(use_groups))\n",
        "            self.len = n_rows * (2 if single_strand else 1)\n",
        "        except Exception:\n",
        "            print(f'Failed to read parquet: {self.data_path}')\n",
        "            self.n_groups, self.len = 0, 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def _process_batch(self, df):\n",
        "      # seq\n",
        "        seq_arr = np.stack(\n",
        "            df['seq'].str.split(\"\")\n",
        "            .list.eval(pl.element().replace_strict(self.vocab, default=4))\n",
        "            .to_numpy()\n",
        "        )\n",
        "        seq_t = torch.tensor(seq_arr, dtype=torch.long)\n",
        "\n",
        "        # kinetics\n",
        "        kin_list = []\n",
        "        for k in self.kin_feats:\n",
        "            vals = df[k].to_numpy() # (N, L)\n",
        "            vals = (np.log(vals + 1) - self.means[k]) / self.stds[k]\n",
        "            kin_list.append(vals)\n",
        "        kin_t = torch.tensor(np.stack(kin_list, axis=1), dtype=torch.bfloat16)\n",
        "\n",
        "        # mask, labels, etc (note that there is no masked data in the downstream set, so it's all zeros here)\n",
        "        mask = torch.zeros((seq_t.shape[0], seq_t.shape[1], 1), dtype=torch.bfloat16)\n",
        "        labels = torch.tensor(df['label'].to_numpy(), dtype=torch.long) if not self.inference else None\n",
        "        r_names, pos = df['read_name'].to_list(), df['cg_pos'].to_list()\n",
        "\n",
        "        # construct forward sample\n",
        "        # Seq (N, L, 1) + Kin (N, 2, L)->(N, L, 2) + Mask (N, L, 1) = (N, L, 4)\n",
        "        fwd_data = torch.cat([\n",
        "            seq_t.unsqueeze(-1).to(torch.bfloat16),\n",
        "            kin_t[:, 0:2].permute(0, 2, 1),\n",
        "            mask\n",
        "        ], dim=2)\n",
        "\n",
        "        # construct reverse data\n",
        "        rev_data = None\n",
        "        if self.single_strand:\n",
        "            rev_seq_t = torch.flip(self.comp_map.to(seq_t.device)[seq_t], dims=[1])\n",
        "            # Kin: slice 2:4, flip time (dim 2), permute channels\n",
        "            rev_kin = torch.flip(kin_t[:, 2:4], dims=[2]).permute(0, 2, 1)\n",
        "            rev_data = torch.cat([\n",
        "                rev_seq_t.unsqueeze(-1).to(torch.bfloat16),\n",
        "                rev_kin,\n",
        "                mask\n",
        "            ], dim=2)\n",
        "\n",
        "        # yield\n",
        "        for i in range(len(df)):\n",
        "            # forward\n",
        "            strand_name = 'fwd' if self.single_strand else 'ds'\n",
        "            item_fwd = {\n",
        "                'data': fwd_data[i],\n",
        "                'metadata': {'read_name': r_names[i], 'position': pos[i], 'strand': strand_name}\n",
        "            }\n",
        "            if labels is not None: item_fwd['label'] = labels[i]\n",
        "            yield item_fwd\n",
        "\n",
        "            # reverse\n",
        "            if rev_data is not None:\n",
        "                item_rev = {\n",
        "                    'data': rev_data[i],\n",
        "                    'metadata': {'read_name': r_names[i], 'position': pos[i], 'strand': 'rev'}\n",
        "                }\n",
        "                if labels is not None: item_rev['label'] = labels[i]\n",
        "                yield item_rev\n",
        "            else:\n",
        "              continue\n",
        "\n",
        "    def __iter__(self):\n",
        "        worker = torch.utils.data.get_worker_info()\n",
        "        valid_groups = min(self.restrict, self.n_groups) if self.restrict else self.n_groups\n",
        "        indices = np.arange(valid_groups)\n",
        "\n",
        "        if worker:\n",
        "            indices = np.array_split(indices, worker.num_workers)[worker.id]\n",
        "\n",
        "        pqf = pq.ParquetFile(self.data_path)\n",
        "        for i in indices:\n",
        "            # array cast\n",
        "            df = pl.from_arrow(pqf.read_row_group(i)).with_columns([\n",
        "                pl.col(c).list.to_array(self.context) for c in self.kin_feats\n",
        "            ])\n",
        "            yield from self._process_batch(df)"
      ],
      "metadata": {
        "id": "PFMN3kbh6UE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KINETICS_FEATURES = ['fi', 'fp', 'ri', 'rp']\n",
        "\n",
        "df = pl.read_parquet('pacbio_standard_train_1m.parquet')\n",
        "train_means, train_stds = compute_log_normalization_stats(df, KINETICS_FEATURES)\n",
        "\n",
        "it_workers=0\n",
        "batch_size=256\n",
        "single_strand=True\n",
        "#train\n",
        "methyl_train_ds = MethylIterableDataset('./pacbio_standard_train_1m.parquet',\n",
        "                                    means=train_means,\n",
        "                                    stds=train_stds,\n",
        "                                    context=32)\n",
        "methyl_train_dl = DataLoader(methyl_train_ds,\n",
        "                             batch_size=batch_size,\n",
        "                             drop_last=True,\n",
        "                             persistent_workers=False,\n",
        "                             prefetch_factor=None,\n",
        "                            )\n",
        "# val\n",
        "methyl_val_ds = MethylIterableDataset('./pacbio_standard_test_1m.parquet',\n",
        "                                    means=train_means,\n",
        "                                    stds=train_stds,\n",
        "                                    context=32)\n",
        "methyl_val_dl = DataLoader(methyl_val_ds,\n",
        "                        batch_size=batch_size,\n",
        "                        drop_last=True,\n",
        "                        persistent_workers=False,\n",
        "                        prefetch_factor=None)"
      ],
      "metadata": {
        "id": "rTB-NjbWRrHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Probe"
      ],
      "metadata": {
        "id": "2Oy3GboOX54S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleIdxProbe(nn.Module):\n",
        "    def __init__(self, encoder, n_classes=1, freeze_encoder=False):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "\n",
        "        if freeze_encoder:\n",
        "          self.encoder.requires_grad_(False)\n",
        "        else:\n",
        "          self.encoder.requires_grad_(True)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(encoder.d_model, encoder.d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoder.d_model // 2, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        c = self.encoder(x)\n",
        "        logit = self.head(c[:, -1, :])\n",
        "        return logit"
      ],
      "metadata": {
        "id": "ekzuqBqfTv_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "# LR = 1e-5\n",
        "EPOCHS = 20\n",
        "DEVICE = torch.device('cuda')\n",
        "encoder_clone = copy.deepcopy(model.encoder)\n",
        "probe = SingleIdxProbe(encoder_clone, freeze_encoder=False).to(device)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(probe.parameters(), lr=LR)\n",
        "\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': probe.encoder.parameters(), 'lr': 5e-7},\n",
        "    {'params': probe.head.parameters(), 'lr': 3e-5}\n",
        "    ])\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "loss_history = []\n",
        "\n",
        "total_params = sum(p.numel() for p in probe.parameters() if p.requires_grad)\n",
        "print(f\"trainable params: {total_params}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    probe.train()\n",
        "    running_loss = 0.0\n",
        "    for i, batch in enumerate(tqdm(methyl_train_dl)):\n",
        "        inputs = batch['data'].to(DEVICE)\n",
        "        labels = batch['label'].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = probe(inputs)\n",
        "        loss = criterion(logits, labels.unsqueeze(1).to(torch.float32))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 100 == 0:\n",
        "            loss_history.append(running_loss / 100)\n",
        "            running_loss = 0.0\n",
        "\n",
        "    probe.eval()\n",
        "    sample_count = 0\n",
        "    sample_correct = 0\n",
        "    for batch in tqdm(methyl_val_dl):\n",
        "        inputs = batch['data'].to(DEVICE)\n",
        "        labels = batch['label'].to(DEVICE)\n",
        "\n",
        "        logits = probe(inputs)\n",
        "        preds = logits > 0\n",
        "        correct = labels == preds.squeeze(-1)\n",
        "        sample_count += correct.shape[0]\n",
        "        sample_correct += correct.sum()\n",
        "    print(f\"epoch val top1_acc: {sample_correct/sample_count}\")"
      ],
      "metadata": {
        "id": "uybV3hRFT-oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pl.DataFrame({'loss': loss_history})\n",
        "def plot_loss(loss_df):\n",
        "  # loss_df_long = loss_df.unpivot(index='stepsx100', value_name='loss')\n",
        "  # min_loss = loss_df_long['loss'].min()\n",
        "  # max_loss = loss_df_long['loss'].max(\n",
        "  loss_df = loss_df.with_row_index()\n",
        "  loss_chart = alt.Chart(loss_df).mark_line().encode(\n",
        "    alt.X('index:Q'),\n",
        "    alt.Y('loss:Q'),\n",
        "  ).properties(\n",
        "    width=700,\n",
        "    height=500,\n",
        "    title = 'Direct Downstream Train Loss'\n",
        "  )\n",
        "  return loss_chart\n",
        "plot_loss(df)"
      ],
      "metadata": {
        "id": "gSUmVjcWeRU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lB0YWA_wZP_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
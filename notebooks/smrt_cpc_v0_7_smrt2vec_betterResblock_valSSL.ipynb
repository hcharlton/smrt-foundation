{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries\n"
      ],
      "metadata": {
        "id": "16QzIa_zmFq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy step"
      ],
      "metadata": {
        "id": "TtxZNa0da4ny"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYGJgwVtMIrb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c786149-e121-4ecd-c3b4-50c6a15f7843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "import torch\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# ssl training data (numpy arrays)\n",
        "# !cp -r /content/gdrive/MyDrive/smrt-foundation/ob007.memmap/ /content/\n",
        "!cp -r /content/gdrive/MyDrive/smrt-foundation/cpg_pos_20.memmap/ /content/\n",
        "# downstream methylation dataset (parquet, tabular)\n",
        "!cp -r /content/gdrive/MyDrive/smrt-foundation/pacbio_standard_train_1m.parquet /content/\n",
        "!cp -r /content/gdrive/MyDrive/smrt-foundation/pacbio_standard_test_1m.parquet /content/\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cpu = torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "!pip install \"vegafusion[embed]>=1.5.0\" \"vl-convert-python>=1.6.0\"\n",
        "sys.path.append('/content/gdrive/MyDrive/smrt-foundation')\n"
      ],
      "metadata": {
        "id": "FLn4nkjnV53P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89ada01-2bb3-448f-a84e-a346e14a22c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vegafusion>=1.5.0 (from vegafusion[embed]>=1.5.0)\n",
            "  Downloading vegafusion-2.0.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting vl-convert-python>=1.6.0\n",
            "  Downloading vl_convert_python-1.9.0.post1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting arro3-core (from vegafusion>=1.5.0->vegafusion[embed]>=1.5.0)\n",
            "  Downloading arro3_core-0.6.5-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (363 bytes)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from vegafusion>=1.5.0->vegafusion[embed]>=1.5.0) (25.0)\n",
            "Requirement already satisfied: narwhals>=1.42 in /usr/local/lib/python3.12/dist-packages (from vegafusion>=1.5.0->vegafusion[embed]>=1.5.0) (2.15.0)\n",
            "Downloading vegafusion-2.0.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m120.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vl_convert_python-1.9.0.post1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arro3_core-0.6.5-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vl-convert-python, arro3-core, vegafusion\n",
            "Successfully installed arro3-core-0.6.5 vegafusion-2.0.3 vl-convert-python-1.9.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glimpse ssl data"
      ],
      "metadata": {
        "id": "icfit_SKmJq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls cpg_pos_20.memmap/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk2b4dOiNRWX",
        "outputId": "c2e91005-f5aa-49b4-dbdf-2e103e8b48e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shard_00010.npy  shard_00015.npy  shard_00020.npy  shard_00025.npy\n",
            "shard_00011.npy  shard_00016.npy  shard_00021.npy  shard_00026.npy\n",
            "shard_00012.npy  shard_00017.npy  shard_00022.npy  shard_00027.npy\n",
            "shard_00013.npy  shard_00018.npy  shard_00023.npy  shard_00028.npy\n",
            "shard_00014.npy  shard_00019.npy  shard_00024.npy  shard_00029.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is our pretraining dataset\n",
        "# take a peak at one of its numpy shards\n",
        "# note how each slice of the array is a single-stranded sample for pretraining\n",
        "# columns are organized [seq, ipd, pw, padding_mask]\n",
        "\n",
        "import numpy as np\n",
        "x = torch.tensor(np.load('./cpg_pos_20.memmap/shard_00020.npy')).to(device)\n",
        "mask = ~x[...,-1].bool()\n",
        "print(x.shape)\n",
        "print(x[0,0:10])"
      ],
      "metadata": {
        "id": "PUg_Q9DXk3vI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c370d9-bea5-4099-b1e0-b496eb238e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16384, 4096, 4])\n",
            "tensor([[2.0000, 3.5547, 3.2188, 0.0000],\n",
            "        [2.0000, 3.2949, 3.4961, 0.0000],\n",
            "        [3.0000, 3.4004, 2.7734, 0.0000],\n",
            "        [0.0000, 2.6387, 2.6387, 0.0000],\n",
            "        [3.0000, 3.2188, 2.9453, 0.0000],\n",
            "        [2.0000, 3.5840, 3.5840, 0.0000],\n",
            "        [3.0000, 3.1348, 2.8906, 0.0000],\n",
            "        [2.0000, 2.4844, 3.3672, 0.0000],\n",
            "        [1.0000, 3.3672, 3.2949, 0.0000],\n",
            "        [0.0000, 2.3984, 2.7734, 0.0000]], device='cuda:0',\n",
            "       dtype=torch.float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first column is seq, center two are features which should have mean 0, sd 1\n",
        "# last column is mask and should be 0's and 1's (mostly 0's)\n",
        "\n",
        "x.mean(dim=(0, 1)).round(decimals=2), x.std(dim=(0,1)).round(decimals=2)"
      ],
      "metadata": {
        "id": "VCwGImgEhTQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the distribution of nucleotides in the flattened seq column\n",
        "# note that in the natural genome it is not a uniform distribution\n",
        "# what we see here after subsetting with the mask matches expectations\n",
        "\n",
        "import altair as alt\n",
        "alt.data_transformers.enable(\"vegafusion\")\n",
        "import polars as pl\n",
        "seq = x[:, :, 0][mask].flatten()\n",
        "seq_df = pl.DataFrame({'seq':seq.to(cpu)})\n",
        "alt.Chart(seq_df).mark_bar(width=70).encode(\n",
        "    alt.X('seq:Q'),\n",
        "    y='count()'\n",
        ")"
      ],
      "metadata": {
        "id": "OLpocjf2h7Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and here we can see the proportions of each nucleotide in tabular format\n",
        "seq = x[:, :, 0][mask.to(cpu)].flatten()\n",
        "seq_df = pl.DataFrame({'seq':seq.to(cpu)})\n",
        "seq_df[\"seq\"].value_counts(sort=True, normalize=True)"
      ],
      "metadata": {
        "id": "jZW27r0HkV8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check that the long transformation doesn't corrupt the floats\n",
        "# if it did, we might see an excess of 0 (A)\n",
        "seq = x[:, :, 0].long()[mask.to(cpu)].flatten()\n",
        "seq_df = pl.DataFrame({'seq':seq.to(cpu)})\n",
        "seq_df[\"seq\"].value_counts(sort=True, normalize=True)"
      ],
      "metadata": {
        "id": "JpWNfSFXwdko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# plot the histograms of the two features\n",
        "cont_df = pl.DataFrame({'ipd':x[...,1][mask].flatten().to(cpu),\n",
        "                        'pw':x[...,2][mask].flatten().to(cpu)})\n",
        "alt.Chart(cont_df.unpivot()).mark_bar().encode(\n",
        "      alt.X('value:Q').title('normalized zmw frames'),\n",
        "      alt.Y('count():Q').scale(type='linear').title('count'),\n",
        "  ).properties(\n",
        "      width=400,\n",
        "      height=400,\n",
        "  ).facet(\n",
        "      column='variable:N'\n",
        "  ).properties(\n",
        "      title=\"Memmap Kinetics Distributions\"\n",
        "  )"
      ],
      "metadata": {
        "id": "kUWzekMmsIjW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "340cad71-6d02-4afd-d1e4-4dc1acd182d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pl' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3833300309.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot the histograms of the two features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m cont_df = pl.DataFrame({'ipd':x[...,1][mask].flatten().to(cpu),\n\u001b[0m\u001b[1;32m      3\u001b[0m                         'pw':x[...,2][mask].flatten().to(cpu)})\n\u001b[1;32m      4\u001b[0m alt.Chart(cont_df.unpivot()).mark_bar().encode(\n\u001b[1;32m      5\u001b[0m       \u001b[0malt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'value:Q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'normalized zmw frames'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glimpse downstream data"
      ],
      "metadata": {
        "id": "gBcyQeb_mOfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our current downstream dataset. Let's look at the first 10 samples. Note how each row is both a forward and reverse sample, and the features are not normalized"
      ],
      "metadata": {
        "id": "CDAkB0hPm5N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_train = pl.read_parquet('pacbio_standard_train_1m.parquet')\n",
        "df_train.head(10)"
      ],
      "metadata": {
        "id": "wI65JvaZTqS2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "945a98a4-0609-4d26-a78d-c59c212c5b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (10, 10)\n",
              "┌────────────┬────────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬───────┐\n",
              "│ read_name  ┆ cg_pos ┆ seq        ┆ qual       ┆ … ┆ fp         ┆ ri         ┆ rp         ┆ label │\n",
              "│ ---        ┆ ---    ┆ ---        ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---   │\n",
              "│ str        ┆ i64    ┆ str        ┆ list[u8]   ┆   ┆ list[u16]  ┆ list[u16]  ┆ list[u16]  ┆ i32   │\n",
              "╞════════════╪════════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪═══════╡\n",
              "│ m64168_200 ┆ 3058   ┆ GATGTCCTGG ┆ [60, 67, … ┆ … ┆ [7, 19, …  ┆ [10, 10, … ┆ [34, 39, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ GGATTCGGGG ┆ 69]        ┆   ┆ 23]        ┆ 5]         ┆ 33]        ┆       │\n",
              "│ /48169889/ ┆        ┆ GCATAACTGC ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 8167   ┆ TCTCCACGTT ┆ [93, 73, … ┆ … ┆ [20, 46, … ┆ [48, 70, … ┆ [20, 16, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ GGCCACGCTG ┆ 93]        ┆   ┆ 27]        ┆ 20]        ┆ 51]        ┆       │\n",
              "│ /45943110/ ┆        ┆ GTCTCGAACT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 1413   ┆ AATTTCTTGA ┆ [93, 93, … ┆ … ┆ [21, 9, …  ┆ [16, 17, … ┆ [16, 23, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ AGAGACGAAA ┆ 93]        ┆   ┆ 13]        ┆ 13]        ┆ 22]        ┆       │\n",
              "│ /50332760/ ┆        ┆ GTCTGTGGGT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 4708   ┆ CAACCCACTG ┆ [93, 82, … ┆ … ┆ [23, 14, … ┆ [9, 21, …  ┆ [21, 13, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ CCAAGCGCTT ┆ 93]        ┆   ┆ 34]        ┆ 31]        ┆ 34]        ┆       │\n",
              "│ /177537981 ┆        ┆ CCTGCCACCT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 5695   ┆ CCTCCCTACC ┆ [13, 58, … ┆ … ┆ [12, 34, … ┆ [17, 13, … ┆ [24, 12, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ GAAAACGGGG ┆ 53]        ┆   ┆ 19]        ┆ 20]        ┆ 43]        ┆       │\n",
              "│ /49154585/ ┆        ┆ ATCGTGTGAA ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 6320   ┆ ATGCAATCAA ┆ [93, 93, … ┆ … ┆ [25, 12, … ┆ [28, 18, … ┆ [14, 25, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ CCTAACGTAA ┆ 93]        ┆   ┆ 15]        ┆ 21]        ┆ 21]        ┆       │\n",
              "│ /163316698 ┆        ┆ GTGCTCTCAC ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 2902   ┆ CACCATGCCT ┆ [93, 93, … ┆ … ┆ [23, 11, … ┆ [21, 13, … ┆ [39, 34, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ GGCCACGAGA ┆ 93]        ┆   ┆ 17]        ┆ 23]        ┆ 26]        ┆       │\n",
              "│ /4260355/c ┆        ┆ CCCCATCTCA ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 5186   ┆ GGCAAGGCCC ┆ [93, 93, … ┆ … ┆ [19, 24, … ┆ [23, 12, … ┆ [17, 29, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ AGGCACGTGG ┆ 93]        ┆   ┆ 29]        ┆ 25]        ┆ 19]        ┆       │\n",
              "│ /71829932/ ┆        ┆ TGCATCTGAA ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 6311   ┆ GAGGGTGGGG ┆ [93, 93, … ┆ … ┆ [32, 38, … ┆ [28, 9, …  ┆ [27, 22, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ GTTAGCGAGT ┆ 93]        ┆   ┆ 17]        ┆ 13]        ┆ 39]        ┆       │\n",
              "│ /50529600/ ┆        ┆ GATAGTGTGG ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 3405   ┆ GCTGGAGTGC ┆ [93, 60, … ┆ … ┆ [28, 50, … ┆ [20, 26, … ┆ [13, 30, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ AGTGACGTGA ┆ 88]        ┆   ┆ 13]        ┆ 32]        ┆ 10]        ┆       │\n",
              "│ /2360276/c ┆        ┆ TCTCGGCTCA ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "└────────────┴────────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴───────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (10, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>read_name</th><th>cg_pos</th><th>seq</th><th>qual</th><th>np</th><th>fi</th><th>fp</th><th>ri</th><th>rp</th><th>label</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>list[u8]</td><td>u8</td><td>list[u16]</td><td>list[u16]</td><td>list[u16]</td><td>list[u16]</td><td>i32</td></tr></thead><tbody><tr><td>&quot;m64168_200820_000733/48169889/…</td><td>3058</td><td>&quot;GATGTCCTGGGGATTCGGGGGCATAACTGC…</td><td>[60, 67, … 69]</td><td>8</td><td>[15, 29, … 35]</td><td>[7, 19, … 23]</td><td>[10, 10, … 5]</td><td>[34, 39, … 33]</td><td>0</td></tr><tr><td>&quot;m64168_200820_000733/45943110/…</td><td>8167</td><td>&quot;TCTCCACGTTGGCCACGCTGGTCTCGAACT…</td><td>[93, 73, … 93]</td><td>13</td><td>[33, 18, … 26]</td><td>[20, 46, … 27]</td><td>[48, 70, … 20]</td><td>[20, 16, … 51]</td><td>0</td></tr><tr><td>&quot;m64168_200823_191315/50332760/…</td><td>1413</td><td>&quot;AATTTCTTGAAGAGACGAAAGTCTGTGGGT…</td><td>[93, 93, … 93]</td><td>32</td><td>[32, 12, … 18]</td><td>[21, 9, … 13]</td><td>[16, 17, … 13]</td><td>[16, 23, … 22]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/177537981…</td><td>4708</td><td>&quot;CAACCCACTGCCAAGCGCTTCCTGCCACCT…</td><td>[93, 82, … 93]</td><td>9</td><td>[13, 19, … 19]</td><td>[23, 14, … 34]</td><td>[9, 21, … 31]</td><td>[21, 13, … 34]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/49154585/…</td><td>5695</td><td>&quot;CCTCCCTACCGAAAACGGGGATCGTGTGAA…</td><td>[13, 58, … 53]</td><td>3</td><td>[6, 35, … 10]</td><td>[12, 34, … 19]</td><td>[17, 13, … 20]</td><td>[24, 12, … 43]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/163316698…</td><td>6320</td><td>&quot;ATGCAATCAACCTAACGTAAGTGCTCTCAC…</td><td>[93, 93, … 93]</td><td>24</td><td>[38, 51, … 76]</td><td>[25, 12, … 15]</td><td>[28, 18, … 21]</td><td>[14, 25, … 21]</td><td>1</td></tr><tr><td>&quot;m64168_200820_000733/4260355/c…</td><td>2902</td><td>&quot;CACCATGCCTGGCCACGAGACCCCATCTCA…</td><td>[93, 93, … 93]</td><td>28</td><td>[11, 8, … 10]</td><td>[23, 11, … 17]</td><td>[21, 13, … 23]</td><td>[39, 34, … 26]</td><td>0</td></tr><tr><td>&quot;m64168_200823_191315/71829932/…</td><td>5186</td><td>&quot;GGCAAGGCCCAGGCACGTGGTGCATCTGAA…</td><td>[93, 93, … 93]</td><td>22</td><td>[28, 30, … 18]</td><td>[19, 24, … 29]</td><td>[23, 12, … 25]</td><td>[17, 29, … 19]</td><td>1</td></tr><tr><td>&quot;m64168_200820_000733/50529600/…</td><td>6311</td><td>&quot;GAGGGTGGGGGTTAGCGAGTGATAGTGTGG…</td><td>[93, 93, … 93]</td><td>15</td><td>[13, 8, … 12]</td><td>[32, 38, … 17]</td><td>[28, 9, … 13]</td><td>[27, 22, … 39]</td><td>0</td></tr><tr><td>&quot;m64168_200820_000733/2360276/c…</td><td>3405</td><td>&quot;GCTGGAGTGCAGTGACGTGATCTCGGCTCA…</td><td>[93, 60, … 88]</td><td>6</td><td>[26, 32, … 21]</td><td>[28, 50, … 13]</td><td>[20, 26, … 32]</td><td>[13, 30, … 10]</td><td>0</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_val = pl.read_parquet('pacbio_standard_test_1m.parquet')\n",
        "df_val.head(10)"
      ],
      "metadata": {
        "id": "-LyR85Nn2T7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "7a7d26e6-86f0-4503-f906-87167602faf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (10, 10)\n",
              "┌────────────┬────────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬───────┐\n",
              "│ read_name  ┆ cg_pos ┆ seq        ┆ qual       ┆ … ┆ fp         ┆ ri         ┆ rp         ┆ label │\n",
              "│ ---        ┆ ---    ┆ ---        ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---   │\n",
              "│ str        ┆ i64    ┆ str        ┆ list[u8]   ┆   ┆ list[u16]  ┆ list[u16]  ┆ list[u16]  ┆ i32   │\n",
              "╞════════════╪════════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪═══════╡\n",
              "│ m64168_200 ┆ 3630   ┆ GGAGTCTCAC ┆ [93, 93, … ┆ … ┆ [43, 41, … ┆ [26, 15, … ┆ [41, 25, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ TCTGTCGCCC ┆ 93]        ┆   ┆ 15]        ┆ 11]        ┆ 22]        ┆       │\n",
              "│ /78053961/ ┆        ┆ AGGCTGGAGC ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 5646   ┆ TGCAGCAACA ┆ [93, 93, … ┆ … ┆ [9, 25, …  ┆ [52, 71, … ┆ [20, 20, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ CATGACGCAT ┆ 93]        ┆   ┆ 18]        ┆ 14]        ┆ 9]         ┆       │\n",
              "│ /151587048 ┆        ┆ TCTAAAATGT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 4396   ┆ ACATTTTTAA ┆ [93, 93, … ┆ … ┆ [21, 24, … ┆ [23, 25, … ┆ [18, 13, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ GTTGCCGTCT ┆ 93]        ┆   ┆ 19]        ┆ 37]        ┆ 21]        ┆       │\n",
              "│ /139657571 ┆        ┆ CTAGGACAAA ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 7207   ┆ GCAGTGGCAT ┆ [93, 93, … ┆ … ┆ [26, 13, … ┆ [15, 22, … ┆ [23, 17, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ GATCTCGGCT ┆ 93]        ┆   ┆ 36]        ┆ 14]        ┆ 21]        ┆       │\n",
              "│ /18940076/ ┆        ┆ CACTGCAACC ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 5185   ┆ GTCTCCAGCA ┆ [93, 93, … ┆ … ┆ [13, 30, … ┆ [27, 8, …  ┆ [26, 13, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ CCCAGCGCTC ┆ 93]        ┆   ┆ 41]        ┆ 16]        ┆ 15]        ┆       │\n",
              "│ /76875948/ ┆        ┆ CCACAAGCCT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 2983   ┆ AGTTCTTGCC ┆ [93, 93, … ┆ … ┆ [13, 14, … ┆ [24, 30, … ┆ [23, 16, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ TAGCTCGACC ┆ 93]        ┆   ┆ 30]        ┆ 11]        ┆ 15]        ┆       │\n",
              "│ /170133346 ┆        ┆ TCAGTCCCGT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 5830   ┆ GGGCGCGGTG ┆ [31, 93, … ┆ … ┆ [22, 13, … ┆ [23, 31, … ┆ [18, 15, … ┆ 1     │\n",
              "│ 823_191315 ┆        ┆ GCTCACGCCT ┆ 93]        ┆   ┆ 13]        ┆ 56]        ┆ 26]        ┆       │\n",
              "│ /148045839 ┆        ┆ GTAATCCCAG ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 267    ┆ TAAGTTTCTA ┆ [93, 93, … ┆ … ┆ [19, 31, … ┆ [17, 17, … ┆ [40, 33, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ GTAACCGTAT ┆ 93]        ┆   ┆ 40]        ┆ 23]        ┆ 22]        ┆       │\n",
              "│ /18220356/ ┆        ┆ TAAAAAGTAA ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 4642   ┆ GAGGTTGTGG ┆ [93, 93, … ┆ … ┆ [20, 19, … ┆ [19, 13, … ┆ [31, 21, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ TAAGCCGAGA ┆ 93]        ┆   ┆ 18]        ┆ 14]        ┆ 23]        ┆       │\n",
              "│ /6751166/c ┆        ┆ TCGCGCCATT ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ m64168_200 ┆ 399    ┆ CTGGGTGTGG ┆ [93, 93, … ┆ … ┆ [7, 9, …   ┆ [23, 16, … ┆ [45, 10, … ┆ 0     │\n",
              "│ 820_000733 ┆        ┆ TGGCACGTGC ┆ 93]        ┆   ┆ 14]        ┆ 13]        ┆ 11]        ┆       │\n",
              "│ /45418455/ ┆        ┆ CTGTAATCTC ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "│ …          ┆        ┆ …          ┆            ┆   ┆            ┆            ┆            ┆       │\n",
              "└────────────┴────────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴───────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (10, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>read_name</th><th>cg_pos</th><th>seq</th><th>qual</th><th>np</th><th>fi</th><th>fp</th><th>ri</th><th>rp</th><th>label</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>list[u8]</td><td>u8</td><td>list[u16]</td><td>list[u16]</td><td>list[u16]</td><td>list[u16]</td><td>i32</td></tr></thead><tbody><tr><td>&quot;m64168_200823_191315/78053961/…</td><td>3630</td><td>&quot;GGAGTCTCACTCTGTCGCCCAGGCTGGAGC…</td><td>[93, 93, … 93]</td><td>34</td><td>[20, 22, … 17]</td><td>[43, 41, … 15]</td><td>[26, 15, … 11]</td><td>[41, 25, … 22]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/151587048…</td><td>5646</td><td>&quot;TGCAGCAACACATGACGCATTCTAAAATGT…</td><td>[93, 93, … 93]</td><td>14</td><td>[26, 37, … 19]</td><td>[9, 25, … 18]</td><td>[52, 71, … 14]</td><td>[20, 20, … 9]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/139657571…</td><td>4396</td><td>&quot;ACATTTTTAAGTTGCCGTCTCTAGGACAAA…</td><td>[93, 93, … 93]</td><td>18</td><td>[20, 62, … 22]</td><td>[21, 24, … 19]</td><td>[23, 25, … 37]</td><td>[18, 13, … 21]</td><td>1</td></tr><tr><td>&quot;m64168_200820_000733/18940076/…</td><td>7207</td><td>&quot;GCAGTGGCATGATCTCGGCTCACTGCAACC…</td><td>[93, 93, … 93]</td><td>27</td><td>[9, 14, … 11]</td><td>[26, 13, … 36]</td><td>[15, 22, … 14]</td><td>[23, 17, … 21]</td><td>0</td></tr><tr><td>&quot;m64168_200823_191315/76875948/…</td><td>5185</td><td>&quot;GTCTCCAGCACCCAGCGCTCCCACAAGCCT…</td><td>[93, 93, … 93]</td><td>17</td><td>[10, 29, … 39]</td><td>[13, 30, … 41]</td><td>[27, 8, … 16]</td><td>[26, 13, … 15]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/170133346…</td><td>2983</td><td>&quot;AGTTCTTGCCTAGCTCGACCTCAGTCCCGT…</td><td>[93, 93, … 93]</td><td>26</td><td>[13, 15, … 28]</td><td>[13, 14, … 30]</td><td>[24, 30, … 11]</td><td>[23, 16, … 15]</td><td>1</td></tr><tr><td>&quot;m64168_200823_191315/148045839…</td><td>5830</td><td>&quot;GGGCGCGGTGGCTCACGCCTGTAATCCCAG…</td><td>[31, 93, … 93]</td><td>22</td><td>[18, 14, … 11]</td><td>[22, 13, … 13]</td><td>[23, 31, … 56]</td><td>[18, 15, … 26]</td><td>1</td></tr><tr><td>&quot;m64168_200820_000733/18220356/…</td><td>267</td><td>&quot;TAAGTTTCTAGTAACCGTATTAAAAAGTAA…</td><td>[93, 93, … 93]</td><td>22</td><td>[16, 18, … 46]</td><td>[19, 31, … 40]</td><td>[17, 17, … 23]</td><td>[40, 33, … 22]</td><td>0</td></tr><tr><td>&quot;m64168_200820_000733/6751166/c…</td><td>4642</td><td>&quot;GAGGTTGTGGTAAGCCGAGATCGCGCCATT…</td><td>[93, 93, … 93]</td><td>20</td><td>[17, 16, … 30]</td><td>[20, 19, … 18]</td><td>[19, 13, … 14]</td><td>[31, 21, … 23]</td><td>0</td></tr><tr><td>&quot;m64168_200820_000733/45418455/…</td><td>399</td><td>&quot;CTGGGTGTGGTGGCACGTGCCTGTAATCTC…</td><td>[93, 93, … 93]</td><td>10</td><td>[28, 43, … 6]</td><td>[7, 9, … 14]</td><td>[23, 16, … 13]</td><td>[45, 10, … 11]</td><td>0</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SSL Dataset Class"
      ],
      "metadata": {
        "id": "tf09N6FHa-VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each \"shard\" is a 512 MB numpy array, and so with this truncated datset we have around 15 GB of data. Based on the test below, it looks like we can transfer that at a rate well over 1 GB/s to the machine."
      ],
      "metadata": {
        "id": "uMi5jNA54V08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import glob\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class ShardedMemmapDataset(Dataset):\n",
        "    def __init__(self, data_dir, cache_size=100):\n",
        "        expanded_dir = os.path.expandvars(data_dir)\n",
        "        self.shard_paths = sorted(glob.glob(os.path.join(expanded_dir, \"*.npy\")))\n",
        "        first_shard = np.load(self.shard_paths[0], mmap_mode='r')\n",
        "        self.shard_size = first_shard.shape[0]\n",
        "        last_shard = np.load(self.shard_paths[-1], mmap_mode='r')\n",
        "        self.total_len = ((len(self.shard_paths) - 1) * self.shard_size) + last_shard.shape[0]\n",
        "        self.cache_size = cache_size\n",
        "        self.memmaps = OrderedDict()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        shard_idx = idx // self.shard_size\n",
        "        local_idx = idx % self.shard_size\n",
        "        if shard_idx not in self.memmaps:\n",
        "            if len(self.memmaps) >= self.cache_size:\n",
        "                self.memmaps.popitem(last=False)\n",
        "            self.memmaps[shard_idx] = np.load(self.shard_paths[shard_idx], mmap_mode='r')\n",
        "        else:\n",
        "            self.memmaps.move_to_end(shard_idx)\n",
        "        return torch.from_numpy(np.array(self.memmaps[shard_idx][local_idx])).bfloat16()"
      ],
      "metadata": {
        "id": "4-3Bear63rpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# ssl_ds = ShardedMemmapDataset(\"ob007.memmap/\")\n",
        "# ssl_dl = DataLoader(ssl_ds, batch_size=256, num_workers=4, pin_memory=True, prefetch_factor=2, shuffle=True)\n",
        "\n",
        "# for batch in iter(tqdm(ssl_dl)):\n",
        "#   x = batch.to(device)\n"
      ],
      "metadata": {
        "id": "yMo2TJ8J3vIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "3UiN1hteGGfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dir_list = [\"/data/dir1\", \"/data/dir2\"]\n",
        "# datasets = [ShardedMemmapDataset(d) for d in dir_list]\n",
        "# combined_dataset = ConcatDataset(datasets)\n",
        "\n",
        "\n",
        "\n",
        "SEQ_LEN = 4096\n",
        "BATCH_SIZE = 64\n",
        "D_MODEL = 128\n",
        "\n",
        "ssl_ds = ShardedMemmapDataset(\"ob007.memmap/\")\n",
        "ssl_dl = DataLoader(ssl_ds, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, prefetch_factor=2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "iqXxINcbGGKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Smrt2Vec"
      ],
      "metadata": {
        "id": "gwsQpOMcbHbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataflow Plan\n",
        "### Embed data: [B, T, C] -> [B, T, d_model] = E\n",
        "Since we have 1 categorical channel and 2 continuous channels we'll use a hybrid embedding. The nucleotide channel gets an embedding table, and the 2 continuous kinetics channels get a single linear projection with a GeLU nonlinearity. Note the continuous channels are normalized across the genome to have 0 mean, unit variance. GeLU is important for this since it allows negative values...\n",
        "\n",
        "### Extract features: [B, T, d_model], Pad -> [B, T', d_model], [B, T', 1] = Z, Pad'\n",
        "Separate out the padding channel. Runn a CNN over the sequence to generate a new sequence with features. Calculate the new padding mask based on the the CNN downsampling stride.\n",
        "\n",
        "### Mask random indices: [B, T', d_model], Pad' -> [B, T', d_model] = Z_masked, Mask_idx\n",
        "We mask the output of the CNN at randomly sampled indices (say 5 percent of them) and then replace a window (say 5 indices) starting at that index with the learnable padding vector (d_model) such that the sequence length remains the same as the output of the CNN.\n",
        "\n",
        "### Positional encoding: [B, T', d_model] -> [B, T', d_model]\n",
        "Only add the positional encoding at this point since the CNN and addition of masking vectors would overwrite its information otherwise\n",
        "\n",
        "### Transformer block: [B, T', d_model], Pad' -> [B, T', d_model]\n",
        "Run through a series of transformer blocks to get contextualized embeddings\n",
        "\n",
        "### Compute contrastive loss: [B, T', d_model], Mask_idx -> Loss\n",
        "Using the masked indices, use each C_t from the transformer output to predict the latent embedding. We will use an MLP for this transformation, and a separate one for the targets, and I suspect a smaller space than d_model will perform better (say 32 instead of 128). Score the prediction with infoNCE, so how much more similar is the predicted embedding vector to the true target at the position (which we retained) in comparison to a set of randomly sampled indices from the batch.\n"
      ],
      "metadata": {
        "id": "FpuXfCr9y9GA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building Blocks"
      ],
      "metadata": {
        "id": "xJRcCFExbU-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  \"\"\"\n",
        "  Generates positional encodings based on a given sequence length and\n",
        "  d_model. Follows Vaswani et al (2017).\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, max_len=4096):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model) # initialize vector\n",
        "        position = torch.arange(0, max_len, dtype=torch.bfloat16).unsqueeze(1) # make an index vector\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) #\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "  def forward(self, x):\n",
        "      return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  \"\"\"\n",
        "  Simple MLP for composition in the TransformerBlock\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, expansion=4):\n",
        "      super().__init__()\n",
        "      self.c_fc = nn.Linear(d_model, d_model * expansion)\n",
        "      self.gelu = nn.GELU() # maybe ReLU would be fine?\n",
        "      self.c_proj = nn.Linear(d_model * expansion, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.c_fc(x)\n",
        "      x = self.gelu(x)\n",
        "      x = self.c_proj(x)\n",
        "      return x\n",
        "\n",
        "class SmrtEmbedding(nn.Module):\n",
        "  \"\"\"\n",
        "  Hybrid embeddings for the SMRT data\n",
        "\n",
        "  Nucleotides are in encoded as float representations of integers in the dataset\n",
        "  so that they can be used to index into an embedding table\n",
        "\n",
        "  Kinetics (ipd, pw) come as floats and are embedded with a linear projection\n",
        "\n",
        "  Issue:\n",
        "  n_nucleotides is set to 5, but in reality we don't use a padding token in the\n",
        "  data preprocessing. Instead, the padded sections of the data are set to 0.0\n",
        "  with the exception of the padding mask channel, which is set to 1.0\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, n_nucleotides=5, n_continuous=2):\n",
        "    super().__init__()\n",
        "    self.nuc_embed = nn.Embedding(n_nucleotides, d_model//2)\n",
        "    self.kin_embed = nn.Linear(n_continuous, d_model//2, dtype=torch.bfloat16)\n",
        "    self.layernorm = nn.LayerNorm(d_model)\n",
        "    self.d_model = d_model\n",
        "  def forward(self, x_nuc, x_kin, is_padding):\n",
        "    scale = math.sqrt(self.d_model)\n",
        "    seq_emb = self.nuc_embed(x_nuc.int())*scale\n",
        "    kin_emb = self.kin_embed(x_kin)*scale\n",
        "    x = torch.concat((seq_emb,kin_emb),dim=-1)\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "class BidirectionalSelfAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Implementation of bidirectional self attention.\n",
        "\n",
        "  Uses a GPT-style single matrix multiplication for computing QKV\n",
        "\n",
        "  Forward pass uses the padding mask (provided in the data as the\n",
        "  last channel) as an attn mask. Pytorch has competing standards on whether\n",
        "  1 should correspond to \"attend\" or \"ignore\". The mask in the data is a\n",
        "  \"padding mask\" and so 1 corresponds to \"pad\" and 0 to \"active data\".\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, n_head=4, max_len=4096):\n",
        "      super().__init__()\n",
        "      assert d_model % n_head == 0\n",
        "      self.n_head = n_head\n",
        "      self.head_dim = d_model // n_head\n",
        "      # produces qkv, so we output 3*d_model\n",
        "      self.c_attn = nn.Linear(d_model, 3 * d_model, bias=False)\n",
        "      self.c_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "  def forward(self, x, x_pad, pad_val=1):\n",
        "      B, T, C = x.size()\n",
        "      # use one big matmul and split\n",
        "      qkv = self.c_attn(x).view(B, T, 3, self.n_head, self.head_dim)\n",
        "      qkv = qkv.permute(2, 0, 3, 1, 4) # -> (3, B, n_head, T, head_dim)\n",
        "      q, k, v = qkv[0], qkv[1], qkv[2] # -> 3 x (B, n_head, T, head_dim)\n",
        "\n",
        "      # broadcast across the head and query dims\n",
        "      # given alignment right to left, we need to reshape to match B,H,T,T\n",
        "      attn_mask = ~x_pad.view(B, 1, 1, T)\n",
        "      output = F.scaled_dot_product_attention(\n",
        "          q, k, v,\n",
        "          attn_mask=attn_mask,\n",
        "          dropout_p=0.0 if not self.training else 0.05,\n",
        "          is_causal=False # since we attend to everything outside the att_mask\n",
        "      )\n",
        "\n",
        "      output = output.transpose(1, 2).contiguous().view(B, T, C)\n",
        "      return self.c_proj(output)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Ties together BidirectionalSelfAttention, MLP, and LayerNorm to form\n",
        "  a layerable transformer block\n",
        "\n",
        "  Issue:\n",
        "  Layernorm does not accept a pad mask\n",
        "  https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, n_head, max_len):\n",
        "      super().__init__()\n",
        "      self.ln1 = nn.LayerNorm(d_model)\n",
        "      self.attn = BidirectionalSelfAttention(d_model, n_head, max_len)\n",
        "      self.ln2 = nn.LayerNorm(d_model)\n",
        "      self.mlp = MLP(d_model)\n",
        "\n",
        "  def forward(self, x, x_pad): # includes unscaled residuals\n",
        "      x = x + self.attn(self.ln1(x), x_pad)\n",
        "      x = x + self.mlp(self.ln2(x))\n",
        "      return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Convolutional block with residual connections\n",
        "\n",
        "  Issue:\n",
        "  Uses batch norm in between layers. This is dubious. Though it's was\n",
        "  approrpriate for the purely convolutional 1d methylation classifier, for\n",
        "  this sequential data it could cause problems\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
        "    super(ResBlock, self).__init__()\n",
        "\n",
        "    self.padding = (kernel_size - 1) // 2\n",
        "    self.kernel_size = kernel_size\n",
        "\n",
        "    self.bn1 = nn.BatchNorm1d(in_channels)\n",
        "    self.conv1 = nn.Conv1d(in_channels=in_channels,\n",
        "                           out_channels=out_channels,\n",
        "                           kernel_size=kernel_size,\n",
        "                           stride=stride,\n",
        "                           padding=self.padding,\n",
        "                           bias=False)\n",
        "    self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "    self.conv2 = nn.Conv1d(in_channels=out_channels,\n",
        "                           out_channels=out_channels,\n",
        "                           kernel_size=kernel_size,\n",
        "                           stride=1,\n",
        "                           padding=self.padding,\n",
        "                           bias=False)\n",
        "\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.stride = stride\n",
        "    # projection residual\n",
        "    if any([in_channels != out_channels, stride != 1]):\n",
        "      self.residual = nn.Sequential(\n",
        "          nn.Conv1d(in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=1, stride=stride,\n",
        "                    bias=False)\n",
        "          )\n",
        "    # identity residual\n",
        "    else:\n",
        "      self.residual = nn.Sequential()\n",
        "  def _resize_mask(self, mask, pad_val=1):\n",
        "    if mask.dtype == torch.bool:\n",
        "      mask = mask.float()\n",
        "    if pad_val == 0:\n",
        "      mask = F.max_pool1d(mask,\n",
        "                          kernel_size=self.kernel_size,\n",
        "                          stride=self.stride,\n",
        "                          padding=self.padding)\n",
        "    elif pad_val == 1:\n",
        "      mask = 1 - F.max_pool1d(1 - mask,\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              stride=self.stride,\n",
        "                              padding=self.padding)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid pad value: Pad value must be 0 or 1\")\n",
        "    return mask.bool()\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    out = self.relu(self.bn1(x))\n",
        "    out = self.conv1(out)\n",
        "    out = self.relu(self.bn2(out))\n",
        "    out = self.conv2(out)\n",
        "    out += self.residual(x)\n",
        "    mask = self._resize_mask(mask)\n",
        "    return out, mask\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  \"\"\"\n",
        "  Composes the ResBlocks into a downsampler, bringing the padding mask\n",
        "  along so that that it also gets downsampled\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, max_len, dropout_p):\n",
        "    super().__init__()\n",
        "    self.max_len = max_len\n",
        "    self.in_channels = d_model\n",
        "    # extractor\n",
        "    self.extractor = nn.ModuleList([\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=7),            # (B, C, T)   -> (B, C, T)\n",
        "\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T)   -> (B, C, T)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T)   -> (B, C, T)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T)   -> (B, C, T)\n",
        "\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3, stride=2),  # (B, C, T)   -> (B, C, T/2)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/2)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/2)\n",
        "\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3, stride=2),  # (B, C, T/2) -> (B, C, T/4)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/4)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/4)\n",
        "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/4)\n",
        "          ])\n",
        "    self.dropout = nn.Dropout(p=dropout_p)\n",
        "    # calculate fc layer input with dummy passthrough\n",
        "    self.output_shapes = self._get_output_shape()\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for block in self.extractor:\n",
        "      x, mask= block(x,mask)\n",
        "    return x, mask\n",
        "\n",
        "  def _get_output_shape(self):\n",
        "      \"\"\"\n",
        "      Returns output shapes for the data and mask\n",
        "      \"\"\"\n",
        "      dummy_x = torch.randn(1, self.in_channels, self.max_len)\n",
        "      dummy_mask = torch.randn(1, self.max_len)\n",
        "\n",
        "      # get outputshapes\n",
        "      output, mask = self.forward(dummy_x, dummy_mask)\n",
        "      return output.shape, mask.shape\n"
      ],
      "metadata": {
        "id": "h2DpvvAbBTgP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "6d0d7e25-1758-4cd4-99b7-3a2b01a3c52a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4096596921.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPositionalEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \"\"\"\n\u001b[1;32m      3\u001b[0m   \u001b[0mGenerates\u001b[0m \u001b[0mpositional\u001b[0m \u001b[0mencodings\u001b[0m \u001b[0mbased\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0msequence\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0md_model\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mFollows\u001b[0m \u001b[0mVaswani\u001b[0m \u001b[0met\u001b[0m \u001b[0mal\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2017\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \"\"\"\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Model Classes"
      ],
      "metadata": {
        "id": "FznAftiOasie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Encoder Class\n",
        "\n",
        "class SmrtEncoder(nn.Module):\n",
        "  def __init__(self, d_model=128, n_layers=4, n_head=4, max_len=4096, dropout_p=0.01):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embed = SmrtEmbedding(d_model)\n",
        "    self.pe = PositionalEncoding(d_model, max_len=max_len)\n",
        "    self.downsample = CNN(d_model, max_len=max_len, dropout_p=dropout_p)\n",
        "    self.layer_norm_target = nn.LayerNorm(d_model)\n",
        "    self.blocks = nn.ModuleList([\n",
        "        TransformerBlock(d_model=d_model, n_head=n_head, max_len=max_len) for _ in range(n_layers)\n",
        "        ])\n",
        "  def get_latents(self, x):\n",
        "    \"\"\"\n",
        "    Runs [x -> Embedding -> CNN -> out] stack (for training)\n",
        "    Returns:\n",
        "      z (downsampled latents with PE)\n",
        "      z_pad (dowsampled padding mask)\n",
        "      targets (latents without PE)\n",
        "    \"\"\"\n",
        "    # separate into features and padding\n",
        "    x_nuc = x[...,0]\n",
        "    x_kin = x[...,1:3]\n",
        "    x_pad = x[...,3]\n",
        "    # generate hybrid embedding\n",
        "    x = self.embed(x_nuc, x_kin, x_pad)\n",
        "    # featurize the emmbeddings (cnn expect BCT)\n",
        "    z, z_pad = self.downsample(x.permute(0,2,1), x_pad)\n",
        "    # permute back to BTC\n",
        "    z = z.permute(0,2,1)\n",
        "    targets = self.layer_norm_target(z.clone())\n",
        "    return z, z_pad, targets\n",
        "\n",
        "  def add_pe(self, z):\n",
        "      return self.pe(z)\n",
        "\n",
        "  def forward_transformer(self, z, z_pad):\n",
        "    \"\"\"\n",
        "    Runs the transformer blocks on the downsampled latents\n",
        "    Returns:\n",
        "      c (context aware latents)\n",
        "    \"\"\"\n",
        "    c = z\n",
        "    for block in self.blocks:\n",
        "      c = block(c, z_pad)\n",
        "    return c\n",
        "  def forward(self, x):\n",
        "    z, z_pad, _ = self.get_latents(x)\n",
        "    z = self.add_pe(z)\n",
        "    c = self.forward_transformer(z, z_pad)\n",
        "    return c\n",
        "\n",
        "### Main Model\n",
        "class Smrt2Vec(nn.Module):\n",
        "  def __init__(self, d_model=128, n_layers=4, n_head=4, max_len=4096):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.encoder = SmrtEncoder(d_model, n_layers, n_head, max_len)\n",
        "\n",
        "    # components specific to pretraining\n",
        "    self.mask_vec = nn.Parameter(torch.randn(d_model))\n",
        "    self.project =  nn.Sequential(\n",
        "        nn.Linear(d_model, d_model),\n",
        "        nn.GELU(), # avoid negative values being ignored with ReLU\n",
        "        nn.Linear(d_model, d_model)\n",
        "        )\n",
        "  def apply_mask(self, x_emb, pad, prob=0.05, size=6):\n",
        "    B, T, C = x_emb.shape\n",
        "    mask_idx_centers = (torch.rand(B, T, device=x_emb.device) < prob) & ~(pad.bool())\n",
        "    mask_idx_full = F.max_pool1d(\n",
        "        mask_idx_centers.bfloat16(),\n",
        "        kernel_size=size, stride=1, # hyperparameter here...\n",
        "        padding=size//2\n",
        "      ).bool()[:, :T] & (~pad.bool())\n",
        "    x_masked = x_emb.clone()\n",
        "    x_masked[mask_idx_full] = self.mask_vec.to(dtype=x_emb.dtype, device=x_emb.device)\n",
        "    return x_masked, mask_idx_full\n",
        "  def forward(self, x):\n",
        "    # dowsampled latents with pe (no transormer block yet)\n",
        "    z, z_pad, targets = self.encoder.get_latents(x)\n",
        "    # mask indices for loss\n",
        "    z_masked, z_masked_bool = self.apply_mask(z, z_pad)\n",
        "    z_masked_pe = self.encoder.add_pe(z_masked)\n",
        "    # run through transformer\n",
        "    c = self.encoder.forward_transformer(z_masked_pe, z_pad)\n",
        "    # project\n",
        "    c_proj = self.project(c)\n",
        "    return c_proj, targets.detach(), z_masked_bool # projected transformer output, detached unmasked downsampled latents (not transfomer applied), boolean matrix of where the targets are\n",
        "\n",
        "### Loss\n",
        "class InfoNCELoss(nn.Module):\n",
        "  def __init__(self, temperature=0.1):\n",
        "    super().__init__()\n",
        "    self.cross_entropy = nn.CrossEntropyLoss()\n",
        "    self.temperature = temperature\n",
        "  def forward(self, c_proj, targets, mask_idx):\n",
        "    # gather the predictions and truth vectors\n",
        "    preds = c_proj[mask_idx]\n",
        "    truth = targets[mask_idx]\n",
        "    # normalize for cosine similarity\n",
        "    # last dim (embedding dim)\n",
        "    preds = F.normalize(preds, dim=-1)\n",
        "    truth = F.normalize(truth, dim=-1)\n",
        "    # print(truth.shape,preds.shape)\n",
        "    logits = torch.mm(preds, truth.permute(1,0)) / self.temperature\n",
        "    labels = torch.arange(truth.shape[0], device=truth.device)\n",
        "    loss = self.cross_entropy(logits, labels)\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0-czBfcm7dmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model forward pass check"
      ],
      "metadata": {
        "id": "1gOoZWXbbjoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "batch = next(iter(ssl_dl)).to(device)\n",
        "model = Smrt2Vec().to(device)\n",
        "c_proj, targets, mask = model(batch)\n",
        "print((c_proj.shape, targets.shape, mask.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bhhVe6vIkfn",
        "outputId": "24cc2b6a-b736-47ea-9d63-fd9378321ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(torch.Size([64, 2048, 128]), torch.Size([64, 2048, 128]), torch.Size([64, 2048]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = InfoNCELoss()\n",
        "loss(c_proj, targets, mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpflfHgtWAvT",
        "outputId": "af30a87c-a3a8-47d4-97e7-4fec1f336c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.4577, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Smrt2Vec().to(device)\n",
        "model.train()\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"trainable params: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSQE0CHcCv-1",
        "outputId": "53acf2ab-24ad-4962-fb0b-4b24c2153c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2059648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Training"
      ],
      "metadata": {
        "id": "DMIzPGZYekWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.02)\n",
        "criterion = InfoNCELoss(temperature=0.1).to(device)\n",
        "EPOCHS = 2\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=6e-4,\n",
        "    total_steps=len(ssl_dl) * EPOCHS,\n",
        "    pct_start=0.05\n",
        ")\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    progress_bar = tqdm(ssl_dl, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for i, batch in enumerate(progress_bar):\n",
        "        batch = batch.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "            c_proj, targets, mask_idx = model(batch)\n",
        "            loss = criterion(c_proj, targets, mask_idx)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            progress_bar.set_postfix(\n",
        "                loss=f\"{loss.item():.4f}\",\n",
        "                lr=f\"{scheduler.get_last_lr()[0]:.6f}\"\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fok1kxgAyvP5",
        "outputId": "9b8889d7-5401-4657-ac72-87cfb70722ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 7976/7976 [27:25<00:00,  4.85it/s, loss=4.6667, lr=0.000325]\n",
            "Epoch 2/2: 100%|██████████| 7976/7976 [27:23<00:00,  4.85it/s, loss=5.4183, lr=0.000000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downstream Task"
      ],
      "metadata": {
        "id": "2PamTn7KcQkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "Honestly this feels a bit funky, and I'm debating whether to make a new preprocessing script that produces numpy arrays like the SSL dataset. This parquet style dataset is inherited from the CNN a while back and is much more difficult to work with, I find. Also much more difficult to get good bandwidth."
      ],
      "metadata": {
        "id": "1kBnYhgnk9yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import pyarrow.parquet as pq\n",
        "from pathlib import Path\n",
        "from torch.utils.data import IterableDataset\n",
        "def compute_log_normalization_stats(df, features, epsilon=1):\n",
        "    means = {col: (df[col].explode() + epsilon).log().mean() for col in features}\n",
        "    stds = {col: (df[col].explode() + epsilon).log().explode().std() for col in features}\n",
        "    return means, stds\n",
        "\n",
        "class MethylIterableDataset(IterableDataset):\n",
        "    def __init__(self, data_path, means, stds, context, restrict_row_groups=0, single_strand=False, inference=False):\n",
        "        super().__init__()\n",
        "        self.data_path = Path(data_path)\n",
        "        self.means, self.stds = means, stds\n",
        "        self.context = context\n",
        "        self.single_strand = single_strand\n",
        "        self.inference = inference\n",
        "        self.restrict = restrict_row_groups\n",
        "\n",
        "        self.kin_feats = ['fi', 'fp', 'ri', 'rp']\n",
        "        self.vocab = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4}\n",
        "        self.comp_map = torch.tensor([3, 2, 1, 0, 4], dtype=torch.long)\n",
        "\n",
        "        try:\n",
        "            meta = pq.read_metadata(self.data_path)\n",
        "            self.n_groups = meta.num_row_groups\n",
        "            use_groups = min(self.restrict, self.n_groups) if self.restrict else self.n_groups\n",
        "\n",
        "            # fast row count\n",
        "            n_rows = sum(meta.row_group(i).num_rows for i in range(use_groups))\n",
        "            self.len = n_rows * (2 if single_strand else 1)\n",
        "        except Exception:\n",
        "            print(f'Failed to read parquet: {self.data_path}')\n",
        "            self.n_groups, self.len = 0, 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def _process_batch(self, df):\n",
        "      # seq\n",
        "        seq_arr = np.stack(\n",
        "            df['seq'].str.split(\"\")\n",
        "            .list.eval(pl.element().replace_strict(self.vocab, default=4))\n",
        "            .to_numpy()\n",
        "        )\n",
        "        seq_t = torch.tensor(seq_arr, dtype=torch.long)\n",
        "\n",
        "        # kinetics\n",
        "        kin_list = []\n",
        "        for k in self.kin_feats:\n",
        "            vals = df[k].to_numpy() # (N, L)\n",
        "            vals = (np.log(vals + 1) - self.means[k]) / self.stds[k]\n",
        "            kin_list.append(vals)\n",
        "        kin_t = torch.tensor(np.stack(kin_list, axis=1), dtype=torch.bfloat16)\n",
        "\n",
        "        # mask, labels, etc (note that there is no masked data in the downstream set, so it's all zeros here)\n",
        "        mask = torch.zeros((seq_t.shape[0], seq_t.shape[1], 1), dtype=torch.bfloat16)\n",
        "        labels = torch.tensor(df['label'].to_numpy(), dtype=torch.long) if not self.inference else None\n",
        "        r_names, pos = df['read_name'].to_list(), df['cg_pos'].to_list()\n",
        "\n",
        "        # construct forward sample\n",
        "        # Seq (N, L, 1) + Kin (N, 2, L)->(N, L, 2) + Mask (N, L, 1) = (N, L, 4)\n",
        "        fwd_data = torch.cat([\n",
        "            seq_t.unsqueeze(-1).to(torch.bfloat16),\n",
        "            kin_t[:, 0:2].permute(0, 2, 1),\n",
        "            mask\n",
        "        ], dim=2)\n",
        "\n",
        "        # construct reverse data\n",
        "        rev_data = None\n",
        "        if self.single_strand:\n",
        "            rev_seq_t = torch.flip(self.comp_map.to(seq_t.device)[seq_t], dims=[1])\n",
        "            # Kin: slice 2:4, flip time (dim 2), permute channels\n",
        "            rev_kin = torch.flip(kin_t[:, 2:4], dims=[2]).permute(0, 2, 1)\n",
        "            rev_data = torch.cat([\n",
        "                rev_seq_t.unsqueeze(-1).to(torch.bfloat16),\n",
        "                rev_kin,\n",
        "                mask\n",
        "            ], dim=2)\n",
        "\n",
        "        # yield\n",
        "        for i in range(len(df)):\n",
        "            # forward\n",
        "            strand_name = 'fwd' if self.single_strand else 'ds'\n",
        "            item_fwd = {\n",
        "                'data': fwd_data[i],\n",
        "                'metadata': {'read_name': r_names[i], 'position': pos[i], 'strand': strand_name}\n",
        "            }\n",
        "            if labels is not None: item_fwd['label'] = labels[i]\n",
        "            yield item_fwd\n",
        "\n",
        "            # reverse\n",
        "            if rev_data is not None:\n",
        "                item_rev = {\n",
        "                    'data': rev_data[i],\n",
        "                    'metadata': {'read_name': r_names[i], 'position': pos[i], 'strand': 'rev'}\n",
        "                }\n",
        "                if labels is not None: item_rev['label'] = labels[i]\n",
        "                yield item_rev\n",
        "            else:\n",
        "              continue\n",
        "\n",
        "    def __iter__(self):\n",
        "        worker = torch.utils.data.get_worker_info()\n",
        "        valid_groups = min(self.restrict, self.n_groups) if self.restrict else self.n_groups\n",
        "        indices = np.arange(valid_groups)\n",
        "\n",
        "        if worker:\n",
        "            indices = np.array_split(indices, worker.num_workers)[worker.id]\n",
        "\n",
        "        pqf = pq.ParquetFile(self.data_path)\n",
        "        for i in indices:\n",
        "            # array cast\n",
        "            df = pl.from_arrow(pqf.read_row_group(i)).with_columns([\n",
        "                pl.col(c).list.to_array(self.context) for c in self.kin_feats\n",
        "            ])\n",
        "            yield from self._process_batch(df)"
      ],
      "metadata": {
        "id": "PFMN3kbh6UE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KINETICS_FEATURES = ['fi', 'fp', 'ri', 'rp']\n",
        "\n",
        "df = pl.read_parquet('pacbio_standard_train_1m.parquet')\n",
        "train_means, train_stds = compute_log_normalization_stats(df, KINETICS_FEATURES)\n",
        "\n",
        "it_workers=0\n",
        "batch_size=256\n",
        "single_strand=True\n",
        "#train\n",
        "methyl_train_ds = MethylIterableDataset('./pacbio_standard_train_1m.parquet',\n",
        "                                    means=train_means,\n",
        "                                    stds=train_stds,\n",
        "                                    context=32)\n",
        "methyl_train_dl = DataLoader(methyl_train_ds,\n",
        "                             batch_size=batch_size,\n",
        "                             drop_last=True,\n",
        "                             persistent_workers=False,\n",
        "                             prefetch_factor=None,\n",
        "                            )\n",
        "# val\n",
        "methyl_val_ds = MethylIterableDataset('./pacbio_standard_test_1m.parquet',\n",
        "                                    means=train_means,\n",
        "                                    stds=train_stds,\n",
        "                                    context=32)\n",
        "methyl_val_dl = DataLoader(methyl_val_ds,\n",
        "                        batch_size=batch_size,\n",
        "                        drop_last=True,\n",
        "                        persistent_workers=False,\n",
        "                        prefetch_factor=None)"
      ],
      "metadata": {
        "id": "rTB-NjbWRrHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Probe"
      ],
      "metadata": {
        "id": "2Oy3GboOX54S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleIdxProbe(nn.Module):\n",
        "    def __init__(self, encoder, n_classes=1, freeze_encoder=False):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "\n",
        "        if freeze_encoder:\n",
        "          self.encoder.requires_grad_(False)\n",
        "        else:\n",
        "          self.encoder.requires_grad_(True)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(encoder.d_model, encoder.d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoder.d_model // 2, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        c = self.encoder(x)\n",
        "        logit = self.head(c[:, -1, :])\n",
        "        return logit"
      ],
      "metadata": {
        "id": "ekzuqBqfTv_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "# LR = 1e-5\n",
        "EPOCHS = 20\n",
        "DEVICE = torch.device('cuda')\n",
        "encoder_clone = copy.deepcopy(model.encoder)\n",
        "probe = SingleIdxProbe(encoder_clone, freeze_encoder=False).to(device)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(probe.parameters(), lr=LR)\n",
        "\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': probe.encoder.parameters(), 'lr': 5e-7},\n",
        "    {'params': probe.head.parameters(), 'lr': 3e-5}\n",
        "    ])\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "loss_history = []\n",
        "\n",
        "total_params = sum(p.numel() for p in probe.parameters() if p.requires_grad)\n",
        "print(f\"trainable params: {total_params}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    probe.train()\n",
        "    running_loss = 0.0\n",
        "    for i, batch in enumerate(tqdm(methyl_train_dl)):\n",
        "        inputs = batch['data'].to(DEVICE)\n",
        "        labels = batch['label'].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = probe(inputs)\n",
        "        loss = criterion(logits, labels.unsqueeze(1).to(torch.float32))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 100 == 0:\n",
        "            loss_history.append(running_loss / 100)\n",
        "            running_loss = 0.0\n",
        "\n",
        "    probe.eval()\n",
        "    sample_count = 0\n",
        "    sample_correct = 0\n",
        "    for batch in tqdm(methyl_val_dl):\n",
        "        inputs = batch['data'].to(DEVICE)\n",
        "        labels = batch['label'].to(DEVICE)\n",
        "\n",
        "        logits = probe(inputs)\n",
        "        preds = logits > 0\n",
        "        correct = labels == preds.squeeze(-1)\n",
        "        sample_count += correct.shape[0]\n",
        "        sample_correct += correct.sum()\n",
        "    print(f\"epoch val top1_acc: {sample_correct/sample_count}\")"
      ],
      "metadata": {
        "id": "uybV3hRFT-oL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f3fe3d-5051-4f13-a64a-c9ad60016944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2034817\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:39<00:00, 39.37it/s]\n",
            "100%|██████████| 3906/3906 [00:46<00:00, 84.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.606289803981781\n",
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.52it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 85.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.6321794390678406\n",
            "Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:39<00:00, 39.44it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.6482734680175781\n",
            "Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.64it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.6627734303474426\n",
            "Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.52it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 85.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.6747452020645142\n",
            "Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.57it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.6823336482048035\n",
            "Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:39<00:00, 39.35it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.6884410381317139\n",
            "Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:39<00:00, 39.36it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.6933473944664001\n",
            "Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.57it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.6968675851821899\n",
            "Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.52it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.6999537944793701\n",
            "Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.58it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.7030689716339111\n",
            "Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.51it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.7054701447486877\n",
            "Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.49it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.7078723311424255\n",
            "Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.46it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.7099254131317139\n",
            "Epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:39<00:00, 39.35it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.7118425369262695\n",
            "Epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.46it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 85.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.7137816548347473\n",
            "Epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.60it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.7153827548027039\n",
            "Epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.58it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.716887891292572\n",
            "Epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:38<00:00, 39.58it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 85.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.7184529900550842\n",
            "Epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3906/3906 [01:39<00:00, 39.41it/s]\n",
            "100%|██████████| 3906/3906 [00:45<00:00, 86.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch val top1_acc: 0.7196740508079529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pl.DataFrame({'loss': loss_history})\n",
        "def plot_loss(loss_df):\n",
        "  # loss_df_long = loss_df.unpivot(index='stepsx100', value_name='loss')\n",
        "  # min_loss = loss_df_long['loss'].min()\n",
        "  # max_loss = loss_df_long['loss'].max(\n",
        "  loss_df = loss_df.with_row_index()\n",
        "  loss_chart = alt.Chart(loss_df).mark_line().encode(\n",
        "    alt.X('index:Q'),\n",
        "    alt.Y('loss:Q'),\n",
        "  ).properties(\n",
        "    width=700,\n",
        "    height=500,\n",
        "    title = 'Direct Downstream Train Loss'\n",
        "  )\n",
        "  return loss_chart\n",
        "plot_loss(df)"
      ],
      "metadata": {
        "id": "gSUmVjcWeRU-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "2a353099-9897-4861-d5fa-17908fc151b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "  #altair-viz-e90a3d5f9bc34dae9034ef1d5af3ac34.vega-embed {\n",
              "    width: 100%;\n",
              "    display: flex;\n",
              "  }\n",
              "\n",
              "  #altair-viz-e90a3d5f9bc34dae9034ef1d5af3ac34.vega-embed details,\n",
              "  #altair-viz-e90a3d5f9bc34dae9034ef1d5af3ac34.vega-embed details summary {\n",
              "    position: relative;\n",
              "  }\n",
              "</style>\n",
              "<div id=\"altair-viz-e90a3d5f9bc34dae9034ef1d5af3ac34\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-e90a3d5f9bc34dae9034ef1d5af3ac34\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-e90a3d5f9bc34dae9034ef1d5af3ac34\");\n",
              "    }\n",
              "\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      let deps = [\"vega-embed\"];\n",
              "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"$schema\": \"https://vega.github.io/schema/vega/v5.json\", \"data\": [{\"name\": \"source_0\", \"values\": [{\"index\": 0, \"loss\": 0.8010361850261688}, {\"index\": 1, \"loss\": 0.6968078744411469}, {\"index\": 2, \"loss\": 0.6942548996210098}, {\"index\": 3, \"loss\": 0.6914898049831391}, {\"index\": 4, \"loss\": 0.6901247590780258}, {\"index\": 5, \"loss\": 0.6889558589458465}, {\"index\": 6, \"loss\": 0.688400132060051}, {\"index\": 7, \"loss\": 0.687631043791771}, {\"index\": 8, \"loss\": 0.6862002795934677}, {\"index\": 9, \"loss\": 0.6858470124006272}, {\"index\": 10, \"loss\": 0.6834086579084396}, {\"index\": 11, \"loss\": 0.6829541021585465}, {\"index\": 12, \"loss\": 0.6820965945720673}, {\"index\": 13, \"loss\": 0.6799608224630356}, {\"index\": 14, \"loss\": 0.6796680444478989}, {\"index\": 15, \"loss\": 0.6777597886323928}, {\"index\": 16, \"loss\": 0.6769711470603943}, {\"index\": 17, \"loss\": 0.6743504613637924}, {\"index\": 18, \"loss\": 0.6749488765001297}, {\"index\": 19, \"loss\": 0.6720881474018097}, {\"index\": 20, \"loss\": 0.6699754965305328}, {\"index\": 21, \"loss\": 0.6705762714147567}, {\"index\": 22, \"loss\": 0.6689328080415726}, {\"index\": 23, \"loss\": 0.666901798248291}, {\"index\": 24, \"loss\": 0.666204861998558}, {\"index\": 25, \"loss\": 0.6671009105443955}, {\"index\": 26, \"loss\": 0.6674663585424423}, {\"index\": 27, \"loss\": 0.6653828257322312}, {\"index\": 28, \"loss\": 0.6660036516189575}, {\"index\": 29, \"loss\": 0.6640219342708588}, {\"index\": 30, \"loss\": 0.6622460079193115}, {\"index\": 31, \"loss\": 0.6615982300043106}, {\"index\": 32, \"loss\": 0.6620100575685501}, {\"index\": 33, \"loss\": 0.6599012869596481}, {\"index\": 34, \"loss\": 0.6580266493558884}, {\"index\": 35, \"loss\": 0.6606224220991135}, {\"index\": 36, \"loss\": 0.653327351808548}, {\"index\": 37, \"loss\": 0.6568241328001022}, {\"index\": 38, \"loss\": 0.6563822042942047}, {\"index\": 39, \"loss\": 0.6580005604028701}, {\"index\": 40, \"loss\": 0.6573260164260865}, {\"index\": 41, \"loss\": 0.6550242960453033}, {\"index\": 42, \"loss\": 0.6532027113437653}, {\"index\": 43, \"loss\": 0.6549425148963928}, {\"index\": 44, \"loss\": 0.6518672132492065}, {\"index\": 45, \"loss\": 0.6507021403312683}, {\"index\": 46, \"loss\": 0.6511314851045609}, {\"index\": 47, \"loss\": 0.6493401807546616}, {\"index\": 48, \"loss\": 0.6513130515813828}, {\"index\": 49, \"loss\": 0.6474511760473252}, {\"index\": 50, \"loss\": 0.6475563424825669}, {\"index\": 51, \"loss\": 0.6491761803627014}, {\"index\": 52, \"loss\": 0.646495189666748}, {\"index\": 53, \"loss\": 0.6482330185174942}, {\"index\": 54, \"loss\": 0.6471583890914917}, {\"index\": 55, \"loss\": 0.6469638955593109}, {\"index\": 56, \"loss\": 0.6447107326984406}, {\"index\": 57, \"loss\": 0.6488688957691192}, {\"index\": 58, \"loss\": 0.644291815161705}, {\"index\": 59, \"loss\": 0.6424019891023636}, {\"index\": 60, \"loss\": 0.6452833187580108}, {\"index\": 61, \"loss\": 0.6439376097917556}, {\"index\": 62, \"loss\": 0.64191446185112}, {\"index\": 63, \"loss\": 0.641640818119049}, {\"index\": 64, \"loss\": 0.641499376296997}, {\"index\": 65, \"loss\": 0.643196296095848}, {\"index\": 66, \"loss\": 0.6423700469732284}, {\"index\": 67, \"loss\": 0.6423394137620926}, {\"index\": 68, \"loss\": 0.640740973353386}, {\"index\": 69, \"loss\": 0.639789577126503}, {\"index\": 70, \"loss\": 0.6396370840072632}, {\"index\": 71, \"loss\": 0.6406670463085175}, {\"index\": 72, \"loss\": 0.6366140776872635}, {\"index\": 73, \"loss\": 0.6369581198692322}, {\"index\": 74, \"loss\": 0.6396029227972031}, {\"index\": 75, \"loss\": 0.6315380722284317}, {\"index\": 76, \"loss\": 0.6376498472690583}, {\"index\": 77, \"loss\": 0.6354412239789963}, {\"index\": 78, \"loss\": 0.6385137397050857}, {\"index\": 79, \"loss\": 0.6383276319503784}, {\"index\": 80, \"loss\": 0.6370250886678696}, {\"index\": 81, \"loss\": 0.6330444896221161}, {\"index\": 82, \"loss\": 0.6370842117071152}, {\"index\": 83, \"loss\": 0.6337122076749802}, {\"index\": 84, \"loss\": 0.6315430253744125}, {\"index\": 85, \"loss\": 0.6328783154487609}, {\"index\": 86, \"loss\": 0.6310020875930786}, {\"index\": 87, \"loss\": 0.6346406781673432}, {\"index\": 88, \"loss\": 0.6301492244005203}, {\"index\": 89, \"loss\": 0.6292591279745102}, {\"index\": 90, \"loss\": 0.6316657292842865}, {\"index\": 91, \"loss\": 0.6296899801492691}, {\"index\": 92, \"loss\": 0.6307220733165741}, {\"index\": 93, \"loss\": 0.6297509741783142}, {\"index\": 94, \"loss\": 0.6311568802595139}, {\"index\": 95, \"loss\": 0.6281743931770325}, {\"index\": 96, \"loss\": 0.633054964542389}, {\"index\": 97, \"loss\": 0.629061513543129}, {\"index\": 98, \"loss\": 0.6263463467359542}, {\"index\": 99, \"loss\": 0.6306125849485398}, {\"index\": 100, \"loss\": 0.627652867436409}, {\"index\": 101, \"loss\": 0.6268657499551773}, {\"index\": 102, \"loss\": 0.6266981816291809}, {\"index\": 103, \"loss\": 0.6260861557722092}, {\"index\": 104, \"loss\": 0.628054814338684}, {\"index\": 105, \"loss\": 0.628164758682251}, {\"index\": 106, \"loss\": 0.6268048501014709}, {\"index\": 107, \"loss\": 0.6254733490943909}, {\"index\": 108, \"loss\": 0.6256596916913986}, {\"index\": 109, \"loss\": 0.6247761577367783}, {\"index\": 110, \"loss\": 0.6262519419193268}, {\"index\": 111, \"loss\": 0.6204444223642349}, {\"index\": 112, \"loss\": 0.6229448825120926}, {\"index\": 113, \"loss\": 0.6253143787384033}, {\"index\": 114, \"loss\": 0.6166339099407196}, {\"index\": 115, \"loss\": 0.6229122334718704}, {\"index\": 116, \"loss\": 0.6215881615877151}, {\"index\": 117, \"loss\": 0.624553632736206}, {\"index\": 118, \"loss\": 0.6239985114336014}, {\"index\": 119, \"loss\": 0.6230830037593842}, {\"index\": 120, \"loss\": 0.6189994943141938}, {\"index\": 121, \"loss\": 0.6233213436603546}, {\"index\": 122, \"loss\": 0.6192450678348541}, {\"index\": 123, \"loss\": 0.616129738688469}, {\"index\": 124, \"loss\": 0.6193613785505295}, {\"index\": 125, \"loss\": 0.6165639728307724}, {\"index\": 126, \"loss\": 0.6208194476366043}, {\"index\": 127, \"loss\": 0.6164270770549775}, {\"index\": 128, \"loss\": 0.6145575809478759}, {\"index\": 129, \"loss\": 0.6186546546220779}, {\"index\": 130, \"loss\": 0.6153656929731369}, {\"index\": 131, \"loss\": 0.6168916898965836}, {\"index\": 132, \"loss\": 0.6163036179542541}, {\"index\": 133, \"loss\": 0.6163962322473526}, {\"index\": 134, \"loss\": 0.6145392042398453}, {\"index\": 135, \"loss\": 0.6194601511955261}, {\"index\": 136, \"loss\": 0.6140385794639588}, {\"index\": 137, \"loss\": 0.6133282554149627}, {\"index\": 138, \"loss\": 0.6173064506053925}, {\"index\": 139, \"loss\": 0.615152143239975}, {\"index\": 140, \"loss\": 0.6134259158372879}, {\"index\": 141, \"loss\": 0.6129000115394593}, {\"index\": 142, \"loss\": 0.6114913034439087}, {\"index\": 143, \"loss\": 0.6136061519384384}, {\"index\": 144, \"loss\": 0.6148206382989884}, {\"index\": 145, \"loss\": 0.6125273180007934}, {\"index\": 146, \"loss\": 0.6100163036584854}, {\"index\": 147, \"loss\": 0.6121541368961334}, {\"index\": 148, \"loss\": 0.6107990628480912}, {\"index\": 149, \"loss\": 0.6114864963293075}, {\"index\": 150, \"loss\": 0.6058306467533111}, {\"index\": 151, \"loss\": 0.6090242487192153}, {\"index\": 152, \"loss\": 0.6105043715238572}, {\"index\": 153, \"loss\": 0.6028415662050247}, {\"index\": 154, \"loss\": 0.6079321348667145}, {\"index\": 155, \"loss\": 0.6067861896753312}, {\"index\": 156, \"loss\": 0.6092326980829239}, {\"index\": 157, \"loss\": 0.609226456284523}, {\"index\": 158, \"loss\": 0.6078053057193756}, {\"index\": 159, \"loss\": 0.6049337214231492}, {\"index\": 160, \"loss\": 0.6097947943210602}, {\"index\": 161, \"loss\": 0.6045058625936508}, {\"index\": 162, \"loss\": 0.6005095362663269}, {\"index\": 163, \"loss\": 0.6065741807222367}, {\"index\": 164, \"loss\": 0.6029478687047959}, {\"index\": 165, \"loss\": 0.6065376961231231}, {\"index\": 166, \"loss\": 0.6032063007354737}, {\"index\": 167, \"loss\": 0.599818309545517}, {\"index\": 168, \"loss\": 0.6052167314291}, {\"index\": 169, \"loss\": 0.600522307753563}, {\"index\": 170, \"loss\": 0.603442997932434}, {\"index\": 171, \"loss\": 0.6022803801298141}, {\"index\": 172, \"loss\": 0.6021968448162078}, {\"index\": 173, \"loss\": 0.600517749786377}, {\"index\": 174, \"loss\": 0.6053851532936096}, {\"index\": 175, \"loss\": 0.5997870761156082}, {\"index\": 176, \"loss\": 0.6002996987104416}, {\"index\": 177, \"loss\": 0.6039984196424484}, {\"index\": 178, \"loss\": 0.6018348133563995}, {\"index\": 179, \"loss\": 0.5991020876169205}, {\"index\": 180, \"loss\": 0.5991898375749588}, {\"index\": 181, \"loss\": 0.5974952214956284}, {\"index\": 182, \"loss\": 0.6009010654687882}, {\"index\": 183, \"loss\": 0.6024283063411713}, {\"index\": 184, \"loss\": 0.5988524961471557}, {\"index\": 185, \"loss\": 0.5964517802000046}, {\"index\": 186, \"loss\": 0.5993347102403641}, {\"index\": 187, \"loss\": 0.5970890408754349}, {\"index\": 188, \"loss\": 0.5975341087579727}, {\"index\": 189, \"loss\": 0.5933432906866074}, {\"index\": 190, \"loss\": 0.5958909398317337}, {\"index\": 191, \"loss\": 0.5980145925283432}, {\"index\": 192, \"loss\": 0.5900442528724671}, {\"index\": 193, \"loss\": 0.5942780297994613}, {\"index\": 194, \"loss\": 0.5942103797197342}, {\"index\": 195, \"loss\": 0.5962415188550949}, {\"index\": 196, \"loss\": 0.5969720309972764}, {\"index\": 197, \"loss\": 0.5947628557682038}, {\"index\": 198, \"loss\": 0.5936487275362015}, {\"index\": 199, \"loss\": 0.5973521190881729}, {\"index\": 200, \"loss\": 0.5931372362375259}, {\"index\": 201, \"loss\": 0.5882780605554581}, {\"index\": 202, \"loss\": 0.5955078196525574}, {\"index\": 203, \"loss\": 0.5920525175333023}, {\"index\": 204, \"loss\": 0.5955384659767151}, {\"index\": 205, \"loss\": 0.5925927448272705}, {\"index\": 206, \"loss\": 0.5877890580892563}, {\"index\": 207, \"loss\": 0.5943007057905197}, {\"index\": 208, \"loss\": 0.5889057606458664}, {\"index\": 209, \"loss\": 0.5927602088451386}, {\"index\": 210, \"loss\": 0.5912286645174026}, {\"index\": 211, \"loss\": 0.5903249835968017}, {\"index\": 212, \"loss\": 0.5891457533836365}, {\"index\": 213, \"loss\": 0.5943720698356628}, {\"index\": 214, \"loss\": 0.5889141345024109}, {\"index\": 215, \"loss\": 0.590294291973114}, {\"index\": 216, \"loss\": 0.594288204908371}, {\"index\": 217, \"loss\": 0.5923843890428543}, {\"index\": 218, \"loss\": 0.5892511159181595}, {\"index\": 219, \"loss\": 0.5902282273769379}, {\"index\": 220, \"loss\": 0.587794200181961}, {\"index\": 221, \"loss\": 0.5917239832878113}, {\"index\": 222, \"loss\": 0.5922975260019302}, {\"index\": 223, \"loss\": 0.5898698645830155}, {\"index\": 224, \"loss\": 0.5856112223863602}, {\"index\": 225, \"loss\": 0.5903249722719193}, {\"index\": 226, \"loss\": 0.5885822743177413}, {\"index\": 227, \"loss\": 0.5878929978609085}, {\"index\": 228, \"loss\": 0.5838537281751632}, {\"index\": 229, \"loss\": 0.5871515995264054}, {\"index\": 230, \"loss\": 0.5893318858742714}, {\"index\": 231, \"loss\": 0.5822793054580688}, {\"index\": 232, \"loss\": 0.5853667712211609}, {\"index\": 233, \"loss\": 0.5860494038462639}, {\"index\": 234, \"loss\": 0.5870001530647277}, {\"index\": 235, \"loss\": 0.5870728880167008}, {\"index\": 236, \"loss\": 0.5852628397941589}, {\"index\": 237, \"loss\": 0.5851065868139267}, {\"index\": 238, \"loss\": 0.5891162389516831}, {\"index\": 239, \"loss\": 0.5855120867490768}, {\"index\": 240, \"loss\": 0.5789837974309922}, {\"index\": 241, \"loss\": 0.5866978800296784}, {\"index\": 242, \"loss\": 0.5842175948619842}, {\"index\": 243, \"loss\": 0.5875582486391068}, {\"index\": 244, \"loss\": 0.5848672437667847}, {\"index\": 245, \"loss\": 0.5793430453538895}, {\"index\": 246, \"loss\": 0.5856787878274917}, {\"index\": 247, \"loss\": 0.5809045439958572}, {\"index\": 248, \"loss\": 0.5851676374673843}, {\"index\": 249, \"loss\": 0.5835058403015136}, {\"index\": 250, \"loss\": 0.5829569435119629}, {\"index\": 251, \"loss\": 0.5819144302606583}, {\"index\": 252, \"loss\": 0.5865838581323624}, {\"index\": 253, \"loss\": 0.5803649091720581}, {\"index\": 254, \"loss\": 0.5829526686668396}, {\"index\": 255, \"loss\": 0.5881057369709015}, {\"index\": 256, \"loss\": 0.5850159019231796}, {\"index\": 257, \"loss\": 0.5812969070672989}, {\"index\": 258, \"loss\": 0.5835203036665917}, {\"index\": 259, \"loss\": 0.5798323595523834}, {\"index\": 260, \"loss\": 0.5844626414775849}, {\"index\": 261, \"loss\": 0.5857521444559097}, {\"index\": 262, \"loss\": 0.5826253861188888}, {\"index\": 263, \"loss\": 0.5778986793756485}, {\"index\": 264, \"loss\": 0.5830886173248291}, {\"index\": 265, \"loss\": 0.5816997766494751}, {\"index\": 266, \"loss\": 0.580376700758934}, {\"index\": 267, \"loss\": 0.5770685368776322}, {\"index\": 268, \"loss\": 0.5802837061882019}, {\"index\": 269, \"loss\": 0.583163315653801}, {\"index\": 270, \"loss\": 0.5757858318090439}, {\"index\": 271, \"loss\": 0.5793527400493622}, {\"index\": 272, \"loss\": 0.5795541653037071}, {\"index\": 273, \"loss\": 0.5797801059484482}, {\"index\": 274, \"loss\": 0.5801221191883087}, {\"index\": 275, \"loss\": 0.5788306391239166}, {\"index\": 276, \"loss\": 0.5781962877511978}, {\"index\": 277, \"loss\": 0.5833013224601745}, {\"index\": 278, \"loss\": 0.5796503782272339}, {\"index\": 279, \"loss\": 0.5728133887052536}, {\"index\": 280, \"loss\": 0.58120372235775}, {\"index\": 281, \"loss\": 0.5784115886688233}, {\"index\": 282, \"loss\": 0.5809568947553635}, {\"index\": 283, \"loss\": 0.5787389582395553}, {\"index\": 284, \"loss\": 0.572881611585617}, {\"index\": 285, \"loss\": 0.5792629712820053}, {\"index\": 286, \"loss\": 0.5752333170175552}, {\"index\": 287, \"loss\": 0.57939424097538}, {\"index\": 288, \"loss\": 0.5772191435098648}, {\"index\": 289, \"loss\": 0.5765046977996826}, {\"index\": 290, \"loss\": 0.5763968867063523}, {\"index\": 291, \"loss\": 0.5799616920948029}, {\"index\": 292, \"loss\": 0.5748552989959717}, {\"index\": 293, \"loss\": 0.5774756491184234}, {\"index\": 294, \"loss\": 0.5824441415071487}, {\"index\": 295, \"loss\": 0.5793430864810943}, {\"index\": 296, \"loss\": 0.5752261954545975}, {\"index\": 297, \"loss\": 0.5779349306225776}, {\"index\": 298, \"loss\": 0.5742907252907753}, {\"index\": 299, \"loss\": 0.5785861206054688}, {\"index\": 300, \"loss\": 0.5801891708374023}, {\"index\": 301, \"loss\": 0.5768881583213806}, {\"index\": 302, \"loss\": 0.5719147664308548}, {\"index\": 303, \"loss\": 0.5774874132871628}, {\"index\": 304, \"loss\": 0.5762441313266754}, {\"index\": 305, \"loss\": 0.5749170452356338}, {\"index\": 306, \"loss\": 0.5711619997024536}, {\"index\": 307, \"loss\": 0.575010199546814}, {\"index\": 308, \"loss\": 0.5783823001384735}, {\"index\": 309, \"loss\": 0.57082659304142}, {\"index\": 310, \"loss\": 0.5734494531154632}, {\"index\": 311, \"loss\": 0.574608588218689}, {\"index\": 312, \"loss\": 0.5740159785747528}, {\"index\": 313, \"loss\": 0.5743408417701721}, {\"index\": 314, \"loss\": 0.5729452091455459}, {\"index\": 315, \"loss\": 0.5730185323953628}, {\"index\": 316, \"loss\": 0.5779135274887085}, {\"index\": 317, \"loss\": 0.5746563655138016}, {\"index\": 318, \"loss\": 0.5677518516778945}, {\"index\": 319, \"loss\": 0.5762153780460357}, {\"index\": 320, \"loss\": 0.5735938829183579}, {\"index\": 321, \"loss\": 0.5751995146274567}, {\"index\": 322, \"loss\": 0.573691861629486}, {\"index\": 323, \"loss\": 0.5682676011323928}, {\"index\": 324, \"loss\": 0.5737974286079407}, {\"index\": 325, \"loss\": 0.5702671492099762}, {\"index\": 326, \"loss\": 0.574567990899086}, {\"index\": 327, \"loss\": 0.5718751877546311}, {\"index\": 328, \"loss\": 0.5718732070922852}, {\"index\": 329, \"loss\": 0.5715205848217011}, {\"index\": 330, \"loss\": 0.5755939343571663}, {\"index\": 331, \"loss\": 0.5697304600477219}, {\"index\": 332, \"loss\": 0.5733056581020355}, {\"index\": 333, \"loss\": 0.577296633720398}, {\"index\": 334, \"loss\": 0.5743646943569183}, {\"index\": 335, \"loss\": 0.5703877311944962}, {\"index\": 336, \"loss\": 0.5734132021665573}, {\"index\": 337, \"loss\": 0.569889485836029}, {\"index\": 338, \"loss\": 0.5736596637964249}, {\"index\": 339, \"loss\": 0.5758865642547607}, {\"index\": 340, \"loss\": 0.5722615116834641}, {\"index\": 341, \"loss\": 0.5670960754156112}, {\"index\": 342, \"loss\": 0.573104675412178}, {\"index\": 343, \"loss\": 0.5722793406248092}, {\"index\": 344, \"loss\": 0.5702649462223053}, {\"index\": 345, \"loss\": 0.5667561483383179}, {\"index\": 346, \"loss\": 0.5706499898433686}, {\"index\": 347, \"loss\": 0.5742816862463951}, {\"index\": 348, \"loss\": 0.5665544813871384}, {\"index\": 349, \"loss\": 0.5690647566318512}, {\"index\": 350, \"loss\": 0.5709279435873031}, {\"index\": 351, \"loss\": 0.5696056586503982}, {\"index\": 352, \"loss\": 0.5691741216182709}, {\"index\": 353, \"loss\": 0.5692999869585037}, {\"index\": 354, \"loss\": 0.5684582871198655}, {\"index\": 355, \"loss\": 0.5737699085474014}, {\"index\": 356, \"loss\": 0.5700921422243118}, {\"index\": 357, \"loss\": 0.563496262729168}, {\"index\": 358, \"loss\": 0.5720748633146286}, {\"index\": 359, \"loss\": 0.5690920865535736}, {\"index\": 360, \"loss\": 0.570953699350357}, {\"index\": 361, \"loss\": 0.5697155737876892}, {\"index\": 362, \"loss\": 0.5635164833068848}, {\"index\": 363, \"loss\": 0.5698095619678497}, {\"index\": 364, \"loss\": 0.5663243907690049}, {\"index\": 365, \"loss\": 0.5703956001996994}, {\"index\": 366, \"loss\": 0.5675119769573211}, {\"index\": 367, \"loss\": 0.5679315865039826}, {\"index\": 368, \"loss\": 0.5675944876670838}, {\"index\": 369, \"loss\": 0.5712799316644669}, {\"index\": 370, \"loss\": 0.565907576084137}, {\"index\": 371, \"loss\": 0.5687279069423675}, {\"index\": 372, \"loss\": 0.5733434838056565}, {\"index\": 373, \"loss\": 0.5705764567852021}, {\"index\": 374, \"loss\": 0.5659299510717392}, {\"index\": 375, \"loss\": 0.5699477958679199}, {\"index\": 376, \"loss\": 0.565580969452858}, {\"index\": 377, \"loss\": 0.5694890511035919}, {\"index\": 378, \"loss\": 0.57227321267128}, {\"index\": 379, \"loss\": 0.5683014860749245}, {\"index\": 380, \"loss\": 0.5631897741556168}, {\"index\": 381, \"loss\": 0.5693767684698104}, {\"index\": 382, \"loss\": 0.5683899736404419}, {\"index\": 383, \"loss\": 0.5661151194572449}, {\"index\": 384, \"loss\": 0.5635400018095971}, {\"index\": 385, \"loss\": 0.5669391757249832}, {\"index\": 386, \"loss\": 0.5705444559454917}, {\"index\": 387, \"loss\": 0.5626657810807228}, {\"index\": 388, \"loss\": 0.564980428814888}, {\"index\": 389, \"loss\": 0.5675050759315491}, {\"index\": 390, \"loss\": 0.5657182687520981}, {\"index\": 391, \"loss\": 0.5654260402917862}, {\"index\": 392, \"loss\": 0.5655206525325776}, {\"index\": 393, \"loss\": 0.5650102829933167}, {\"index\": 394, \"loss\": 0.5705524218082428}, {\"index\": 395, \"loss\": 0.5667440265417099}, {\"index\": 396, \"loss\": 0.5606427255272866}, {\"index\": 397, \"loss\": 0.5678542959690094}, {\"index\": 398, \"loss\": 0.5654720309376716}, {\"index\": 399, \"loss\": 0.5670727205276489}, {\"index\": 400, \"loss\": 0.5658626282215118}, {\"index\": 401, \"loss\": 0.5606121379137039}, {\"index\": 402, \"loss\": 0.5659973800182343}, {\"index\": 403, \"loss\": 0.5622528877854347}, {\"index\": 404, \"loss\": 0.5665690636634827}, {\"index\": 405, \"loss\": 0.5645198547840118}, {\"index\": 406, \"loss\": 0.5644712394475937}, {\"index\": 407, \"loss\": 0.5639366471767425}, {\"index\": 408, \"loss\": 0.5677061232924462}, {\"index\": 409, \"loss\": 0.5626518869400025}, {\"index\": 410, \"loss\": 0.5651754355430603}, {\"index\": 411, \"loss\": 0.5703305089473725}, {\"index\": 412, \"loss\": 0.5671075338125229}, {\"index\": 413, \"loss\": 0.5628057044744491}, {\"index\": 414, \"loss\": 0.5664160683751106}, {\"index\": 415, \"loss\": 0.5623641222715378}, {\"index\": 416, \"loss\": 0.5656847041845322}, {\"index\": 417, \"loss\": 0.5686058205366135}, {\"index\": 418, \"loss\": 0.5647524079680443}, {\"index\": 419, \"loss\": 0.559026083946228}, {\"index\": 420, \"loss\": 0.5658394849300384}, {\"index\": 421, \"loss\": 0.5654646933078766}, {\"index\": 422, \"loss\": 0.5626782846450805}, {\"index\": 423, \"loss\": 0.5597759485244751}, {\"index\": 424, \"loss\": 0.56435965269804}, {\"index\": 425, \"loss\": 0.5673613432049751}, {\"index\": 426, \"loss\": 0.5591925713419914}, {\"index\": 427, \"loss\": 0.5615997248888016}, {\"index\": 428, \"loss\": 0.5646019303798675}, {\"index\": 429, \"loss\": 0.5618327987194062}, {\"index\": 430, \"loss\": 0.5620093214511871}, {\"index\": 431, \"loss\": 0.5617370641231537}, {\"index\": 432, \"loss\": 0.5614068353176117}, {\"index\": 433, \"loss\": 0.566981166601181}, {\"index\": 434, \"loss\": 0.5641884183883668}, {\"index\": 435, \"loss\": 0.557633849978447}, {\"index\": 436, \"loss\": 0.564537581205368}, {\"index\": 437, \"loss\": 0.5622833967208862}, {\"index\": 438, \"loss\": 0.563265218436718}, {\"index\": 439, \"loss\": 0.562701969742775}, {\"index\": 440, \"loss\": 0.5570246958732605}, {\"index\": 441, \"loss\": 0.5632614868879319}, {\"index\": 442, \"loss\": 0.5594967138767243}, {\"index\": 443, \"loss\": 0.5633912318944931}, {\"index\": 444, \"loss\": 0.5615125894546509}, {\"index\": 445, \"loss\": 0.5612037432193756}, {\"index\": 446, \"loss\": 0.5607806608080864}, {\"index\": 447, \"loss\": 0.5644568994641304}, {\"index\": 448, \"loss\": 0.5592670261859893}, {\"index\": 449, \"loss\": 0.5620187050104142}, {\"index\": 450, \"loss\": 0.5673348313570022}, {\"index\": 451, \"loss\": 0.5636817437410354}, {\"index\": 452, \"loss\": 0.5597356024384499}, {\"index\": 453, \"loss\": 0.5632461729645729}, {\"index\": 454, \"loss\": 0.559643662571907}, {\"index\": 455, \"loss\": 0.5627643299102784}, {\"index\": 456, \"loss\": 0.5650669622421265}, {\"index\": 457, \"loss\": 0.5616303387284279}, {\"index\": 458, \"loss\": 0.5559196585416794}, {\"index\": 459, \"loss\": 0.562634466290474}, {\"index\": 460, \"loss\": 0.562154876589775}, {\"index\": 461, \"loss\": 0.5600348481535912}, {\"index\": 462, \"loss\": 0.55664779484272}, {\"index\": 463, \"loss\": 0.5615979748964309}, {\"index\": 464, \"loss\": 0.5642895746231079}, {\"index\": 465, \"loss\": 0.5567352241277694}, {\"index\": 466, \"loss\": 0.557840091586113}, {\"index\": 467, \"loss\": 0.5613714250922203}, {\"index\": 468, \"loss\": 0.5589598685503006}, {\"index\": 469, \"loss\": 0.5589618986845016}, {\"index\": 470, \"loss\": 0.559392758011818}, {\"index\": 471, \"loss\": 0.558838469684124}, {\"index\": 472, \"loss\": 0.5644228005409241}, {\"index\": 473, \"loss\": 0.5610151755809784}, {\"index\": 474, \"loss\": 0.555360572040081}, {\"index\": 475, \"loss\": 0.5616132485866546}, {\"index\": 476, \"loss\": 0.5591498371958733}, {\"index\": 477, \"loss\": 0.5596141800284385}, {\"index\": 478, \"loss\": 0.5597695511579514}, {\"index\": 479, \"loss\": 0.5540676090121269}, {\"index\": 480, \"loss\": 0.56018898665905}, {\"index\": 481, \"loss\": 0.557098733484745}, {\"index\": 482, \"loss\": 0.5607675051689148}, {\"index\": 483, \"loss\": 0.5587390777468682}, {\"index\": 484, \"loss\": 0.5583554095029831}, {\"index\": 485, \"loss\": 0.5584070754051208}, {\"index\": 486, \"loss\": 0.5610392189025879}, {\"index\": 487, \"loss\": 0.556608407497406}, {\"index\": 488, \"loss\": 0.5587437397241592}, {\"index\": 489, \"loss\": 0.5637976163625718}, {\"index\": 490, \"loss\": 0.5610581547021866}, {\"index\": 491, \"loss\": 0.5573216858506203}, {\"index\": 492, \"loss\": 0.5603244572877883}, {\"index\": 493, \"loss\": 0.5564451560378074}, {\"index\": 494, \"loss\": 0.560069985985756}, {\"index\": 495, \"loss\": 0.5627190008759498}, {\"index\": 496, \"loss\": 0.5585530376434327}, {\"index\": 497, \"loss\": 0.5532404920458793}, {\"index\": 498, \"loss\": 0.5597574573755264}, {\"index\": 499, \"loss\": 0.5594367176294327}, {\"index\": 500, \"loss\": 0.5567298153042793}, {\"index\": 501, \"loss\": 0.5541131958365441}, {\"index\": 502, \"loss\": 0.5583981359004975}, {\"index\": 503, \"loss\": 0.5611746940016746}, {\"index\": 504, \"loss\": 0.5542564526200294}, {\"index\": 505, \"loss\": 0.5553299036622047}, {\"index\": 506, \"loss\": 0.5593375131487847}, {\"index\": 507, \"loss\": 0.5560493564605713}, {\"index\": 508, \"loss\": 0.5560606807470322}, {\"index\": 509, \"loss\": 0.5567694383859635}, {\"index\": 510, \"loss\": 0.5565677338838577}, {\"index\": 511, \"loss\": 0.5618625980615616}, {\"index\": 512, \"loss\": 0.5585132566094398}, {\"index\": 513, \"loss\": 0.5529768735170364}, {\"index\": 514, \"loss\": 0.5585352239012719}, {\"index\": 515, \"loss\": 0.5568680718541146}, {\"index\": 516, \"loss\": 0.5568475982546807}, {\"index\": 517, \"loss\": 0.5569275695085526}, {\"index\": 518, \"loss\": 0.5515092289447785}, {\"index\": 519, \"loss\": 0.5570951101183891}, {\"index\": 520, \"loss\": 0.5541019314527511}, {\"index\": 521, \"loss\": 0.5581586366891861}, {\"index\": 522, \"loss\": 0.5556168046593666}, {\"index\": 523, \"loss\": 0.555391113460064}, {\"index\": 524, \"loss\": 0.5555828499794007}, {\"index\": 525, \"loss\": 0.5592532789707184}, {\"index\": 526, \"loss\": 0.5532135486602783}, {\"index\": 527, \"loss\": 0.5560299289226532}, {\"index\": 528, \"loss\": 0.5615113049745559}, {\"index\": 529, \"loss\": 0.5584860530495643}, {\"index\": 530, \"loss\": 0.5551275435090065}, {\"index\": 531, \"loss\": 0.5574778607487678}, {\"index\": 532, \"loss\": 0.5536458531022072}, {\"index\": 533, \"loss\": 0.5580507361888886}, {\"index\": 534, \"loss\": 0.5603470075130462}, {\"index\": 535, \"loss\": 0.5559603160619736}, {\"index\": 536, \"loss\": 0.550174423456192}, {\"index\": 537, \"loss\": 0.556869145333767}, {\"index\": 538, \"loss\": 0.5566050836443901}, {\"index\": 539, \"loss\": 0.5544809776544571}, {\"index\": 540, \"loss\": 0.5513463774323464}, {\"index\": 541, \"loss\": 0.5559437331557274}, {\"index\": 542, \"loss\": 0.5582890459895133}, {\"index\": 543, \"loss\": 0.551537135541439}, {\"index\": 544, \"loss\": 0.5525408408045769}, {\"index\": 545, \"loss\": 0.5570849984884262}, {\"index\": 546, \"loss\": 0.5535028085112572}, {\"index\": 547, \"loss\": 0.5538049161434173}, {\"index\": 548, \"loss\": 0.5542976361513138}, {\"index\": 549, \"loss\": 0.5535798969864846}, {\"index\": 550, \"loss\": 0.5590200144052505}, {\"index\": 551, \"loss\": 0.5560929024219513}, {\"index\": 552, \"loss\": 0.5500107401609421}, {\"index\": 553, \"loss\": 0.5562643760442734}, {\"index\": 554, \"loss\": 0.5539378756284714}, {\"index\": 555, \"loss\": 0.5544803342223168}, {\"index\": 556, \"loss\": 0.5541825893521309}, {\"index\": 557, \"loss\": 0.5490764990448952}, {\"index\": 558, \"loss\": 0.5547755950689316}, {\"index\": 559, \"loss\": 0.5516706117987633}, {\"index\": 560, \"loss\": 0.556294292807579}, {\"index\": 561, \"loss\": 0.5530972129106522}, {\"index\": 562, \"loss\": 0.5529803130030632}, {\"index\": 563, \"loss\": 0.5532173952460289}, {\"index\": 564, \"loss\": 0.5563951274752617}, {\"index\": 565, \"loss\": 0.5511395418643952}, {\"index\": 566, \"loss\": 0.553879811167717}, {\"index\": 567, \"loss\": 0.5593372708559037}, {\"index\": 568, \"loss\": 0.5560822442173958}, {\"index\": 569, \"loss\": 0.5526095786690712}, {\"index\": 570, \"loss\": 0.5555092534422874}, {\"index\": 571, \"loss\": 0.5512070021033287}, {\"index\": 572, \"loss\": 0.5555164688825607}, {\"index\": 573, \"loss\": 0.5577877303957939}, {\"index\": 574, \"loss\": 0.5533122518658637}, {\"index\": 575, \"loss\": 0.5480104181170463}, {\"index\": 576, \"loss\": 0.5549113836884498}, {\"index\": 577, \"loss\": 0.554444514811039}, {\"index\": 578, \"loss\": 0.5521060174703598}, {\"index\": 579, \"loss\": 0.549158108830452}, {\"index\": 580, \"loss\": 0.553545772433281}, {\"index\": 581, \"loss\": 0.5557973831892014}, {\"index\": 582, \"loss\": 0.5495469725131988}, {\"index\": 583, \"loss\": 0.5500690469145775}, {\"index\": 584, \"loss\": 0.5550914952158927}, {\"index\": 585, \"loss\": 0.5513914597034454}, {\"index\": 586, \"loss\": 0.5516941860318184}, {\"index\": 587, \"loss\": 0.5519880646467209}, {\"index\": 588, \"loss\": 0.5513943848013878}, {\"index\": 589, \"loss\": 0.5567786079645157}, {\"index\": 590, \"loss\": 0.5540455251932144}, {\"index\": 591, \"loss\": 0.5481537413597107}, {\"index\": 592, \"loss\": 0.5532337662577629}, {\"index\": 593, \"loss\": 0.5513831883668899}, {\"index\": 594, \"loss\": 0.5523276564478874}, {\"index\": 595, \"loss\": 0.5521802014112472}, {\"index\": 596, \"loss\": 0.5468030226230621}, {\"index\": 597, \"loss\": 0.5526387012004852}, {\"index\": 598, \"loss\": 0.5489741560816764}, {\"index\": 599, \"loss\": 0.5537128877639771}, {\"index\": 600, \"loss\": 0.5510482317209244}, {\"index\": 601, \"loss\": 0.5508654597401619}, {\"index\": 602, \"loss\": 0.5513183477520943}, {\"index\": 603, \"loss\": 0.5543512639403343}, {\"index\": 604, \"loss\": 0.5484160295128823}, {\"index\": 605, \"loss\": 0.5511585134267807}, {\"index\": 606, \"loss\": 0.5567764464020729}, {\"index\": 607, \"loss\": 0.5536968603730201}, {\"index\": 608, \"loss\": 0.5506455540657044}, {\"index\": 609, \"loss\": 0.5523335605859756}, {\"index\": 610, \"loss\": 0.5490669104456901}, {\"index\": 611, \"loss\": 0.5528550902009011}, {\"index\": 612, \"loss\": 0.5553685501217842}, {\"index\": 613, \"loss\": 0.5509432309865951}, {\"index\": 614, \"loss\": 0.5457383173704148}, {\"index\": 615, \"loss\": 0.5532659304141998}, {\"index\": 616, \"loss\": 0.5524498108029365}, {\"index\": 617, \"loss\": 0.5496254539489747}, {\"index\": 618, \"loss\": 0.5466731026768684}, {\"index\": 619, \"loss\": 0.5517108970880509}, {\"index\": 620, \"loss\": 0.5531576898694038}, {\"index\": 621, \"loss\": 0.5474896889925003}, {\"index\": 622, \"loss\": 0.5472456738352776}, {\"index\": 623, \"loss\": 0.5524150964617729}, {\"index\": 624, \"loss\": 0.549031600356102}, {\"index\": 625, \"loss\": 0.5496220058202743}, {\"index\": 626, \"loss\": 0.5495354467630387}, {\"index\": 627, \"loss\": 0.5491132593154907}, {\"index\": 628, \"loss\": 0.554396153986454}, {\"index\": 629, \"loss\": 0.5518841159343719}, {\"index\": 630, \"loss\": 0.5460532706975937}, {\"index\": 631, \"loss\": 0.5510513028502464}, {\"index\": 632, \"loss\": 0.549143118262291}, {\"index\": 633, \"loss\": 0.5501483079791069}, {\"index\": 634, \"loss\": 0.5499324965476989}, {\"index\": 635, \"loss\": 0.544835299551487}, {\"index\": 636, \"loss\": 0.5503800964355469}, {\"index\": 637, \"loss\": 0.5468233293294906}, {\"index\": 638, \"loss\": 0.5513523101806641}, {\"index\": 639, \"loss\": 0.5483725550770759}, {\"index\": 640, \"loss\": 0.5493012046813965}, {\"index\": 641, \"loss\": 0.5490622320771217}, {\"index\": 642, \"loss\": 0.5520072227716446}, {\"index\": 643, \"loss\": 0.5468115672469139}, {\"index\": 644, \"loss\": 0.5491239354014397}, {\"index\": 645, \"loss\": 0.5544260230660438}, {\"index\": 646, \"loss\": 0.5518224668502808}, {\"index\": 647, \"loss\": 0.5486245083808899}, {\"index\": 648, \"loss\": 0.550782088637352}, {\"index\": 649, \"loss\": 0.5472582221031189}, {\"index\": 650, \"loss\": 0.5506964504718781}, {\"index\": 651, \"loss\": 0.5532073917984962}, {\"index\": 652, \"loss\": 0.5487712061405182}, {\"index\": 653, \"loss\": 0.543240737915039}, {\"index\": 654, \"loss\": 0.5509706702828407}, {\"index\": 655, \"loss\": 0.5500452139973641}, {\"index\": 656, \"loss\": 0.5472546535730362}, {\"index\": 657, \"loss\": 0.5447849482297897}, {\"index\": 658, \"loss\": 0.5496963366866112}, {\"index\": 659, \"loss\": 0.5508771869540214}, {\"index\": 660, \"loss\": 0.5449906155467034}, {\"index\": 661, \"loss\": 0.5448362603783607}, {\"index\": 662, \"loss\": 0.5501357516646386}, {\"index\": 663, \"loss\": 0.5469556146860123}, {\"index\": 664, \"loss\": 0.5472420910000801}, {\"index\": 665, \"loss\": 0.5475633117556572}, {\"index\": 666, \"loss\": 0.546801028251648}, {\"index\": 667, \"loss\": 0.5518112856149674}, {\"index\": 668, \"loss\": 0.5497928139567375}, {\"index\": 669, \"loss\": 0.5442757871747017}, {\"index\": 670, \"loss\": 0.5486274173855782}, {\"index\": 671, \"loss\": 0.5469372686743736}, {\"index\": 672, \"loss\": 0.5480843645334244}, {\"index\": 673, \"loss\": 0.5477658104896546}, {\"index\": 674, \"loss\": 0.5424784874916077}, {\"index\": 675, \"loss\": 0.5484804937243462}, {\"index\": 676, \"loss\": 0.5450212058424949}, {\"index\": 677, \"loss\": 0.5490584549307823}, {\"index\": 678, \"loss\": 0.546262591779232}, {\"index\": 679, \"loss\": 0.5470563349127769}, {\"index\": 680, \"loss\": 0.5470343497395516}, {\"index\": 681, \"loss\": 0.5498588889837265}, {\"index\": 682, \"loss\": 0.5443941101431846}, {\"index\": 683, \"loss\": 0.5473169800639153}, {\"index\": 684, \"loss\": 0.5529681375622749}, {\"index\": 685, \"loss\": 0.5497677835822106}, {\"index\": 686, \"loss\": 0.5461112743616104}, {\"index\": 687, \"loss\": 0.5488187581300735}, {\"index\": 688, \"loss\": 0.5451437735557556}, {\"index\": 689, \"loss\": 0.5487841713428497}, {\"index\": 690, \"loss\": 0.5515107327699661}, {\"index\": 691, \"loss\": 0.547095830142498}, {\"index\": 692, \"loss\": 0.5408668556809425}, {\"index\": 693, \"loss\": 0.5491921386122703}, {\"index\": 694, \"loss\": 0.547925278544426}, {\"index\": 695, \"loss\": 0.5449228265881538}, {\"index\": 696, \"loss\": 0.542877829670906}, {\"index\": 697, \"loss\": 0.5479232272505761}, {\"index\": 698, \"loss\": 0.5487638747692108}, {\"index\": 699, \"loss\": 0.5426777884364128}, {\"index\": 700, \"loss\": 0.5429051786661148}, {\"index\": 701, \"loss\": 0.5484302699565887}, {\"index\": 702, \"loss\": 0.5451142236590385}, {\"index\": 703, \"loss\": 0.5456570553779602}, {\"index\": 704, \"loss\": 0.5452892008423805}, {\"index\": 705, \"loss\": 0.5442026782035828}, {\"index\": 706, \"loss\": 0.550062934756279}, {\"index\": 707, \"loss\": 0.548094955086708}, {\"index\": 708, \"loss\": 0.5423763686418533}, {\"index\": 709, \"loss\": 0.5466206336021423}, {\"index\": 710, \"loss\": 0.5448458138108253}, {\"index\": 711, \"loss\": 0.5459883561730385}, {\"index\": 712, \"loss\": 0.5452760341763496}, {\"index\": 713, \"loss\": 0.5405149102210999}, {\"index\": 714, \"loss\": 0.5463978865742684}, {\"index\": 715, \"loss\": 0.5427871009707451}, {\"index\": 716, \"loss\": 0.5467943614721298}, {\"index\": 717, \"loss\": 0.5443468543887139}, {\"index\": 718, \"loss\": 0.5454125583171845}, {\"index\": 719, \"loss\": 0.5451350951194763}, {\"index\": 720, \"loss\": 0.5474005883932114}, {\"index\": 721, \"loss\": 0.5424450924992561}, {\"index\": 722, \"loss\": 0.5448209902644158}, {\"index\": 723, \"loss\": 0.5504839646816254}, {\"index\": 724, \"loss\": 0.5479173663258553}, {\"index\": 725, \"loss\": 0.5440990403294563}, {\"index\": 726, \"loss\": 0.5466855707764625}, {\"index\": 727, \"loss\": 0.5434517422318459}, {\"index\": 728, \"loss\": 0.5465580272674561}, {\"index\": 729, \"loss\": 0.5499380841851235}, {\"index\": 730, \"loss\": 0.5447227874398232}, {\"index\": 731, \"loss\": 0.5385597178339958}, {\"index\": 732, \"loss\": 0.5473308736085891}, {\"index\": 733, \"loss\": 0.546259137392044}, {\"index\": 734, \"loss\": 0.5431779766082764}, {\"index\": 735, \"loss\": 0.5405705291032791}, {\"index\": 736, \"loss\": 0.5460217118263244}, {\"index\": 737, \"loss\": 0.5463823634386062}, {\"index\": 738, \"loss\": 0.5401642075181008}, {\"index\": 739, \"loss\": 0.5406224742531777}, {\"index\": 740, \"loss\": 0.5467198067903518}, {\"index\": 741, \"loss\": 0.5429223212599754}, {\"index\": 742, \"loss\": 0.5432314205169678}, {\"index\": 743, \"loss\": 0.5431006443500519}, {\"index\": 744, \"loss\": 0.5421360883116723}, {\"index\": 745, \"loss\": 0.5479258614778518}, {\"index\": 746, \"loss\": 0.5457988882064819}, {\"index\": 747, \"loss\": 0.5407216587662697}, {\"index\": 748, \"loss\": 0.5444314101338387}, {\"index\": 749, \"loss\": 0.5431543058156967}, {\"index\": 750, \"loss\": 0.5443599393963814}, {\"index\": 751, \"loss\": 0.5435427379608154}, {\"index\": 752, \"loss\": 0.5386322811245918}, {\"index\": 753, \"loss\": 0.5444628882408142}, {\"index\": 754, \"loss\": 0.5412010765075683}, {\"index\": 755, \"loss\": 0.5456208273768425}, {\"index\": 756, \"loss\": 0.5422689130902291}, {\"index\": 757, \"loss\": 0.5436842906475067}, {\"index\": 758, \"loss\": 0.5435161155462265}, {\"index\": 759, \"loss\": 0.5454412397742271}, {\"index\": 760, \"loss\": 0.5407373085618019}, {\"index\": 761, \"loss\": 0.5432808494567871}, {\"index\": 762, \"loss\": 0.5490406301617622}, {\"index\": 763, \"loss\": 0.5463136675953865}, {\"index\": 764, \"loss\": 0.5422685268521309}, {\"index\": 765, \"loss\": 0.5455471539497375}, {\"index\": 766, \"loss\": 0.5411051842570305}, {\"index\": 767, \"loss\": 0.5448576074838638}, {\"index\": 768, \"loss\": 0.5481530514359474}, {\"index\": 769, \"loss\": 0.5427794098854065}, {\"index\": 770, \"loss\": 0.5367647254467011}, {\"index\": 771, \"loss\": 0.5456964200735093}, {\"index\": 772, \"loss\": 0.5443611362576485}, {\"index\": 773, \"loss\": 0.5416564685106278}, {\"index\": 774, \"loss\": 0.538955281674862}, {\"index\": 775, \"loss\": 0.5444763213396072}, {\"index\": 776, \"loss\": 0.5444747522473335}, {\"index\": 777, \"loss\": 0.5387110957503318}, {\"index\": 778, \"loss\": 0.538515048623085}, {\"index\": 779, \"loss\": 0.5448480558395385}]}, {\"name\": \"source_0_x_domain_index\", \"values\": [{\"min\": 0, \"max\": 779}]}, {\"name\": \"source_0_y_domain_loss\", \"values\": [{\"min\": 0.5367647254467011, \"max\": 0.8010361850261688}]}], \"marks\": [{\"type\": \"line\", \"name\": \"marks\", \"from\": {\"data\": \"source_0\"}, \"sort\": {\"field\": \"x\"}, \"encode\": {\"update\": {\"x\": {\"field\": \"index\", \"scale\": \"x\"}, \"stroke\": {\"value\": \"#4c78a8\"}, \"defined\": {\"signal\": \"isValid(datum[\\\"index\\\"]) && isFinite(+datum[\\\"index\\\"]) && isValid(datum[\\\"loss\\\"]) && isFinite(+datum[\\\"loss\\\"])\"}, \"y\": {\"field\": \"loss\", \"scale\": \"y\"}}}, \"style\": [\"line\"]}], \"scales\": [{\"name\": \"x\", \"type\": \"linear\", \"domain\": [{\"signal\": \"(data(\\\"source_0_x_domain_index\\\")[0] || {}).min\"}, {\"signal\": \"(data(\\\"source_0_x_domain_index\\\")[0] || {}).max\"}], \"range\": [0, {\"signal\": \"width\"}], \"nice\": true, \"zero\": false}, {\"name\": \"y\", \"type\": \"linear\", \"domain\": [{\"signal\": \"(data(\\\"source_0_y_domain_loss\\\")[0] || {}).min\"}, {\"signal\": \"(data(\\\"source_0_y_domain_loss\\\")[0] || {}).max\"}], \"range\": [{\"signal\": \"height\"}, 0], \"zero\": true, \"nice\": true}], \"axes\": [{\"scale\": \"x\", \"labels\": false, \"gridScale\": \"y\", \"tickCount\": {\"signal\": \"ceil(width/40)\"}, \"domain\": false, \"grid\": true, \"zindex\": 0, \"orient\": \"bottom\", \"minExtent\": 0, \"aria\": false, \"ticks\": false, \"maxExtent\": 0}, {\"scale\": \"y\", \"aria\": false, \"minExtent\": 0, \"ticks\": false, \"grid\": true, \"zindex\": 0, \"maxExtent\": 0, \"gridScale\": \"x\", \"orient\": \"left\", \"tickCount\": {\"signal\": \"ceil(height/40)\"}, \"labels\": false, \"domain\": false}, {\"scale\": \"x\", \"grid\": false, \"orient\": \"bottom\", \"labelFlush\": true, \"title\": \"index\", \"tickCount\": {\"signal\": \"ceil(width/40)\"}, \"zindex\": 0, \"labelOverlap\": true}, {\"scale\": \"y\", \"title\": \"loss\", \"orient\": \"left\", \"zindex\": 0, \"tickCount\": {\"signal\": \"ceil(height/40)\"}, \"labelOverlap\": true, \"grid\": false}], \"title\": {\"text\": \"Direct Downstream Train Loss\", \"frame\": \"group\"}, \"padding\": 5, \"width\": 700, \"style\": \"cell\", \"height\": 500, \"background\": \"white\"}, {\"mode\": \"vega\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lB0YWA_wZP_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
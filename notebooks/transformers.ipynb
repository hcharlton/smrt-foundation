{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34aceb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 32])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Dummy Data\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "d_model = 32\n",
    "\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf2db3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([4, 10, 32])\n",
      "Output: torch.Size([4, 10, 64])\n",
      "Weights: torch.Size([4, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "class SingleHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, mask=None):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        self.W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_v, bias=False)\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if self.mask is not None:\n",
    "            scores = scores.masked_fill(self.mask, -1e9)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = attn_weights @ V\n",
    "        return output, attn_weights\n",
    "\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "single_attn = SingleHeadSelfAttention(d_model, d_k = 64, d_v = 64, mask = causal_mask)\n",
    "\n",
    "outputs, weights = single_attn(x)\n",
    "\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Output: {outputs.shape}\")\n",
    "print(f\"Weights: {weights.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fefbd280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1374, -0.0454, -0.3611,  ..., -0.0726, -0.0083,  0.1151],\n",
       "         [-0.0951, -0.1035, -0.2478,  ..., -0.1652, -0.0087,  0.1415],\n",
       "         [-0.0890, -0.1329, -0.3132,  ..., -0.1566, -0.0844,  0.2087],\n",
       "         ...,\n",
       "         [-0.1074, -0.0594, -0.4337,  ..., -0.2579, -0.0057,  0.2130],\n",
       "         [-0.0835, -0.0351, -0.4396,  ..., -0.2425, -0.0050,  0.1830],\n",
       "         [-0.0571, -0.0250, -0.4612,  ..., -0.2392, -0.0007,  0.1635]],\n",
       "\n",
       "        [[-0.0416, -0.1580, -0.2462,  ..., -0.0277,  0.0939,  0.2789],\n",
       "         [ 0.1005, -0.1812, -0.4332,  ..., -0.1473,  0.0541,  0.2413],\n",
       "         [ 0.0637, -0.1874, -0.4412,  ..., -0.3078,  0.1104,  0.2456],\n",
       "         ...,\n",
       "         [-0.0608, -0.0794, -0.4670,  ..., -0.3781,  0.0565,  0.2503],\n",
       "         [-0.0652, -0.0807, -0.4422,  ..., -0.3694,  0.0459,  0.2385],\n",
       "         [-0.0797, -0.0541, -0.3983,  ..., -0.3372,  0.0205,  0.2331]],\n",
       "\n",
       "        [[-0.2602,  0.0650, -0.5341,  ..., -0.3594,  0.0830,  0.1337],\n",
       "         [-0.1267, -0.0390, -0.3174,  ..., -0.4676,  0.1739,  0.1134],\n",
       "         [-0.1417, -0.0262, -0.3936,  ..., -0.3604,  0.1629,  0.0954],\n",
       "         ...,\n",
       "         [-0.1051,  0.0407, -0.4902,  ..., -0.3326,  0.1806,  0.0913],\n",
       "         [-0.1106,  0.0315, -0.4612,  ..., -0.3039,  0.1732,  0.0859],\n",
       "         [-0.1055,  0.0474, -0.4656,  ..., -0.2770,  0.1555,  0.0908]],\n",
       "\n",
       "        [[ 0.1644,  0.0453, -0.3316,  ..., -0.0517,  0.0921,  0.0996],\n",
       "         [-0.1559,  0.0259, -0.2517,  ..., -0.2796,  0.0733,  0.1332],\n",
       "         [-0.1312,  0.0728, -0.3691,  ..., -0.3316,  0.1192,  0.0372],\n",
       "         ...,\n",
       "         [-0.2221,  0.1095, -0.3608,  ..., -0.4196,  0.0898,  0.0938],\n",
       "         [-0.2193,  0.1197, -0.3695,  ..., -0.3904,  0.0834,  0.0965],\n",
       "         [-0.2099,  0.1375, -0.3811,  ..., -0.3624,  0.0949,  0.0933]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f602fe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Transformer Block ---\n",
      "Input to block: torch.Size([4, 10, 32])\n",
      "Output from block: torch.Size([4, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        b, s, d = x.shape\n",
    "\n",
    "        qkv = self.W_qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        q = q.view(b, s, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(b, s, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(b, s, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        context = (attn_weights @ v).transpose(1, 2).contiguous().view(b, s, d)\n",
    "        \n",
    "        output = self.W_o(context)\n",
    "        return output\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention sub-layer\n",
    "        attn_output = self.mha(x)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Feed-forward sub-layer\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "        return x\n",
    "\n",
    "print(\"\\n--- Transformer Block ---\")\n",
    "num_heads = 8\n",
    "d_ff = 64\n",
    "transformer_block = TransformerBlock(d_model, num_heads, d_ff)\n",
    "\n",
    "# Re-using the same dummy data 'x'\n",
    "print(f\"Input to block: {x.shape}\")\n",
    "output_block = transformer_block(x)\n",
    "print(f\"Output from block: {output_block.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "738179c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30810713",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_module = transformer_block.mha\n",
    "output_masked = mha_module(x, mask=causal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bbbbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_weights(mha, x, mask):\n",
    "    b, s, d = x.shape\n",
    "    qkv = mha.W_qkv(x)\n",
    "    q, k, v = qkv.chunk(3, dim=-1)\n",
    "    \n",
    "    q = q.view(b, s, mha.num_heads, mha.d_k).transpose(1, 2)\n",
    "    k = k.view(b, s, mha.num_heads, mha.d_k).transpose(1, 2)\n",
    "\n",
    "    scores = (q @ k.transpose(-2, -1)) / math.sqrt(mha.d_k)\n",
    "    \n",
    "    print(f\"\\nScores shape before masking: {scores.shape}\")\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, -1e9)\n",
    "        print(\"Scores after masking (showing first head, first item in batch):\")\n",
    "        print(scores[0, 0])\n",
    "        \n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    return attn_weights\n",
    "\n",
    "print(\"\\n--- Verifying Weights ---\")\n",
    "masked_weights = get_masked_weights(mha_module, x, causal_mask)\n",
    "\n",
    "print(f\"\\nFinal Attention Weights shape: {masked_weights.shape}\")\n",
    "print(\"Weights (first head, first item in batch):\")\n",
    "print(masked_weights[0, 0].detach().round(decimals=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a8ed591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n",
    "generate_square_subsequent_mask(4).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb6b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

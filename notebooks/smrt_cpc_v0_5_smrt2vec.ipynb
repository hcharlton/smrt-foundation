{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16QzIa_zmFq1"
   },
   "source": [
    "# Copy step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./shard_00028.npy' -> '/tmp/shard_00028.npy'\n",
      "'./shard_00026.npy' -> '/tmp/shard_00026.npy'\n",
      "'./shard_00021.npy' -> '/tmp/shard_00021.npy'\n",
      "'./shard_00013.npy' -> '/tmp/shard_00013.npy'\n",
      "'./shard_00014.npy' -> '/tmp/shard_00014.npy'\n",
      "'./shard_00005.npy' -> '/tmp/shard_00005.npy'\n",
      "'./shard_00002.npy' -> '/tmp/shard_00002.npy'\n",
      "'./shard_00030.npy' -> '/tmp/shard_00030.npy'\n",
      "'./shard_00015.npy' -> '/tmp/shard_00015.npy'\n",
      "'./shard_00027.npy' -> '/tmp/shard_00027.npy'\n",
      "'./shard_00020.npy' -> '/tmp/shard_00020.npy'\n",
      "'./shard_00012.npy' -> '/tmp/shard_00012.npy'\n",
      "'./shard_00029.npy' -> '/tmp/shard_00029.npy'\n",
      "'./shard_00031.npy' -> '/tmp/shard_00031.npy'\n",
      "'./shard_00003.npy' -> '/tmp/shard_00003.npy'\n",
      "'./shard_00004.npy' -> '/tmp/shard_00004.npy'\n",
      "'./shard_00007.npy' -> '/tmp/shard_00007.npy'\n",
      "'./shard_00000.npy' -> '/tmp/shard_00000.npy'\n",
      "'./shard_00009.npy' -> '/tmp/shard_00009.npy'\n",
      "'./shard_00018.npy' -> '/tmp/shard_00018.npy'\n",
      "'./shard_00011.npy' -> '/tmp/shard_00011.npy'\n",
      "'./shard_00016.npy' -> '/tmp/shard_00016.npy'\n",
      "'./shard_00024.npy' -> '/tmp/shard_00024.npy'\n",
      "'./shard_00023.npy' -> '/tmp/shard_00023.npy'\n",
      "'./shard_00001.npy' -> '/tmp/shard_00001.npy'\n",
      "'./shard_00008.npy' -> '/tmp/shard_00008.npy'\n",
      "'./shard_00006.npy' -> '/tmp/shard_00006.npy'\n",
      "'./shard_00025.npy' -> '/tmp/shard_00025.npy'\n",
      "'./shard_00022.npy' -> '/tmp/shard_00022.npy'\n",
      "'./shard_00017.npy' -> '/tmp/shard_00017.npy'\n",
      "'./shard_00010.npy' -> '/tmp/shard_00010.npy'\n",
      "'./shard_00019.npy' -> '/tmp/shard_00019.npy'\n",
      "\n",
      "real\t0m7.280s\n",
      "user\t0m0.133s\n",
      "sys\t0m13.519s\n"
     ]
    }
   ],
   "source": [
    "! cd ../data/01_processed/ssl_sets/ob007_test.memmap/ && time find  -type f -name '*.npy' | xargs -P16 -IX cp -v X $TMPDIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shard_00000.npy  shard_00008.npy  shard_00016.npy  shard_00024.npy\n",
      "shard_00001.npy  shard_00009.npy  shard_00017.npy  shard_00025.npy\n",
      "shard_00002.npy  shard_00010.npy  shard_00018.npy  shard_00026.npy\n",
      "shard_00003.npy  shard_00011.npy  shard_00019.npy  shard_00027.npy\n",
      "shard_00004.npy  shard_00012.npy  shard_00020.npy  shard_00028.npy\n",
      "shard_00005.npy  shard_00013.npy  shard_00021.npy  shard_00029.npy\n",
      "shard_00006.npy  shard_00014.npy  shard_00022.npy  shard_00030.npy\n",
      "shard_00007.npy  shard_00015.npy  shard_00023.npy  shard_00031.npy\n"
     ]
    }
   ],
   "source": [
    "! ls $TMPDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "tmpfs          1008G   16G  993G   2% /tmp\n"
     ]
    }
   ],
   "source": [
    "! df -h ${TMPDIR:-/tmp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vYGJgwVtMIrb",
    "outputId": "11bd2843-af9d-4957-c9e5-30f4a92d529d"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icfit_SKmJq-"
   },
   "source": [
    "# Glimpse ssl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUg_Q9DXk3vI",
    "outputId": "5134239b-d2bc-4594-c5f4-602d19db46d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384, 4096, 4])\n",
      "tensor([[ 2.0000, -0.0258,  0.7886,  0.0000],\n",
      "        [ 0.0000,  0.5142,  1.3975,  0.0000],\n",
      "        [ 0.0000, -1.3145,  0.3120,  0.0000],\n",
      "        [ 3.0000,  1.6426, -1.4004,  0.0000],\n",
      "        [ 0.0000, -0.5776, -0.5669,  0.0000],\n",
      "        [ 2.0000,  0.8359,  0.9258,  0.0000],\n",
      "        [ 1.0000, -0.7842, -0.0809,  0.0000],\n",
      "        [ 1.0000, -1.8984, -1.6172,  0.0000],\n",
      "        [ 1.0000, -0.8994,  0.1249,  0.0000],\n",
      "        [ 3.0000, -0.0928, -0.8608,  0.0000]], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# this is our pretraining dataset\n",
    "# take a peak at one of its numpy shards\n",
    "# note how each slice of the array is a single-stranded sample for pretraining\n",
    "# columns are organized [seq, ipd, pw, padding_mask]\n",
    "\n",
    "import numpy as np\n",
    "x = torch.tensor(np.load('../data/01_processed/ssl_sets/ob007_test.memmap/shard_00002.npy')).to(device)\n",
    "mask = ~x[...,-1].bool()\n",
    "print(x.shape)\n",
    "print(x[0,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCwGImgEhTQp",
    "outputId": "41b6ba8c-c10a-4863-c4b2-39f220a813b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.1904, -0.0100,  0.0000,  0.2000], device='cuda:0',\n",
       "        dtype=torch.float16),\n",
       " tensor([1.1904, 0.8999, 0.8901, 0.3999], device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first column is seq, center two are features which should have mean 0, sd 1\n",
    "# last column is mask and should be 0's and 1's (mostly 0's)\n",
    "\n",
    "x.mean(dim=(0, 1)).round(decimals=2), x.std(dim=(0,1)).round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "OLpocjf2h7Dy",
    "outputId": "ae90bb45-b07c-4efd-c1cf-c7c149a839fc"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Arrow error: Json error: Unsupported datatype for JSON serialization: Float16\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/IPython/core/formatters.py:1036\u001b[39m, in \u001b[36mMimeBundleFormatter.__call__\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m   1033\u001b[39m     method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/altair/vegalite/v6/api.py:3807\u001b[39m, in \u001b[36mTopLevelMixin._repr_mimebundle_\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m   3805\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m renderer := renderers.get():\n\u001b[32m-> \u001b[39m\u001b[32m3807\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdct\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/altair/utils/display.py:233\u001b[39m, in \u001b[36mHTMLRenderer.__call__\u001b[39m\u001b[34m(self, spec, **metadata)\u001b[39m\n\u001b[32m    231\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m.kwargs.copy()\n\u001b[32m    232\u001b[39m kwargs.update(**metadata, output_div=\u001b[38;5;28mself\u001b[39m.output_div)\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspec_to_mimebundle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhtml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/altair/utils/mimebundle.py:129\u001b[39m, in \u001b[36mspec_to_mimebundle\u001b[39m\u001b[34m(spec, format, mode, vega_version, vegaembed_version, vegalite_version, embed_options, engine, **kwargs)\u001b[39m\n\u001b[32m    127\u001b[39m internal_mode: Literal[\u001b[33m\"\u001b[39m\u001b[33mvega-lite\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvega\u001b[39m\u001b[33m\"\u001b[39m] = mode\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m using_vegafusion():\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     spec = \u001b[43mcompile_with_vegafusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m     internal_mode = \u001b[33m\"\u001b[39m\u001b[33mvega\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Default to the embed options set by alt.renderers.set_embed_options\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/altair/utils/_vegafusion_data.py:273\u001b[39m, in \u001b[36mcompile_with_vegafusion\u001b[39m\u001b[34m(vegalite_spec)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# Pre-evaluate transforms in vega spec with vegafusion\u001b[39;00m\n\u001b[32m    272\u001b[39m row_limit = data_transformers.options.get(\u001b[33m\"\u001b[39m\u001b[33mmax_rows\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m transformed_vega_spec, warnings = \u001b[43mvf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mruntime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_transform_spec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvega_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_local_tz\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43minline_datasets\u001b[49m\u001b[43m=\u001b[49m\u001b[43minline_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrow_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# Check from row limit warning and convert to MaxRowsError\u001b[39;00m\n\u001b[32m    281\u001b[39m handle_row_limit_exceeded(row_limit, warnings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/vegafusion/runtime.py:418\u001b[39m, in \u001b[36mVegaFusionRuntime.pre_transform_spec\u001b[39m\u001b[34m(self, spec, local_tz, default_input_tz, row_limit, preserve_interactivity, inline_datasets, keep_signals, keep_datasets)\u001b[39m\n\u001b[32m    413\u001b[39m local_tz = local_tz \u001b[38;5;129;01mor\u001b[39;00m get_local_tz()\n\u001b[32m    414\u001b[39m imported_inline_dataset = \u001b[38;5;28mself\u001b[39m._import_inline_datasets(\n\u001b[32m    415\u001b[39m     inline_datasets, get_inline_column_usage(spec)\n\u001b[32m    416\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m new_spec, warnings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mruntime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_transform_spec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_tz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_tz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_input_tz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_input_tz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrow_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreserve_interactivity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_interactivity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43minline_datasets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimported_inline_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_signals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeep_signals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_datasets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeep_datasets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_spec, warnings\n",
      "\u001b[31mValueError\u001b[39m: Arrow error: Json error: Unsupported datatype for JSON serialization: Float16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the distribution of nucleotides in the flattened seq column\n",
    "# note that in the natural genome it is not a uniform distribution\n",
    "# what we see here after subsetting with the mask matches expectations\n",
    "\n",
    "import altair as alt\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "import polars as pl\n",
    "seq = x[:, :, 0][mask].flatten()\n",
    "seq_df = pl.DataFrame({'seq':seq.to(cpu)})\n",
    "alt.Chart(seq_df).mark_bar(width=70).encode(\n",
    "    alt.X('seq:Q'),\n",
    "    y='count()'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "jZW27r0HkV8W",
    "outputId": "037df12f-8f14-4f3b-fe4d-a1196ae9b8f8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# and here we can see the proportions of each nucleotide in tabular format\u001b[39;00m\n\u001b[32m      2\u001b[39m seq = x[:, :, \u001b[32m0\u001b[39m][mask.to(cpu)].flatten()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m seq_df = \u001b[43mpl\u001b[49m.DataFrame({\u001b[33m'\u001b[39m\u001b[33mseq\u001b[39m\u001b[33m'\u001b[39m:seq.to(cpu)})\n\u001b[32m      4\u001b[39m seq_df[\u001b[33m\"\u001b[39m\u001b[33mseq\u001b[39m\u001b[33m\"\u001b[39m].value_counts(sort=\u001b[38;5;28;01mTrue\u001b[39;00m, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "# and here we can see the proportions of each nucleotide in tabular format\n",
    "seq = x[:, :, 0][mask.to(cpu)].flatten()\n",
    "seq_df = pl.DataFrame({'seq':seq.to(cpu)})\n",
    "seq_df[\"seq\"].value_counts(sort=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "JpWNfSFXwdko",
    "outputId": "e65a75e8-5c1e-472f-9bf8-1b017f47328b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# check that the long transformation doesn't corrupt the floats\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# if it did, we might see an excess of 0 (A)\u001b[39;00m\n\u001b[32m      3\u001b[39m seq = x[:, :, \u001b[32m0\u001b[39m].long()[mask.to(cpu)].flatten()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m seq_df = \u001b[43mpl\u001b[49m.DataFrame({\u001b[33m'\u001b[39m\u001b[33mseq\u001b[39m\u001b[33m'\u001b[39m:seq.to(cpu)})\n\u001b[32m      5\u001b[39m seq_df[\u001b[33m\"\u001b[39m\u001b[33mseq\u001b[39m\u001b[33m\"\u001b[39m].value_counts(sort=\u001b[38;5;28;01mTrue\u001b[39;00m, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "# check that the long transformation doesn't corrupt the floats\n",
    "# if it did, we might see an excess of 0 (A)\n",
    "seq = x[:, :, 0].long()[mask.to(cpu)].flatten()\n",
    "seq_df = pl.DataFrame({'seq':seq.to(cpu)})\n",
    "seq_df[\"seq\"].value_counts(sort=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "id": "kUWzekMmsIjW",
    "outputId": "6eb49ca2-40a4-4eaf-c1cc-aeb72e01fdb3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# plot the histograms of the two features\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m cont_df = \u001b[43mpl\u001b[49m.DataFrame({\u001b[33m'\u001b[39m\u001b[33mipd\u001b[39m\u001b[33m'\u001b[39m:x[...,\u001b[32m1\u001b[39m][mask].flatten().to(cpu),\n\u001b[32m      3\u001b[39m                         \u001b[33m'\u001b[39m\u001b[33mpw\u001b[39m\u001b[33m'\u001b[39m:x[...,\u001b[32m2\u001b[39m][mask].flatten().to(cpu)})\n\u001b[32m      4\u001b[39m alt.Chart(cont_df.unpivot()).mark_bar().encode(\n\u001b[32m      5\u001b[39m       alt.X(\u001b[33m'\u001b[39m\u001b[33mvalue:Q\u001b[39m\u001b[33m'\u001b[39m).title(\u001b[33m'\u001b[39m\u001b[33mnormalized zmw frames\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m      6\u001b[39m       alt.Y(\u001b[33m'\u001b[39m\u001b[33mcount():Q\u001b[39m\u001b[33m'\u001b[39m).scale(\u001b[38;5;28mtype\u001b[39m=\u001b[33m'\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m'\u001b[39m).title(\u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m       title=\u001b[33m\"\u001b[39m\u001b[33mMemmap Kinetics Distributions\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m   )\n",
      "\u001b[31mNameError\u001b[39m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# plot the histograms of the two features\n",
    "cont_df = pl.DataFrame({'ipd':x[...,1][mask].flatten().to(cpu),\n",
    "                        'pw':x[...,2][mask].flatten().to(cpu)})\n",
    "alt.Chart(cont_df.unpivot()).mark_bar().encode(\n",
    "      alt.X('value:Q').title('normalized zmw frames'),\n",
    "      alt.Y('count():Q').scale(type='linear').title('count'),\n",
    "  ).properties(\n",
    "      width=400,\n",
    "      height=400,\n",
    "  ).facet(\n",
    "      column='variable:N'\n",
    "  ).properties(\n",
    "      title=\"Memmap Kinetics Distributions\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBcyQeb_mOfS"
   },
   "source": [
    "# Glimpse downstream data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDAkB0hPm5N4"
   },
   "source": [
    "This is our current downstream dataset. Let's look at the first 10 samples. Note how each row is both a forward and reverse sample, and the features are not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "wI65JvaZTqS2",
    "outputId": "a24fa735-327f-41d8-fcab-d067891a10bb"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory (os error 2): pacbio_standard_train_1m.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_train = \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpacbio_standard_train_1m.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m df_train.head(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/io/parquet/functions.py:289\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(source, columns, n_rows, row_index_name, row_index_offset, parallel, use_statistics, hive_partitioning, glob, schema, hive_schema, try_parse_hive_dates, rechunk, low_memory, storage_options, credential_provider, retries, use_pyarrow, pyarrow_options, memory_map, include_file_paths, missing_columns, allow_missing_columns)\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    287\u001b[39m         lf = lf.select(columns)\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/lazyframe/opt_flags.py:324\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    323\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/lazyframe/frame.py:2429\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2427\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2428\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No such file or directory (os error 2): pacbio_standard_train_1m.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train = pl.read_parquet('pacbio_standard_train_1m.parquet')\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "-LyR85Nn2T7v",
    "outputId": "ed91d7bc-d9f6-4995-86aa-bf04427ccc6c"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory (os error 2): pacbio_standard_test_1m.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_val = \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpacbio_standard_test_1m.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m df_val.head(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/io/parquet/functions.py:289\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(source, columns, n_rows, row_index_name, row_index_offset, parallel, use_statistics, hive_partitioning, glob, schema, hive_schema, try_parse_hive_dates, rechunk, low_memory, storage_options, credential_provider, retries, use_pyarrow, pyarrow_options, memory_map, include_file_paths, missing_columns, allow_missing_columns)\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    287\u001b[39m         lf = lf.select(columns)\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/lazyframe/opt_flags.py:324\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    323\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/lazyframe/frame.py:2429\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2427\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2428\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No such file or directory (os error 2): pacbio_standard_test_1m.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n"
     ]
    }
   ],
   "source": [
    "df_val = pl.read_parquet('pacbio_standard_test_1m.parquet')\n",
    "df_val.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7y2DteU3soI"
   },
   "source": [
    "## The SSL Dataset Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMi5jNA54V08"
   },
   "source": [
    "Each \"shard\" is a 512 MB numpy array, and so with this truncated datset we have around 15 GB of data. Based on the test below, it looks like we can transfer that at a rate well over 1 GB/s to the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4-3Bear63rpT"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import glob\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class ShardedMemmapDataset(Dataset):\n",
    "    def __init__(self, data_dir, cache_size=100):\n",
    "        expanded_dir = os.path.expandvars(data_dir)\n",
    "        self.shard_paths = sorted(glob.glob(os.path.join(expanded_dir, \"*.npy\")))\n",
    "        first_shard = np.load(self.shard_paths[0], mmap_mode='r')\n",
    "        self.shard_size = first_shard.shape[0]\n",
    "        last_shard = np.load(self.shard_paths[-1], mmap_mode='r')\n",
    "        self.total_len = ((len(self.shard_paths) - 1) * self.shard_size) + last_shard.shape[0]\n",
    "        self.cache_size = cache_size\n",
    "        self.memmaps = OrderedDict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        shard_idx = idx // self.shard_size\n",
    "        local_idx = idx % self.shard_size\n",
    "        if shard_idx not in self.memmaps:\n",
    "            if len(self.memmaps) >= self.cache_size:\n",
    "                self.memmaps.popitem(last=False)\n",
    "            self.memmaps[shard_idx] = np.load(self.shard_paths[shard_idx], mmap_mode='r')\n",
    "        else:\n",
    "            self.memmaps.move_to_end(shard_idx)\n",
    "        return torch.from_numpy(np.array(self.memmaps[shard_idx][local_idx])).bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yMo2TJ8J3vIp"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# ssl_ds = ShardedMemmapDataset(\"ob007.memmap/\")\n",
    "# ssl_dl = DataLoader(ssl_ds, batch_size=256, num_workers=4, pin_memory=True, prefetch_factor=2, shuffle=True)\n",
    "\n",
    "# for batch in iter(tqdm(ssl_dl)):\n",
    "#   x = batch.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UiN1hteGGfR"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iqXxINcbGGKK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "SEQ_LEN = 4096\n",
    "BATCH_SIZE = 128\n",
    "D_MODEL = 128\n",
    "\n",
    "ssl_ds = ShardedMemmapDataset(\"../data/01_processed/ssl_sets/ob007.memmap\")\n",
    "# ssl_ds = ShardedMemmapDataset(\"/tmp\")\n",
    "ssl_dl = DataLoader(ssl_ds, batch_size=BATCH_SIZE, num_workers=8, pin_memory=True, prefetch_factor=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.7812,  1.2344,  0.0000],\n",
       "        [ 0.0000,  1.6250,  1.6875,  0.0000],\n",
       "        [ 0.0000, -0.6758,  0.3984,  0.0000],\n",
       "        [ 3.0000,  0.4688, -0.8594,  0.0000],\n",
       "        [ 0.0000,  1.1016, -1.0234,  0.0000],\n",
       "        [ 3.0000,  1.4688, -0.0811,  0.0000],\n",
       "        [ 3.0000,  0.9414, -1.3984,  0.0000],\n",
       "        [ 3.0000,  0.0996, -1.6172,  0.0000],\n",
       "        [ 1.0000,  0.0383,  0.3984,  0.0000],\n",
       "        [ 3.0000,  1.5781,  0.7891,  0.0000]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ssl_dl))[0,:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 81920/780929 [1:06:56<9:31:08, 20.40it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mssl_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:740\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    738\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    743\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    744\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    746\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:1505\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding <= \u001b[32m0\u001b[39m:\n\u001b[32m   1502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m   1503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInvalid iterator state: shutdown or no outstanding tasks when fetching next data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1504\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1505\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:1454\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1452\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1453\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1454\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1455\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1456\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:1295\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1282\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1283\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1292\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1293\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1294\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1298\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1299\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1300\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for batch in tqdm(ssl_dl):\n",
    "    x=batch.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpuXfCr9y9GA"
   },
   "source": [
    "# smrt2vec plan\n",
    "## Embed data: [B, T, C] -> [B, T, d_model] = E\n",
    "Since we have 1 categorical channel and 2 continuous channels we'll use a hybrid embedding. The nucleotide channel gets an embedding table, and the 2 continuous kinetics channels get a single linear projection with a GeLU nonlinearity. Note the continuous channels are normalized across the genome to have 0 mean, unit variance. GeLU is important for this since it allows negative values...\n",
    "\n",
    "## Extract features: [B, T, d_model], Pad -> [B, T', d_model], [B, T', 1] = Z, Pad'\n",
    "Separate out the padding channel. Runn a CNN over the sequence to generate a new sequence with features. Calculate the new padding mask based on the the CNN downsampling stride.\n",
    "\n",
    "## Mask random indices: [B, T', d_model], Pad' -> [B, T', d_model] = Z_masked, Mask_idx\n",
    "We mask the output of the CNN at randomly sampled indices (say 5 percent of them) and then replace a window (say 5 indices) starting at that index with the learnable padding vector (d_model) such that the sequence length remains the same as the output of the CNN.\n",
    "\n",
    "## Positional encoding: [B, T', d_model] -> [B, T', d_model]\n",
    "Only add the positional encoding at this point since the CNN and addition of masking vectors would overwrite its information otherwise\n",
    "\n",
    "## Transformer block: [B, T', d_model], Pad' -> [B, T', d_model]\n",
    "Run through a series of transformer blocks to get contextualized embeddings\n",
    "\n",
    "## Compute contrastive loss: [B, T', d_model], Mask_idx -> Loss\n",
    "Using the masked indices, use each C_t from the transformer output to predict the latent embedding. We will use an MLP for this transformation, and a separate one for the targets, and I suspect a smaller space than d_model will perform better (say 32 instead of 128). Score the prediction with infoNCE, so how much more similar is the predicted embedding vector to the true target at the position (which we retained) in comparison to a set of randomly sampled indices from the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "h2DpvvAbBTgP"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=4096):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, expansion=4):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(d_model, d_model * expansion)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(d_model * expansion, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class SmrtEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, n_nucleotides=5, n_continuous=2):\n",
    "    super().__init__()\n",
    "    self.nuc_embed = nn.Embedding(n_nucleotides, d_model//2)\n",
    "    self.kin_embed = nn.Linear(n_continuous, d_model//2, dtype=torch.bfloat16)\n",
    "    self.layernorm = nn.LayerNorm(d_model)\n",
    "    self.d_model = d_model\n",
    "  def forward(self, x_nuc, x_kin, is_padding):\n",
    "    scale = math.sqrt(self.d_model)\n",
    "    seq_emb = self.nuc_embed(x_nuc.int())*scale\n",
    "    kin_emb = self.kin_embed(x_kin)*scale\n",
    "    x = torch.concat((seq_emb,kin_emb),dim=-1)\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class BidirectionalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head=4, max_len=4096):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = d_model // n_head\n",
    "        # produces qkv, so we output 3*d_model\n",
    "        self.c_attn = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.c_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x, x_pad, pad_val=1):\n",
    "        B, T, C = x.size()\n",
    "        # use one big matmul and split\n",
    "        qkv = self.c_attn(x).view(B, T, 3, self.n_head, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # -> (3, B, n_head, T, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # -> 3 x (B, n_head, T, head_dim)\n",
    "\n",
    "        # F.scaled_dot_product_attention expects the padding mask s.t.:\n",
    "        # --- True: Attend, False: Ignore ---\n",
    "        # We are committing to the fact that our mask is True for padded\n",
    "        # sequences, so we need to invert it here\n",
    "        # Also, we want to broadcast across the head and query dims\n",
    "        # Given alignment right to left, we need to reshape to match B,H,T,T\n",
    "        attn_mask = ~x_pad.view(B, 1, 1, T)\n",
    "        output = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=attn_mask,\n",
    "            dropout_p=0.0 if not self.training else 0.05,\n",
    "            is_causal=False # since we attend to everything outside the att_mask\n",
    "        )\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.c_proj(output)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, max_len):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = BidirectionalSelfAttention(d_model, n_head, max_len)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = MLP(d_model)\n",
    "\n",
    "    def forward(self, x, x_pad): # includes unscaled residuals\n",
    "        x = x + self.attn(self.ln1(x), x_pad)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "    super(ResBlock, self).__init__()\n",
    "\n",
    "    self.padding = (kernel_size - 1) // 2\n",
    "    self.kernel_size = kernel_size\n",
    "\n",
    "    self.bn1 = nn.BatchNorm1d(in_channels)\n",
    "    self.conv1 = nn.Conv1d(in_channels=in_channels,\n",
    "                           out_channels=out_channels,\n",
    "                           kernel_size=kernel_size,\n",
    "                           stride=stride,\n",
    "                           padding=self.padding,\n",
    "                           bias=False)\n",
    "    self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "    self.conv2 = nn.Conv1d(in_channels=out_channels,\n",
    "                           out_channels=out_channels,\n",
    "                           kernel_size=kernel_size,\n",
    "                           stride=1,\n",
    "                           padding=self.padding,\n",
    "                           bias=False)\n",
    "\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "    self.stride = stride\n",
    "    # projection residual\n",
    "    if any([in_channels != out_channels, stride != 1]):\n",
    "      self.residual = nn.Sequential(\n",
    "          nn.Conv1d(in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=1, stride=stride,\n",
    "                    bias=False)\n",
    "          )\n",
    "    # identity residual\n",
    "    else:\n",
    "      self.residual = nn.Sequential()\n",
    "  def _resize_mask(self, mask, pad_val=1):\n",
    "    if mask.dtype == torch.bool:\n",
    "      mask = mask.float()\n",
    "    if pad_val == 0:\n",
    "      mask = F.max_pool1d(mask,\n",
    "                          kernel_size=self.kernel_size,\n",
    "                          stride=self.stride,\n",
    "                          padding=self.padding)\n",
    "    elif pad_val == 1:\n",
    "      mask = 1 - F.max_pool1d(1 - mask,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              stride=self.stride,\n",
    "                              padding=self.padding)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid pad value: Pad value must be 0 or 1\")\n",
    "    return mask.bool()\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    out = self.relu(self.bn1(x))\n",
    "    out = self.conv1(out)\n",
    "    out = self.relu(self.bn2(out))\n",
    "    out = self.conv2(out)\n",
    "    out += self.residual(x)\n",
    "    mask = self._resize_mask(mask)\n",
    "    return out, mask\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self, d_model, max_len, dropout_p):\n",
    "    super().__init__()\n",
    "    self.max_len = max_len\n",
    "    self.in_channels = d_model\n",
    "    # extractor\n",
    "    self.extractor = nn.ModuleList([\n",
    "          ResBlock(self.in_channels, self.in_channels, kernel_size=7),            # (B, C, T)   -> (B, C, T)\n",
    "\n",
    "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T)   -> (B, C, T)\n",
    "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T)   -> (B, C, T)\n",
    "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T)   -> (B, C, T)\n",
    "\n",
    "          ResBlock(self.in_channels, self.in_channels, kernel_size=3, stride=2),  # (B, C, T)   -> (B, C, T/2)\n",
    "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/2)\n",
    "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/2)\n",
    "\n",
    "          ResBlock(self.in_channels, self.in_channels, kernel_size=3, stride=1),  # (B, C, T/2) -> (B, C, T/4)\n",
    "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/4)\n",
    "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/4)\n",
    "          ResBlock(self.in_channels, self.in_channels, kernel_size=3),            # (B, C, T/2) -> (B, C, T/4)\n",
    "          ])\n",
    "    self.dropout = nn.Dropout(p=dropout_p)\n",
    "    # calculate fc layer input with dummy passthrough\n",
    "    self.output_shapes = self._get_output_shape()\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    for block in self.extractor:\n",
    "      x, mask= block(x,mask)\n",
    "    return x, mask\n",
    "\n",
    "  def _get_output_shape(self):\n",
    "      \"\"\"\n",
    "      Returns output shapes for the data and mask\n",
    "      \"\"\"\n",
    "      dummy_x = torch.randn(1, self.in_channels, self.max_len)\n",
    "      dummy_mask = torch.randn(1, self.max_len)\n",
    "\n",
    "      # get outputshapes\n",
    "      output, mask = self.forward(dummy_x, dummy_mask)\n",
    "      return output.shape, mask.shape\n",
    "\n",
    "\n",
    "class SMRT2Vec(nn.Module):\n",
    "  def __init__(self, d_model=128, n_layers=16, n_head=8, max_len=4096):\n",
    "      super().__init__()\n",
    "      self.d_model = d_model\n",
    "      self.mask_vec = nn.Parameter(torch.randn(d_model))\n",
    "      self.embed = SmrtEmbedding(d_model)\n",
    "      self.pe = PositionalEncoding(d_model)\n",
    "      self.downsample = CNN(d_model, max_len=max_len, dropout_p=0.01)\n",
    "      self.layer_norm_target = nn.LayerNorm(d_model)\n",
    "      self.blocks = nn.ModuleList([TransformerBlock(d_model=d_model, n_head=n_head, max_len=max_len) for _ in range(n_layers)])\n",
    "      self.project =  nn.Sequential(\n",
    "          nn.Linear(d_model, d_model),\n",
    "          nn.GELU(), # avoid negatives being ignored\n",
    "          nn.Linear(d_model, d_model)\n",
    "          )\n",
    "  def apply_mask(self, x_emb, pad, prob=0.05, size=6):\n",
    "    B, T, C = x_emb.shape\n",
    "    mask_idx_centers = (torch.rand(B, T, device=device) < prob) & ~(pad.bool())\n",
    "    mask_idx_full = F.max_pool1d(\n",
    "        mask_idx_centers.float(),\n",
    "        kernel_size=6, stride=1,\n",
    "        padding=size//2\n",
    "      ).bool()[:, :T] & (~pad.bool())\n",
    "    x_masked = x_emb.clone()\n",
    "    x_masked[mask_idx_full] = self.mask_vec.to(device) # torch.ones(self.d_model).to(device) # for debugging\n",
    "    return x_masked, mask_idx_full\n",
    "  def forward(self,x):\n",
    "    x_nuc = x[...,0]\n",
    "    x_kin = x[...,1:3]\n",
    "    x_pad = x[...,3]\n",
    "\n",
    "    x = self.embed(x_nuc, x_kin, x_pad)\n",
    "    # print(f\"shape after embedding {x.shape}\")\n",
    "    z, z_pad = self.downsample(x.permute(0,2,1), x_pad)\n",
    "    z = z.permute(0,2,1)\n",
    "    # bound the outputs of the downsample and capture before PE is added\n",
    "    targets = self.layer_norm_target(z.clone())\n",
    "    z = self.pe(z)\n",
    "    # get masked sequence and the mask tensor\n",
    "    c, z_mask = self.apply_mask(z, z_pad)\n",
    "    # apply transformer\n",
    "    for block in self.blocks:\n",
    "      c = block(c, z_pad)\n",
    "    # project predictions\n",
    "    c_proj = self.project(c)\n",
    "    return c_proj, targets.detach(), z_mask\n",
    "# infoc NCE loss with in-batch negatives\n",
    "class InfoNCELoss(nn.Module):\n",
    "  def __init__(self, temperature=0.1):\n",
    "    super().__init__()\n",
    "    self.cross_entropy = nn.CrossEntropyLoss()\n",
    "    self.temperature = temperature\n",
    "  def forward(self, c_proj, targets, mask_idx):\n",
    "    # gather the predictions and truth vectors\n",
    "    preds = c_proj[mask_idx]\n",
    "    truth = targets[mask_idx]\n",
    "    # normalize for cosine similarity\n",
    "    # last dim (embedding dim)\n",
    "    preds = F.normalize(preds, dim=-1)\n",
    "    truth = F.normalize(truth, dim=-1)\n",
    "    print(truth.shape,preds.shape)\n",
    "    logits = torch.mm(preds, truth.permute(1,0)) / self.temperature\n",
    "    labels = torch.arange(truth.shape[0], device=truth.device)\n",
    "    loss = self.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0I6h3OrIkvE",
    "outputId": "2c22521e-871b-4674-f927-e661fa374309"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 32, 2048]), torch.Size([1, 2048]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN(d_model=32, max_len=4096, dropout_p=0.01)\n",
    "cnn._get_output_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sbSyvAjcnwOu",
    "outputId": "a2b1910b-9592-42c2-824b-13f05b676472"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4096, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(ssl_dl)).to(device)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bhhVe6vIkfn",
    "outputId": "8e92e9f7-ccc0-4319-c942-c351c80deeb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([128, 2048, 128]), torch.Size([128, 2048, 128]), torch.Size([128, 2048]))\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = SMRT2Vec().to(device)\n",
    "c_proj, targets, mask = model(batch)\n",
    "print((c_proj.shape, targets.shape, mask.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CpflfHgtWAvT",
    "outputId": "747869bf-be98-48a2-a665-0e348a623c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60429, 128]) torch.Size([60429, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(11.2981, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = InfoNCELoss()\n",
    "loss(c_proj, targets, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSQE0CHcCv-1",
    "outputId": "2a90b4b7-057b-49c8-b175-707076b25dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4432768\n"
     ]
    }
   ],
   "source": [
    "model = SMRT2Vec().to(device)\n",
    "model.train()\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"trainable params: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVels_tK4aBn",
    "outputId": "6b501f00-1947-4ed0-c6bb-ccc5dea1fde7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60429, 128])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_proj[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v4Z_Bj9J_t-H",
    "outputId": "fc18a047-93eb-4edb-f2cd-ae74dccdb1d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2048, 128, 128]), torch.Size([2048, 128, 128]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_proj.transpose(0,1).shape, c_proj.permute(1,0,2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PamTn7KcQkU"
   },
   "source": [
    "# Downstream Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kBnYhgnk9yp"
   },
   "source": [
    "## Dataset\n",
    "Honestly this feels a bit funky, and I'm debating whether to make a new preprocessing script that produces numpy arrays like the SSL dataset. This parquet style dataset is inherited from the CNN a while back and is much more difficult to work with, I find. Also much more difficult to get good bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "PFMN3kbh6UE1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from torch.utils.data import IterableDataset\n",
    "def compute_log_normalization_stats(df, features, epsilon=1):\n",
    "    means = {col: (df[col].explode() + epsilon).log().mean() for col in features}\n",
    "    stds = {col: (df[col].explode() + epsilon).log().explode().std() for col in features}\n",
    "    return means, stds\n",
    "\n",
    "class MethylIterableDataset(IterableDataset):\n",
    "    def __init__(self, data_path, means, stds, context, restrict_row_groups=0, single_strand=False, inference=False):\n",
    "        super().__init__()\n",
    "        self.data_path = Path(data_path)\n",
    "        self.means, self.stds = means, stds\n",
    "        self.context = context\n",
    "        self.single_strand = single_strand\n",
    "        self.inference = inference\n",
    "        self.restrict = restrict_row_groups\n",
    "\n",
    "        self.kin_feats = ['fi', 'fp', 'ri', 'rp']\n",
    "        self.vocab = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4}\n",
    "        self.comp_map = torch.tensor([3, 2, 1, 0, 4], dtype=torch.long)\n",
    "\n",
    "        try:\n",
    "            meta = pq.read_metadata(self.data_path)\n",
    "            self.n_groups = meta.num_row_groups\n",
    "            use_groups = min(self.restrict, self.n_groups) if self.restrict else self.n_groups\n",
    "\n",
    "            # fast row count\n",
    "            n_rows = sum(meta.row_group(i).num_rows for i in range(use_groups))\n",
    "            self.len = n_rows * (2 if single_strand else 1)\n",
    "        except Exception:\n",
    "            print(f'Failed to read parquet: {self.data_path}')\n",
    "            self.n_groups, self.len = 0, 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def _process_batch(self, df):\n",
    "      # seq\n",
    "        seq_arr = np.stack(\n",
    "            df['seq'].str.split(\"\")\n",
    "            .list.eval(pl.element().replace_strict(self.vocab, default=4))\n",
    "            .to_numpy()\n",
    "        )\n",
    "        seq_t = torch.tensor(seq_arr, dtype=torch.long)\n",
    "\n",
    "        # kinetics\n",
    "        kin_list = []\n",
    "        for k in self.kin_feats:\n",
    "            vals = df[k].to_numpy() # (N, L)\n",
    "            vals = (np.log(vals + 1) - self.means[k]) / self.stds[k]\n",
    "            kin_list.append(vals)\n",
    "        kin_t = torch.tensor(np.stack(kin_list, axis=1), dtype=torch.bfloat16)\n",
    "\n",
    "        # mask, labels, etc (note that there is no masked data in the downstream set, so it's all zeros here)\n",
    "        mask = torch.zeros((seq_t.shape[0], seq_t.shape[1], 1), dtype=torch.bfloat16)\n",
    "        labels = torch.tensor(df['label'].to_numpy(), dtype=torch.long) if not self.inference else None\n",
    "        r_names, pos = df['read_name'].to_list(), df['cg_pos'].to_list()\n",
    "\n",
    "        # construct forward sample\n",
    "        # Seq (N, L, 1) + Kin (N, 2, L)->(N, L, 2) + Mask (N, L, 1) = (N, L, 4)\n",
    "        fwd_data = torch.cat([\n",
    "            seq_t.unsqueeze(-1).to(torch.bfloat16),\n",
    "            kin_t[:, 0:2].permute(0, 2, 1),\n",
    "            mask\n",
    "        ], dim=2)\n",
    "\n",
    "        # construct reverse data\n",
    "        rev_data = None\n",
    "        if self.single_strand:\n",
    "            rev_seq_t = torch.flip(self.comp_map.to(seq_t.device)[seq_t], dims=[1])\n",
    "            # Kin: slice 2:4, flip time (dim 2), permute channels\n",
    "            rev_kin = torch.flip(kin_t[:, 2:4], dims=[2]).permute(0, 2, 1)\n",
    "            rev_data = torch.cat([\n",
    "                rev_seq_t.unsqueeze(-1).to(torch.bfloat16),\n",
    "                rev_kin,\n",
    "                mask\n",
    "            ], dim=2)\n",
    "\n",
    "        # yield\n",
    "        for i in range(len(df)):\n",
    "            # forward\n",
    "            strand_name = 'fwd' if self.single_strand else 'ds'\n",
    "            item_fwd = {\n",
    "                'data': fwd_data[i],\n",
    "                'metadata': {'read_name': r_names[i], 'position': pos[i], 'strand': strand_name}\n",
    "            }\n",
    "            if labels is not None: item_fwd['label'] = labels[i]\n",
    "            yield item_fwd\n",
    "\n",
    "            # reverse\n",
    "            if rev_data is not None:\n",
    "                item_rev = {\n",
    "                    'data': rev_data[i],\n",
    "                    'metadata': {'read_name': r_names[i], 'position': pos[i], 'strand': 'rev'}\n",
    "                }\n",
    "                if labels is not None: item_rev['label'] = labels[i]\n",
    "                yield item_rev\n",
    "            else:\n",
    "              continue\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker = torch.utils.data.get_worker_info()\n",
    "        valid_groups = min(self.restrict, self.n_groups) if self.restrict else self.n_groups\n",
    "        indices = np.arange(valid_groups)\n",
    "\n",
    "        if worker:\n",
    "            indices = np.array_split(indices, worker.num_workers)[worker.id]\n",
    "\n",
    "        pqf = pq.ParquetFile(self.data_path)\n",
    "        for i in indices:\n",
    "            # array cast\n",
    "            df = pl.from_arrow(pqf.read_row_group(i)).with_columns([\n",
    "                pl.col(c).list.to_array(self.context) for c in self.kin_feats\n",
    "            ])\n",
    "            yield from self._process_batch(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "rTB-NjbWRrHN"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory (os error 2): pacbio_standard_train_1m.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m KINETICS_FEATURES = [\u001b[33m'\u001b[39m\u001b[33mfi\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mri\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrp\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpacbio_standard_train_1m.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m train_means, train_stds = compute_log_normalization_stats(df, KINETICS_FEATURES)\n\u001b[32m      6\u001b[39m it_workers=\u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/io/parquet/functions.py:289\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(source, columns, n_rows, row_index_name, row_index_offset, parallel, use_statistics, hive_partitioning, glob, schema, hive_schema, try_parse_hive_dates, rechunk, low_memory, storage_options, credential_provider, retries, use_pyarrow, pyarrow_options, memory_map, include_file_paths, missing_columns, allow_missing_columns)\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    287\u001b[39m         lf = lf.select(columns)\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/lazyframe/opt_flags.py:324\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    323\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/polars/lazyframe/frame.py:2429\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2427\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2428\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No such file or directory (os error 2): pacbio_standard_train_1m.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n"
     ]
    }
   ],
   "source": [
    "KINETICS_FEATURES = ['fi', 'fp', 'ri', 'rp']\n",
    "\n",
    "df = pl.read_parquet('pacbio_standard_train_1m.parquet')\n",
    "train_means, train_stds = compute_log_normalization_stats(df, KINETICS_FEATURES)\n",
    "\n",
    "it_workers=0\n",
    "batch_size=256\n",
    "single_strand=True\n",
    "#train\n",
    "methyl_train_ds = MethylIterableDataset('./pacbio_standard_train_1m.parquet',\n",
    "                                    means=train_means,\n",
    "                                    stds=train_stds,\n",
    "                                    context=32)\n",
    "methyl_train_dl = DataLoader(methyl_train_ds,\n",
    "                             batch_size=batch_size,\n",
    "                             drop_last=True,\n",
    "                             persistent_workers=False,\n",
    "                             prefetch_factor=None,\n",
    "                            )\n",
    "# val\n",
    "methyl_val_ds = MethylIterableDataset('./pacbio_standard_test_1m.parquet',\n",
    "                                    means=train_means,\n",
    "                                    stds=train_stds,\n",
    "                                    context=32)\n",
    "methyl_val_dl = DataLoader(methyl_val_ds,\n",
    "                        batch_size=batch_size,\n",
    "                        drop_last=True,\n",
    "                        persistent_workers=False,\n",
    "                        prefetch_factor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Oy3GboOX54S"
   },
   "source": [
    "## Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "iDCVHzsO-lra"
   },
   "outputs": [],
   "source": [
    "downstream_len =32\n",
    "class LastIdxProbe(nn.Module):\n",
    "  def __init__(self, trainable_encoder=True, d_model=256, n_head=4, n_layers=4, max_len=downstream_len):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.mask_vec = nn.Parameter(torch.randn(d_model))\n",
    "    self.embed = SmrtEmbedding(d_model)\n",
    "    self.pe = PositionalEncoding(d_model)\n",
    "    self.downsample = CNN(d_model, max_len=max_len, dropout_p=0.1)\n",
    "    self.layer_norm_target = nn.LayerNorm(d_model)\n",
    "    self.blocks = nn.ModuleList([TransformerBlock(d_model=d_model,\n",
    "                                                  n_head=n_head,\n",
    "                                                  max_len=max_len) for _ in range(n_layers)])\n",
    "    self.head = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model//2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(d_model//2, 1)\n",
    "        )\n",
    "    if not trainable_encoder:\n",
    "      for param in self.pe.parameters():\n",
    "              param.requires_grad = False\n",
    "      for param in self.blocks.parameters():\n",
    "              param.requires_grad = False\n",
    "      for param in self.embed.parameters():\n",
    "              param.requires_grad = False\n",
    "      for param in self.downsample.parameters():\n",
    "              param.requires_grad = False\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_nuc = x[...,0]\n",
    "    x_kin = x[...,1:3]\n",
    "    x_pad = x[...,3]\n",
    "\n",
    "    x = self.embed(x_nuc, x_kin, x_pad)\n",
    "\n",
    "    z, z_pad = self.downsample(x.permute(0,2,1), x_pad)\n",
    "    z = z.permute(0,2,1)\n",
    "    # bound the outputs of the downsample and capture before PE is added\n",
    "    targets = self.layer_norm_target(z.clone())\n",
    "    z = self.pe(z)\n",
    "    c = z\n",
    "    # apply transformer\n",
    "    for block in self.blocks:\n",
    "      c = block(c, z_pad)\n",
    "    logit = self.head(c[:, 8, :])\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "3s_hGQLlXDVK"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(methyl_train_dl))\n",
    "x = batch['data'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NScfmau_BbCA",
    "outputId": "794f53f9-2bf5-49a2-a5d1-41e864189611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8116737\n"
     ]
    }
   ],
   "source": [
    "model = LastIdxProbe(trainable_encoder=True).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"trainable params: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KIMof7FEVRYr",
    "outputId": "8ca44e80-6d6e-4399-c75a-591bcf5c0d31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7PFs-PB2UXeR",
    "outputId": "9adebe59-910b-457d-de41-e5d295d3902c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LastIdxProbe(\n",
      "  (embed): SmrtEmbedding(\n",
      "    (nuc_embed): Embedding(5, 128)\n",
      "    (kin_embed): Linear(in_features=2, out_features=128, bias=True)\n",
      "    (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (pe): PositionalEncoding()\n",
      "  (downsample): CNN(\n",
      "    (extractor): ModuleList(\n",
      "      (0): ResBlock(\n",
      "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (residual): Sequential()\n",
      "      )\n",
      "      (1-3): 3 x ResBlock(\n",
      "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (residual): Sequential()\n",
      "      )\n",
      "      (4): ResBlock(\n",
      "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
      "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (residual): Sequential(\n",
      "          (0): Conv1d(256, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (5-10): 6 x ResBlock(\n",
      "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (residual): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (layer_norm_target): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-3): 4 x TransformerBlock(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): BidirectionalSelfAttention(\n",
      "        (c_attn): Linear(in_features=256, out_features=768, bias=False)\n",
      "        (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "      )\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "SXjMyFZqBi90"
   },
   "outputs": [],
   "source": [
    "# # hyperparams\n",
    "# LR = 1e-4\n",
    "# EPOCHS = 20\n",
    "# DEVICE = torch.device('cuda')\n",
    "# model = model.to(DEVICE)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f\"trainable params: {total_params}\")\n",
    "# for epoch in range(EPOCHS):\n",
    "#     print(f\"Epoch {epoch+1}\")\n",
    "#     model.train()\n",
    "#     for batch in tqdm(methyl_train_dl):\n",
    "#         inputs = batch['data'].to(DEVICE)\n",
    "\n",
    "#         labels = batch['label'].to(DEVICE)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         logits = model(inputs)\n",
    "\n",
    "#         loss = criterion(logits, labels.unsqueeze(1).to(torch.float32)) # need to convert to float32 for loss\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     model.eval()\n",
    "#     sample_count = 0\n",
    "#     sample_correct = 0\n",
    "#     for batch in tqdm(methyl_val_dl):\n",
    "#         inputs = batch['data'].to(DEVICE)\n",
    "#         labels = batch['label'].to(DEVICE)\n",
    "\n",
    "#         logits = model(inputs)\n",
    "#         preds = logits > 0\n",
    "#         correct = labels == preds.squeeze(-1)\n",
    "#         sample_count += correct.shape[0]\n",
    "#         sample_correct += correct.sum()\n",
    "#     print(f\"epoch val top1_acc: {sample_correct/sample_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIhDYMCkeNUV",
    "outputId": "b60843d4-e6fb-4a2f-e422-b35f70706e03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8116737\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3906/3906 [01:42<00:00, 38.11it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3906/3906 [00:48<00:00, 81.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch val top1_acc: 0.7934758067131042\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3906/3906 [01:42<00:00, 38.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3906/3906 [00:47<00:00, 82.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch val top1_acc: 0.7981410622596741\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3906/3906 [01:43<00:00, 37.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3906/3906 [00:47<00:00, 82.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch val top1_acc: 0.7975660562515259\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 543/3906 [00:14<01:24, 40.00it/s]"
     ]
    }
   ],
   "source": [
    "LR = 3e-5\n",
    "EPOCHS = 6\n",
    "DEVICE = torch.device('cuda')\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "loss_history = []\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"trainable params: {total_params}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(tqdm(methyl_train_dl)):\n",
    "        inputs = batch['data'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, labels.unsqueeze(1).to(torch.float32))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            loss_history.append(running_loss / 100)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    sample_count = 0\n",
    "    sample_correct = 0\n",
    "    for batch in tqdm(methyl_val_dl):\n",
    "        inputs = batch['data'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        preds = logits > 0\n",
    "        correct = labels == preds.squeeze(-1)\n",
    "        sample_count += correct.shape[0]\n",
    "        sample_correct += correct.sum()\n",
    "    print(f\"epoch val top1_acc: {sample_correct/sample_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSUmVjcWeRU-"
   },
   "outputs": [],
   "source": [
    "df = pl.DataFrame({'loss': loss_history})\n",
    "def plot_loss(loss_df):\n",
    "  # loss_df_long = loss_df.unpivot(index='stepsx100', value_name='loss')\n",
    "  # min_loss = loss_df_long['loss'].min()\n",
    "  # max_loss = loss_df_long['loss'].max(\n",
    "  loss_df = loss_df.with_row_index()\n",
    "  loss_chart = alt.Chart(loss_df).mark_line().encode(\n",
    "    alt.X('index:Q'),\n",
    "    alt.Y('loss:Q'),\n",
    "  ).properties(\n",
    "    width=700,\n",
    "    height=500,\n",
    "    title = 'Direct Downstream Train Loss'\n",
    "  )\n",
    "  return loss_chart\n",
    "plot_loss(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbRMamHqgLwG"
   },
   "outputs": [],
   "source": [
    "4df.with_row_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33pXYUAPfxrm"
   },
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "LR = 1e-4\n",
    "EPOCHS = 2\n",
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "model = Classifier_Idx20(trainable_encoder=True, d_model=256, n_head=8, n_layers=8).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"trainable params: {total_params}\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    for batch in tqdm(methyl_train_dl):\n",
    "        inputs = batch['data'].to(DEVICE)\n",
    "\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(inputs)\n",
    "\n",
    "        loss = criterion(logits, labels.unsqueeze(1).to(torch.float32)) # need to convert to float32 for loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    sample_count = 0\n",
    "    sample_correct = 0\n",
    "    for batch in tqdm(methyl_val_dl):\n",
    "        inputs = batch['data'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        preds = logits > 0\n",
    "        correct = labels == preds.squeeze(-1)\n",
    "        sample_count += correct.shape[0]\n",
    "        sample_correct += correct.sum()\n",
    "    print(f\"epoch val top1_acc: {sample_correct/sample_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wlq0pLYEyibY"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(ssl_dl))\n",
    "batch.shape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

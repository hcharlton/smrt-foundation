{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78ca8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import polars as pl\n",
    "from torch.utils.data import Dataset\n",
    "import yaml\n",
    "\n",
    "class SMRTSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    loads a full context of SMRT data (seq, ipd, pw)\n",
    "    \"\"\"\n",
    "    def __init__(self, parquet_path: str, columns: list):\n",
    "        self.data_df = pl.read_parquet(parquet_path)\n",
    "        self.columns = columns\n",
    "        # always include seq\n",
    "        if 'seq' not in self.columns:\n",
    "            self.columns.insert(0, 'seq')\n",
    "        self.data_df = self.data_df.select(self.columns)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_data = self.data_df.row(idx, named=True)\n",
    "        \n",
    "        seq_ids = torch.tensor(row_data['seq'], dtype=torch.long)\n",
    "        \n",
    "        kinetics = []\n",
    "        for col in ['fi', 'fp', 'ri', 'rp']:\n",
    "            if col in row_data:\n",
    "                kinetics.append(torch.tensor(row_data[col], dtype=torch.float32))\n",
    "            \n",
    "        if kinetics:\n",
    "            # make into [L, 4] tensor\n",
    "            kinetics_tensor = torch.stack(kinetics, dim=1)\n",
    "            # TODO: normalize \n",
    "        else:\n",
    "            kinetics_tensor = torch.empty(len(seq_ids), 0, dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"seq_ids\": seq_ids,       # [L]\n",
    "            \"kinetics\": kinetics_tensor # nominal case -> [L, 4];  no kinetics -> [L, 0] \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0d6bb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d7dc90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7878, 10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "q = (\n",
    "    pl.scan_parquet('../data/01_processed/ssl_sets/da1.parquet')\n",
    ")\n",
    "df=q.collect()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6184201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13., 12.,  8., 14.],\n",
       "        [13., 16., 16., 24.],\n",
       "        [12.,  8., 11., 24.],\n",
       "        ...,\n",
       "        [10., 24.,  9., 11.],\n",
       "        [13.,  5., 15., 14.],\n",
       "        [17., 33., 16., 13.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = SMRTSequenceDataset('../data/01_processed/ssl_sets/da1.parquet', columns = ['seq', 'fi', 'fp', 'ri', 'rp'])\n",
    "ds[0]['kinetics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0dd02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SMRTEncoder(nn.Module):\n",
    "    \"\"\"Encodes (seq_id, kinetics) at each position into a latent vector z_t.\"\"\"\n",
    "    def __init__(self, vocab_size, n_kinetics, embed_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.seq_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # A small MLP to process the 4 kinetic features\n",
    "        self.kinetics_mlp = nn.Sequential(\n",
    "            nn.Linear(n_kinetics, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Project the combined embedding to the final latent dimension\n",
    "        self.projection = nn.Linear(embed_dim * 2, latent_dim)\n",
    "        self.layer_norm = nn.LayerNorm(latent_dim)\n",
    "\n",
    "    def forward(self, seq_ids, kinetics):\n",
    "        # seq_ids: [N, L]\n",
    "        # kinetics: [N, L, 4]\n",
    "        \n",
    "        seq_z = self.seq_embed(seq_ids)      # [N, L, embed_dim]\n",
    "        kinetics_z = self.kinetics_mlp(kinetics) # [N, L, embed_dim]\n",
    "        \n",
    "        # Combine by concatenation\n",
    "        combined_z = torch.cat([seq_z, kinetics_z], dim=-1) # [N, L, embed_dim * 2]\n",
    "        \n",
    "        # Project to latent space\n",
    "        z_t = self.projection(combined_z)    # [N, L, latent_dim]\n",
    "        z_t = self.layer_norm(z_t)\n",
    "        return z_t\n",
    "    \n",
    "\n",
    "class SMRTEncoder(nn.Module):\n",
    "    \"\"\"Encodes (seq_id, kinetics) at each position into a latent vector z_t.\"\"\"\n",
    "    def __init__(self, vocab_size, n_kinetics, embed_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.seq_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # A small MLP to process the 4 kinetic features\n",
    "        self.kinetics_mlp = nn.Sequential(\n",
    "            nn.Linear(n_kinetics, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Project the combined embedding to the final latent dimension\n",
    "        self.projection = nn.Linear(embed_dim * 2, latent_dim)\n",
    "        self.layer_norm = nn.LayerNorm(latent_dim)\n",
    "\n",
    "    def forward(self, seq_ids, kinetics):\n",
    "        # seq_ids: [N, L]\n",
    "        # kinetics: [N, L, 4]\n",
    "        \n",
    "        seq_z = self.seq_embed(seq_ids)      # [N, L, embed_dim]\n",
    "        kinetics_z = self.kinetics_mlp(kinetics) # [N, L, embed_dim]\n",
    "        \n",
    "        # Combine by concatenation\n",
    "        combined_z = torch.cat([seq_z, kinetics_z], dim=-1) # [N, L, embed_dim * 2]\n",
    "        \n",
    "        # Project to latent space\n",
    "        z_t = self.projection(combined_z)    # [N, L, latent_dim]\n",
    "        z_t = self.layer_norm(z_t)\n",
    "        return z_t\n",
    "    \n",
    "\n",
    "class CPCModel(nn.Module):\n",
    "    def __init__(self, encoder, autoregressive_model, predictor):\n",
    "        super().__init__()\n",
    "        self.g_enc = encoder\n",
    "        self.g_ar = autoregressive_model\n",
    "        \n",
    "        # A simple linear layer to predict future latents from context\n",
    "        # predictor projects context_dim -> latent_dim\n",
    "        self.W_k = predictor \n",
    "\n",
    "    def forward(self, seq_ids, kinetics):\n",
    "        # seq_ids: [N, L], kinetics: [N, L, 4]\n",
    "        N, L = seq_ids.shape\n",
    "        \n",
    "        # 1. Get all latents\n",
    "        z = self.g_enc(seq_ids, kinetics) # [N, L, D_z]\n",
    "        \n",
    "        # 2. Split into context and target\n",
    "        # Let's use a random split point for robustness\n",
    "        # e.g., split halfway\n",
    "        split_point = L // 2\n",
    "        \n",
    "        z_context = z[:, :split_point, :]\n",
    "        z_target = z[:, split_point:, :]\n",
    "        \n",
    "        # 3. Summarize context\n",
    "        # c_all shape: [N, L_ctx, D_c]\n",
    "        c_all, _ = self.g_ar(z_context)\n",
    "        \n",
    "        # Get the last context vector\n",
    "        c_summary = c_all[:, -1, :] # [N, D_c]\n",
    "        \n",
    "        # 4. Predict future latents\n",
    "        # This z_hat is the *prediction* for the future\n",
    "        z_hat = self.W_k(c_summary) # [N, D_z]\n",
    "        \n",
    "        # 5. Calculate InfoNCE Loss\n",
    "        \n",
    "        # We will compare our prediction z_hat[i] against all\n",
    "        # *actual* future latents z_target[j] in the batch.\n",
    "        \n",
    "        # For simplicity, let's just use the *first* target latent\n",
    "        # from each sequence as the \"positive\" target.\n",
    "        z_positive = z_target[:, 0, :] # [N, D_z]\n",
    "        \n",
    "        # This implementation uses in-batch negatives.\n",
    "        # We calculate an N x N score matrix.\n",
    "        # scores[i, j] = dot(z_hat[i], z_positive[j])\n",
    "        \n",
    "        # Normalize vectors for stable dot product (cosine similarity)\n",
    "        z_hat_norm = F.normalize(z_hat, p=2, dim=1)\n",
    "        z_positive_norm = F.normalize(z_positive, p=2, dim=1)\n",
    "        \n",
    "        scores = torch.matmul(z_hat_norm, z_positive_norm.T) # [N, N]\n",
    "        \n",
    "        # Temperature scaling\n",
    "        temperature = 0.1 \n",
    "        scores = scores / temperature\n",
    "        \n",
    "        # The positive pairs are on the diagonal (i, i).\n",
    "        # We want to maximize scores[i, i] and minimize scores[i, j].\n",
    "        # This is a standard cross-entropy loss problem.\n",
    "        \n",
    "        # Labels are simply [0, 1, 2, ..., N-1]\n",
    "        labels = torch.arange(N, device=seq_ids.device)\n",
    "        \n",
    "        loss = F.cross_entropy(scores, labels)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b12df20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = SMRTEncoder(4, n_kinetics=4, embed_dim=128, latent_dim=256)\n",
    "encoder(ds[0]['seq_ids'], ds[0]['kinetics']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b234ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory (os error 2): path/to/your.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Dataset\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Use the SMRT tags you extracted\u001b[39;00m\n\u001b[32m      6\u001b[39m data_cols = [\u001b[33m'\u001b[39m\u001b[33mseq\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfi\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mri\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrp\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m dataset = \u001b[43mSMRTSequenceDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath/to/your.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_cols\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m dataloader = DataLoader(dataset, batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 2. Model\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# (Define vocab_size, embed_dim, latent_dim, context_dim)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mSMRTSequenceDataset.__init__\u001b[39m\u001b[34m(self, parquet_path, columns)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, parquet_path: \u001b[38;5;28mstr\u001b[39m, columns: \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28mself\u001b[39m.data_df = \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mself\u001b[39m.columns = columns\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# always include seq\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch-mps2/lib/python3.11/site-packages/polars/_utils/deprecation.py:119\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    116\u001b[39m     _rename_keyword_argument(\n\u001b[32m    117\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    118\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch-mps2/lib/python3.11/site-packages/polars/_utils/deprecation.py:119\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    116\u001b[39m     _rename_keyword_argument(\n\u001b[32m    117\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    118\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch-mps2/lib/python3.11/site-packages/polars/io/parquet/functions.py:252\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(source, columns, n_rows, row_index_name, row_index_offset, parallel, use_statistics, hive_partitioning, glob, schema, hive_schema, try_parse_hive_dates, rechunk, low_memory, storage_options, credential_provider, retries, use_pyarrow, pyarrow_options, memory_map, include_file_paths, allow_missing_columns)\u001b[39m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    250\u001b[39m         lf = lf.select(columns)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch-mps2/lib/python3.11/site-packages/polars/_utils/deprecation.py:93\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch-mps2/lib/python3.11/site-packages/polars/lazyframe/frame.py:2206\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, _type_check, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, _check_order, _eager, **_kwargs)\u001b[39m\n\u001b[32m   2204\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2205\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No such file or directory (os error 2): path/to/your.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n"
     ]
    }
   ],
   "source": [
    "# Example setup\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Dataset\n",
    "# Use the SMRT tags you extracted\n",
    "data_cols = ['seq', 'fi', 'fp', 'ri', 'rp']\n",
    "dataset = SMRTSequenceDataset(\n",
    "    parquet_path=\"path/to/your.parquet\",\n",
    "    columns=data_cols\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 2. Model\n",
    "# (Define vocab_size, embed_dim, latent_dim, context_dim)\n",
    "encoder = SMRTEncoder(vocab_size, n_kinetics=4, embed_dim=128, latent_dim=256)\n",
    "gru = nn.GRU(input_size=256, hidden_size=512, batch_first=True)\n",
    "predictor = nn.Linear(512, 256) # D_c -> D_z\n",
    "\n",
    "model = CPCModel(encoder, gru, predictor)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 3. Training\n",
    "model.train()\n",
    "for batch in dataloader:\n",
    "    seq_ids = batch['seq_ids']\n",
    "    kinetics = batch['kinetics']\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model(seq_ids, kinetics)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358daec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "class CPCTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size: 5, \n",
    "                 embed_dim: 64, \n",
    "                 nhead: 4, \n",
    "                 num_layers: 2, \n",
    "                 mlp_hidden_dim: 128,\n",
    "                 k_max: 3, # number of steps to predict ahead\n",
    "                 max_seq_len: 2048,\n",
    "                 dropout: 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.k_max = k_max\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=mlp_hidden_dim, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # make a seperate projection head for each step of extrapolation\n",
    "        self.predictors = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(mlp_hidden_dim, embed_dim)\n",
    "            ) for _ in range(k_max)\n",
    "        ])\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def _generate_causal_mask(self, sz: int) -> Tensor:\n",
    "        mask = (torch.tril(torch.ones(sz, sz)) == 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        B, T = x.shape\n",
    "        \n",
    "        if T > self.max_seq_len:\n",
    "            raise ValueError(f\"Input sequence length ({T}) exceeds max_seq_len ({self.max_seq_len})\")\n",
    "            \n",
    "        causal_mask = self._generate_causal_mask(T).to(x.device)\n",
    "        \n",
    "        x_emb = self.token_embed(x) + self.pos_encoder[:, :T, :]\n",
    "        \n",
    "        z = self.transformer_encoder(x_emb, mask=causal_mask)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        num_losses = 0\n",
    "        \n",
    "        for k in range(1, self.k_max + 1):\n",
    "            t_max = T - k\n",
    "            if t_max <= 0:\n",
    "                continue\n",
    "                \n",
    "            c_t = z[:, :t_max, :]\n",
    "            z_k = z[:, k:, :][:, :t_max, :]\n",
    "            \n",
    "            z_hat = self.predictors[k-1](c_t)\n",
    "            \n",
    "            z_hat_flat = z_hat.reshape(-1, self.embed_dim)\n",
    "            z_k_flat = z_k.reshape(-1, self.embed_dim)\n",
    "            \n",
    "            scores = torch.matmul(z_hat_flat, z_k_flat.T)\n",
    "            \n",
    "            labels = torch.arange(z_hat_flat.size(0)).to(x.device)\n",
    "            \n",
    "            loss = self.loss_fn(scores, labels)\n",
    "            total_loss += loss\n",
    "            num_losses += 1\n",
    "            \n",
    "        return total_loss / num_losses if num_losses > 0 else torch.tensor(0.0).to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85a155a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 100])\n",
      "Calculated CPC Loss: 7.840989589691162\n",
      "Backward pass successful.\n",
      "Total trainable parameters: 149760\n"
     ]
    }
   ],
   "source": [
    "B, T = 16, 100\n",
    "V = 5 \n",
    "\n",
    "model = CPCTransformer(\n",
    "    vocab_size=V,\n",
    "    embed_dim=64,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    mlp_hidden_dim=128,\n",
    "    k_max=3,\n",
    "    max_seq_len=512\n",
    ")\n",
    "\n",
    "seq_data = torch.randint(0, V, (B, T))\n",
    "\n",
    "print(f\"Input shape: {seq_data.shape}\")\n",
    "\n",
    "loss = model(seq_data)\n",
    "\n",
    "print(f\"Calculated CPC Loss: {loss.item()}\")\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"Backward pass successful.\")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd4d73c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes: nuc=torch.Size([16, 100]), k1=torch.Size([16, 100]), k2=torch.Size([16, 100])\n",
      "Calculated CPC Loss: 8.542759895324707\n",
      "Backward pass successful.\n",
      "Total trainable parameters: 297600\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "class CPCTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size: int = 5, \n",
    "                 nuc_embed_dim: int = 48,\n",
    "                 kinetic_embed_dim: int = 8,\n",
    "                 embed_dim: int = 64, \n",
    "                 nhead: int = 4, \n",
    "                 num_layers: int = 2, \n",
    "                 mlp_hidden_dim: int = 128,\n",
    "                 k_max: int = 3, \n",
    "                 max_seq_len: int = 2048,\n",
    "                 dropout: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.k_max = k_max\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # 1. Separate embedding layers for each input channel\n",
    "        self.token_embed = nn.Embedding(vocab_size, nuc_embed_dim)\n",
    "        # We assume kinetic values are integers from 0-255\n",
    "        self.kinetic_embed_1 = nn.Embedding(256, kinetic_embed_dim)\n",
    "        self.kinetic_embed_2 = nn.Embedding(256, kinetic_embed_dim)\n",
    "\n",
    "        # 2. A projection layer to mix features and match model's embed_dim\n",
    "        total_input_dim = nuc_embed_dim + kinetic_embed_dim + kinetic_embed_dim\n",
    "        self.input_projector = nn.Linear(total_input_dim, embed_dim)\n",
    "\n",
    "        # 3. Positional encoder now adds to the *projected* embedding\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n",
    "        \n",
    "        # The rest of the model is unchanged, as it operates on embed_dim\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=mlp_hidden_dim, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.predictors = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(mlp_hidden_dim, embed_dim)\n",
    "            ) for _ in range(k_max)\n",
    "        ])\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def _generate_causal_mask(self, sz: int) -> Tensor:\n",
    "        # Using .tril for a cleaner implementation\n",
    "        mask = (torch.tril(torch.ones(sz, sz)) == 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: Tuple[Tensor, Tensor, Tensor]) -> Tensor:\n",
    "        x_nuc, x_k1, x_k2 = x\n",
    "        \n",
    "        B, T = x_nuc.shape\n",
    "        \n",
    "        if T > self.max_seq_len:\n",
    "            raise ValueError(f\"Input sequence length ({T}) exceeds max_seq_len ({self.max_seq_len})\")\n",
    "            \n",
    "        causal_mask = self._generate_causal_mask(T).to(x_nuc.device)\n",
    "        \n",
    "        # 1. Get separate embeddings\n",
    "        nuc_emb = self.token_embed(x_nuc)\n",
    "        k1_emb = self.kinetic_embed_1(x_k1)\n",
    "        k2_emb = self.kinetic_embed_2(x_k2)\n",
    "        \n",
    "        # 2. Concatenate and project\n",
    "        combined_emb = torch.cat([nuc_emb, k1_emb, k2_emb], dim=-1)\n",
    "        projected_emb = self.input_projector(combined_emb)\n",
    "        \n",
    "        # 3. Add positional encoding\n",
    "        x_emb = projected_emb + self.pos_encoder[:, :T, :]\n",
    "        \n",
    "        # The rest of the logic is identical\n",
    "        z = self.transformer_encoder(x_emb, mask=causal_mask)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        num_losses = 0\n",
    "        \n",
    "        for k in range(1, self.k_max + 1):\n",
    "            t_max = T - k\n",
    "            if t_max <= 0:\n",
    "                continue\n",
    "                \n",
    "            c_t = z[:, :t_max, :]\n",
    "            z_k = z[:, k:, :][:, :t_max, :]\n",
    "            \n",
    "            z_hat = self.predictors[k-1](c_t)\n",
    "            \n",
    "            z_hat_flat = z_hat.reshape(-1, self.embed_dim)\n",
    "            z_k_flat = z_k.reshape(-1, self.embed_dim)\n",
    "            \n",
    "            scores = torch.matmul(z_hat_flat, z_k_flat.T)\n",
    "            \n",
    "            labels = torch.arange(z_hat_flat.size(0)).to(x_nuc.device)\n",
    "            \n",
    "            loss = self.loss_fn(scores, labels)\n",
    "            total_loss += loss\n",
    "            num_losses += 1\n",
    "            \n",
    "        return total_loss / num_losses if num_losses > 0 else torch.tensor(0.0).to(x_nuc.device)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    B, T = 16, 100\n",
    "    V = 5 # A, C, G, T, N\n",
    "    \n",
    "    # Main model dimension\n",
    "    D_MODEL = 64\n",
    "    \n",
    "    model = CPCTransformer(\n",
    "        vocab_size=V,\n",
    "        nuc_embed_dim=128,       #128 dims for nucleotides\n",
    "        kinetic_embed_dim=64,    #64 dims for each kinetic channel\n",
    "        embed_dim=D_MODEL,      # Total projected dim is 64 (48 + 8 + 8)\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        mlp_hidden_dim=128,\n",
    "        k_max=3,\n",
    "        max_seq_len=2048\n",
    "    )\n",
    "    \n",
    "    # Create the 3 input tensors\n",
    "    seq_data = torch.randint(0, V, (B, T))\n",
    "    # Kinetic data (integers 0-255)\n",
    "    kinetic_1 = torch.randint(0, 256, (B, T))\n",
    "    kinetic_2 = torch.randint(0, 256, (B, T))\n",
    "\n",
    "    # Pass data as a tuple\n",
    "    model_input = (seq_data, kinetic_1, kinetic_2)\n",
    "    \n",
    "    print(f\"Input shapes: nuc={seq_data.shape}, k1={kinetic_1.shape}, k2={kinetic_2.shape}\")\n",
    "    \n",
    "    loss = model(model_input)\n",
    "    \n",
    "    print(f\"Calculated CPC Loss: {loss.item()}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    print(\"Backward pass successful.\")\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75727b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes: nuc=torch.Size([16, 100]), kinetics=torch.Size([16, 100, 4])\n",
      "Calculated CPC Loss: 8.252659797668457\n",
      "Backward pass successful.\n",
      "Total trainable parameters: 153072\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "class CPCTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size: int = 5, \n",
    "                 nuc_embed_dim: int = 48,\n",
    "                 kinetics_channels: int = 4,\n",
    "                 embed_dim: int = 64, \n",
    "                 nhead: int = 4, \n",
    "                 num_layers: int = 2, \n",
    "                 mlp_hidden_dim: int = 128,\n",
    "                 k_max: int = 3, \n",
    "                 max_seq_len: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.k_max = k_max\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # 1. Separate embedding layers for each input channel\n",
    "        self.token_embed = nn.Embedding(vocab_size, nuc_embed_dim)\n",
    "        # Kinetic data is now assumed to be a [B, T, 4] float tensor\n",
    "        # self.kinetic_embed_1 = nn.Embedding(256, kinetic_embed_dim)\n",
    "        # self.kinetic_embed_2 = nn.Embedding(256, kinetic_embed_dim)\n",
    "\n",
    "        # 2. A projection layer to mix features and match model's embed_dim\n",
    "        total_input_dim = nuc_embed_dim + kinetics_channels\n",
    "        self.input_projector = nn.Linear(total_input_dim, embed_dim)\n",
    "\n",
    "        # 3. Positional encoder now adds to the *projected* embedding\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n",
    "        \n",
    "        # The rest of the model is unchanged, as it operates on embed_dim\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=mlp_hidden_dim, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.predictors = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(mlp_hidden_dim, embed_dim)\n",
    "            ) for _ in range(k_max)\n",
    "        ])\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def _generate_causal_mask(self, sz: int) -> Tensor:\n",
    "        # Using .tril for a cleaner implementation\n",
    "        mask = (torch.tril(torch.ones(sz, sz)) == 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "        x_nuc, x_kinetics = x\n",
    "        \n",
    "        B, T = x_nuc.shape\n",
    "        \n",
    "        if T > self.max_seq_len:\n",
    "            raise ValueError(f\"Input sequence length ({T}) exceeds max_seq_len ({self.max_seq_len})\")\n",
    "            \n",
    "        causal_mask = self._generate_causal_mask(T).to(x_nuc.device)\n",
    "        \n",
    "        # 1. Get separate embeddings\n",
    "        nuc_emb = self.token_embed(x_nuc)\n",
    "        # k1_emb = self.kinetic_embed_1(x_k1)\n",
    "        # k2_emb = self.kinetic_embed_2(x_k2)\n",
    "        \n",
    "        # 2. Concatenate and project\n",
    "        # nuc_emb shape: [B, T, nuc_embed_dim]\n",
    "        # x_kinetics shape: [B, T, kinetics_channels]\n",
    "        combined_emb = torch.cat([nuc_emb, x_kinetics], dim=-1)\n",
    "        projected_emb = self.input_projector(combined_emb)\n",
    "        \n",
    "        # 3. Add positional encoding\n",
    "        x_emb = projected_emb + self.pos_encoder[:, :T, :]\n",
    "        \n",
    "        # The rest of the logic is identical\n",
    "        z = self.transformer_encoder(x_emb, mask=causal_mask)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        num_losses = 0\n",
    "        \n",
    "        for k in range(1, self.k_max + 1):\n",
    "            t_max = T - k\n",
    "            if t_max <= 0:\n",
    "                continue\n",
    "                \n",
    "            c_t = z[:, :t_max, :]\n",
    "            z_k = z[:, k:, :][:, :t_max, :]\n",
    "            \n",
    "            z_hat = self.predictors[k-1](c_t)\n",
    "            \n",
    "            z_hat_flat = z_hat.reshape(-1, self.embed_dim)\n",
    "            z_k_flat = z_k.reshape(-1, self.embed_dim)\n",
    "            \n",
    "            scores = torch.matmul(z_hat_flat, z_k_flat.T)\n",
    "            \n",
    "            labels = torch.arange(z_hat_flat.size(0)).to(x_nuc.device)\n",
    "            \n",
    "            loss = self.loss_fn(scores, labels)\n",
    "            total_loss += loss\n",
    "            num_losses += 1\n",
    "            \n",
    "        return total_loss / num_losses if num_losses > 0 else torch.tensor(0.0).to(x_nuc.device)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    B, T = 16, 100\n",
    "    V = 5 # A, C, G, T, N\n",
    "    \n",
    "    # Main model dimension\n",
    "    D_MODEL = 64\n",
    "    KINETICS_CHANNELS = 4\n",
    "    \n",
    "    model = CPCTransformer(\n",
    "        vocab_size=V,\n",
    "        nuc_embed_dim=48,       # 48 dims for nucleotides\n",
    "        kinetics_channels=KINETICS_CHANNELS,    # 4 dims for kinetics data\n",
    "        embed_dim=D_MODEL,      # Total projected dim is 64\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        mlp_hidden_dim=128,\n",
    "        k_max=3,\n",
    "        max_seq_len=512\n",
    "    )\n",
    "    \n",
    "    # Create the 2 input tensors\n",
    "    seq_data = torch.randint(0, V, (B, T))\n",
    "    # Kinetic data (floats, shape [B, T, 4])\n",
    "    kinetics_data = torch.randn(B, T, KINETICS_CHANNELS)\n",
    "\n",
    "    # Pass data as a tuple\n",
    "    model_input = (seq_data, kinetics_data)\n",
    "    \n",
    "    print(f\"Input shapes: nuc={seq_data.shape}, kinetics={kinetics_data.shape}\")\n",
    "    \n",
    "    loss = model(model_input)\n",
    "    \n",
    "    print(f\"Calculated CPC Loss: {loss.item()}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    print(\"Backward pass successful.\")\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57439bf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CPCTransformer.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mseq_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkinetics\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch-mps2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch-mps2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: CPCTransformer.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "model(ds[0]['seq_ids'], ds[0]['kinetics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "342ed693",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPCTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size: int = 5, \n",
    "                 nuc_embed_dim: int = 48,\n",
    "                 kinetics_channels: int = 4,\n",
    "                 embed_dim: int = 64, \n",
    "                 nhead: int = 4, \n",
    "                 num_layers: int = 2, \n",
    "                 mlp_hidden_dim: int = 128,\n",
    "                 k_max: int = 3, \n",
    "                 max_seq_len: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.k_max = k_max\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # 1. Separate embedding layers for each input channel\n",
    "        self.token_embed = nn.Embedding(vocab_size, nuc_embed_dim)\n",
    "        # Kinetic data is now assumed to be a [B, T, 4] float tensor\n",
    "        # self.kinetic_embed_1 = nn.Embedding(256, kinetic_embed_dim)\n",
    "        # self.kinetic_embed_2 = nn.Embedding(256, kinetic_embed_dim)\n",
    "\n",
    "        # 2. A projection layer to mix features and match model's embed_dim\n",
    "        total_input_dim = nuc_embed_dim + kinetics_channels\n",
    "        self.input_projector = nn.Linear(total_input_dim, embed_dim)\n",
    "\n",
    "        # 3. Positional encoder now adds to the *projected* embedding\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n",
    "        \n",
    "        # The rest of the model is unchanged, as it operates on embed_dim\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=mlp_hidden_dim, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.predictors = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(mlp_hidden_dim, embed_dim)\n",
    "            ) for _ in range(k_max)\n",
    "        ])\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def _generate_causal_mask(self, sz: int) -> Tensor:\n",
    "        # Using .tril for a cleaner implementation\n",
    "        mask = (torch.tril(torch.ones(sz, sz)) == 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "        x_nuc, x_kinetics = x\n",
    "        \n",
    "        B, T = x_nuc.shape\n",
    "        \n",
    "        if T > self.max_seq_len:\n",
    "            raise ValueError(f\"Input sequence length ({T}) exceeds max_seq_len ({self.max_seq_len})\")\n",
    "            \n",
    "        causal_mask = self._generate_causal_mask(T).to(x_nuc.device)\n",
    "        \n",
    "        # 1. Get separate embeddings\n",
    "        # --- FIX for MPS Error ---\n",
    "        # The nn.Embedding layer on MPS has a bug with long tensors.\n",
    "        # The fix is to move the input tensor to the CPU for the lookup,\n",
    "        # even if the embedding layer's weights are on the MPS device.\n",
    "        nuc_emb = self.token_embed(x_nuc.cpu())\n",
    "        # Move the result back to the original device (MPS)\n",
    "        nuc_emb = nuc_emb.to(x_nuc.device)\n",
    "        # --- End of FIX ---\n",
    "\n",
    "        # k1_emb = self.kinetic_embed_1(x_k1)\n",
    "        # k2_emb = self.kinetic_embed_2(x_k2)\n",
    "        \n",
    "        # 2. Concatenate and project\n",
    "        # nuc_emb shape: [B, T, nuc_embed_dim]\n",
    "        # x_kinetics shape: [B, T, kinetics_channels]\n",
    "        combined_emb = torch.cat([nuc_emb, x_kinetics], dim=-1)\n",
    "        projected_emb = self.input_projector(combined_emb)\n",
    "        \n",
    "        # 3. Add positional encoding\n",
    "        x_emb = projected_emb + self.pos_encoder[:, :T, :]\n",
    "        \n",
    "        # The rest of the logic is identical\n",
    "        z = self.transformer_encoder(x_emb, mask=causal_mask)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        num_losses = 0\n",
    "        \n",
    "        for k in range(1, self.k_max + 1):\n",
    "            t_max = T - k\n",
    "            if t_max <= 0:\n",
    "                continue\n",
    "                \n",
    "            c_t = z[:, :t_max, :]\n",
    "            z_k = z[:, k:, :][:, :t_max, :]\n",
    "            \n",
    "            z_hat = self.predictors[k-1](c_t)\n",
    "            \n",
    "            z_hat_flat = z_hat.reshape(-1, self.embed_dim)\n",
    "            z_k_flat = z_k.reshape(-1, self.embed_dim)\n",
    "            \n",
    "            scores = torch.matmul(z_hat_flat, z_k_flat.T)\n",
    "            \n",
    "            labels = torch.arange(z_hat_flat.size(0)).to(x_nuc.device)\n",
    "            \n",
    "            loss = self.loss_fn(scores, labels)\n",
    "            total_loss += loss\n",
    "            num_losses += 1\n",
    "            \n",
    "        return total_loss / num_losses if num_losses > 0 else torch.tensor(0.0).to(x_nuc.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19d35e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/493 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m seq = batch[\u001b[33m'\u001b[39m\u001b[33mseq_ids\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     21\u001b[39m kinetics = batch[\u001b[33m'\u001b[39m\u001b[33mkinetics\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkinetics\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ...\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch-mps2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch-mps2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mCPCTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     70\u001b[39m causal_mask = \u001b[38;5;28mself\u001b[39m._generate_causal_mask(T).to(x_nuc.device)\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# 1. Get separate embeddings\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# --- FIX for MPS Error ---\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# The nn.Embedding layer on MPS has a bug with long tensors.\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# The fix is to move the input tensor to the CPU for the lookup,\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# even if the embedding layer's weights are on the MPS device.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m nuc_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_nuc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Move the result back to the original device (MPS)\u001b[39;00m\n\u001b[32m     79\u001b[39m nuc_emb = nuc_emb.to(x_nuc.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch-mps2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch-mps2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch-mps2/lib/python3.11/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch-mps2/lib/python3.11/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = CPCTransformer(\n",
    "    vocab_size=V,\n",
    "    nuc_embed_dim=48,       # 48 dims for nucleotides\n",
    "    kinetics_channels=KINETICS_CHANNELS,    # 4 dims for kinetics data\n",
    "    embed_dim=D_MODEL,      # Total projected dim is 64\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    mlp_hidden_dim=128,\n",
    "    k_max=3,\n",
    "    max_seq_len=2048\n",
    ")\n",
    "model.to(device)\n",
    "dataset = SMRTSequenceDataset('../data/01_processed/ssl_sets/da1.parquet', columns = ['seq', 'fi', 'fp', 'ri', 'rp'])\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16)\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    seq = batch['seq_ids'].to(device)\n",
    "    kinetics = batch['kinetics'].to(device)\n",
    "    \n",
    "    loss = model((seq, kinetics))\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547ef1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "cd062738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import pysam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "96c0d898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 4],\n",
       "       [1, 2, 4]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.array([1,2,4])\n",
    "n = np.array([1,3,4])\n",
    "\n",
    "np.stack((n,m), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "75d89755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 0)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store = zarr.storage.MemoryStore()\n",
    "z = zarr.create_array(store=store, shape=(10, 0), chunks=(10, 100), dtype='int32')\n",
    "z.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "120e1df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.arange(200).reshape((10,20))\n",
    "z.append(m, axis = 1)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "970802c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2]], dtype=int32)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "z2 = zarr.create_array(zarr.storage.MemoryStore(), shape=(1,0), chunks=(1,10), dtype='int32')\n",
    "z2.append(np.array([1,2]).reshape(1,-1), axis=1)\n",
    "z2[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2979d80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19],\n",
       "       [ 20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,\n",
       "         33,  34,  35,  36,  37,  38,  39],\n",
       "       [ 40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "         53,  54,  55,  56,  57,  58,  59],\n",
       "       [ 60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,\n",
       "         73,  74,  75,  76,  77,  78,  79],\n",
       "       [ 80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
       "         93,  94,  95,  96,  97,  98,  99],\n",
       "       [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "        113, 114, 115, 116, 117, 118, 119],\n",
       "       [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132,\n",
       "        133, 134, 135, 136, 137, 138, 139],\n",
       "       [140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152,\n",
       "        153, 154, 155, 156, 157, 158, 159],\n",
       "       [160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172,\n",
       "        173, 174, 175, 176, 177, 178, 179],\n",
       "       [180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192,\n",
       "        193, 194, 195, 196, 197, 198, 199]], dtype=int32)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "daf1fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "da1 = zarr.open_group('../data/01_processed/ssl_sets/da1_subset_10k.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c55abd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  2, ...,  2,  1,  1],\n",
       "       [33, 36, 40, ..., 24, 46, 18],\n",
       "       [11, 15, 18, ...,  7, 30, 33],\n",
       "       [ 8, 43, 15, ..., 69,  9, 35],\n",
       "       [33, 33,  8, ..., 20, 14, 15],\n",
       "       [93, 44, 93, ..., 93, 93, 93]], shape=(6, 21159), dtype=uint8)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read=50\n",
    "da1['data'][:,int(da1['indptr'][read]):int(da1['indptr'][read+1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1e2740cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(20986, dtype=uint32)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da1['indptr'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "01610e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 171219240)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da1['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ea4536c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17121.924"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "171219240/10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "3578a0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "x = [1,2] + [2,3]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "b9250899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6666666666666665"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(300*(8_800_000/200_000))/(60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "75b6f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob007_no_shards = zarr.open_group('../data/01_processed/ssl_sets/ob007_20mchunk_noshard.zarr')\n",
    "ob007_with_shards = zarr.open_group('../data/01_processed/ssl_sets/ob007.zarr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "56ceccac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1000000)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ob007_no_shards['data'][:,21_000_000:22_000_000]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1705f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ob007_with_shards['data'][:,21_000_000:22_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "bc4a3bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.5"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[1]/8_0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e2fd6f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(8, 0), dtype=uint8)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,10_020_000:10_040_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b04f92cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  2,  1, ...,  3,  1,  1],\n",
       "       [20, 16,  6, ..., 11,  7,  6],\n",
       "       [20, 21, 17, ..., 26, 73, 30],\n",
       "       ...,\n",
       "       [86, 93, 93, ..., 93, 93, 93],\n",
       "       [14, 16, 16, ..., 18, 18, 16],\n",
       "       [ 1,  0,  0, ...,  0,  0,  1]], shape=(8, 12915), dtype=uint8)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read=54_000\n",
    "ob007['data'][:,int(ob007['indptr'][read]):int(ob007['indptr'][read+1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "1b4bf765",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_read = 50_000\n",
    "reads = 10_000\n",
    "x=ob007['data'][:,int(ob007['indptr'][start_read]):int(ob007['indptr'][start_read+reads])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a6b54296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  3, ...,  3,  3,  3],\n",
       "       [10,  9, 15, ..., 12,  4, 14],\n",
       "       [27, 18, 14, ..., 10, 11, 35],\n",
       "       ...,\n",
       "       [93, 93, 93, ..., 93, 93, 60],\n",
       "       [60, 59, 55, ...,  9,  9,  8],\n",
       "       [ 0,  1,  1, ...,  0,  0,  0]], shape=(8, 3999900), dtype=uint8)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,100:40_00_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a878fc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(8123.962086513995)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean read length\n",
    "np.diff(ob007['indptr']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c1960817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1596358550, 8)\n",
      "(20, 128)\n",
      "  > Total Read:  161.22 MB\n",
      "  > Time Taken:  11.5737 s\n",
      "  > Throughput:  13.93 MB/s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "def benchmark_indptr_read(path, n_batches=20, batch_size=128):\n",
    "    backend='zarr3'\n",
    "    \n",
    "    # 1. Open Dataset\n",
    "    try:\n",
    "        group = zarr.open_group(path, mode='r')\n",
    "        arr = group['data']\n",
    "        indptr = group['indptr']\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening zarr: {e}\")\n",
    "        return\n",
    "\n",
    "    print(arr.shape)\n",
    "    # Select random STARTING sequences\n",
    "    # e.g., if we need 32 sequences, we pick 32 random indices from indptr\n",
    "    batch_indices = np.random.randint(0, indptr.shape[0] - 2, size=(n_batches, batch_size))\n",
    "    print(batch_indices.shape)\n",
    "    total_bytes = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        # Current batch of sequence indices\n",
    "        seq_idxs = batch_indices[i]\n",
    "        \n",
    "        # In a real dataloader, this part happens in the worker process\n",
    "        for seq_id in seq_idxs:\n",
    "            # A. LOOKUP (In Memory - Fast)\n",
    "            start = indptr[seq_id]\n",
    "            end = indptr[seq_id+1]\n",
    "            \n",
    "            # B. READ (From Disk - Slow)\n",
    "            if backend == 'zarr':\n",
    "                _ = arr[start:end]\n",
    "            elif backend == 'tensorstore':\n",
    "                # .read().result() blocks until data is loaded\n",
    "                _ = arr[start:end].read().result()\n",
    "            \n",
    "            # Calculate bytes (approx based on uint8 * num_features)\n",
    "            # We assume num_features is the second dim\n",
    "            total_bytes += (end - start) * arr.shape[1]\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    mb_read = total_bytes / 1024 / 1024\n",
    "    throughput = mb_read / total_time\n",
    "    \n",
    "    print(f\"  > Total Read:  {mb_read:.2f} MB\")\n",
    "    print(f\"  > Time Taken:  {total_time:.4f} s\")\n",
    "    print(f\"  > Throughput:  {throughput:.2f} MB/s\")\n",
    "\n",
    "benchmark_indptr_read(\"../data/01_processed/ssl_sets/ob007_no_shards.zarr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "e2cb0ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1596358550, 8)\n",
      "(20, 32)\n",
      "[array([[1202984159,  398081438, 1171842333,  788178001,  414463086,\n",
      "         135410407, 1008686920, 1010988990, 1215364123, 1227164950,\n",
      "         121203324,  298271676, 1439648611,  206026887,  928608050,\n",
      "         573819350, 1285910634,  335592211, 1451380530,  807935321,\n",
      "        1414930295, 1369669774,    6151992,  491385923,  814962298,\n",
      "        1194421690, 1133937580,  277032647,  302042842, 1552853440,\n",
      "        1542584911, 1505262715],\n",
      "       [ 406029653, 1078488457,  559661529, 1156330518, 1410935788,\n",
      "         510142580,  925209567,   25590750,  615585806, 1176110408,\n",
      "         371872630,  155322109,  950831502, 1232607611, 1155510096,\n",
      "         460845056, 1187089153, 1131398660,  413728954,  367347514,\n",
      "        1366168868,  245380409,   26443233, 1183127668,  832761378,\n",
      "        1378538345, 1387954041,  733797327,  927347293, 1175696858,\n",
      "         878717226,  725546758],\n",
      "       [1550205909,   13937086,  358192037,   44445391, 1530634748,\n",
      "         782952424, 1053848216,  112760586, 1200624817,  793559392,\n",
      "        1280487900,  620005533,  139923258, 1367556462,  177687879,\n",
      "         930462481,  645877239,  890158866,  763530064,  134074162,\n",
      "         856547430, 1154783961,  159994146,  266598175,  341473398,\n",
      "         755723075,  375264900,    1464254, 1194195090,  438826166,\n",
      "        1423188564,  808726961],\n",
      "       [1530722078, 1396782043, 1157023791, 1146133976, 1560185428,\n",
      "         464505425, 1081684749, 1194750791, 1284581637, 1434340911,\n",
      "         595982811, 1052331776, 1223736279,  831844941, 1017179720,\n",
      "        1562785213, 1350371589, 1501426767,  939017901,  565925884,\n",
      "        1024015191,  618439877, 1044557263,  584828256,  364951437,\n",
      "         184547590,  325946324,  274309843,  226183943,   39559895,\n",
      "          13739927, 1449588683],\n",
      "       [ 295747243, 1204164300,  841434893,  205633299,  610881267,\n",
      "        1577371000, 1156495548,  352688169, 1383089991, 1108054586,\n",
      "         550990606,  581732812,  247423110,  191335380, 1134167655,\n",
      "         746738354, 1288654385,  671806617,  298790802,  340067706,\n",
      "        1135483518,  353061097,  874592814,  479286982,  735886845,\n",
      "         182893625, 1172183719, 1091718382,  598931478, 1249455111,\n",
      "         621617875,  160987654],\n",
      "       [ 348405474,  987946641, 1242818069, 1498236244,  299397133,\n",
      "         850059922,  495506918,  779915099, 1547290008,  842077654,\n",
      "          13370033,  923160814, 1216477474, 1040180161, 1165525286,\n",
      "         502729981,  737872312, 1589563047, 1180541651,  546043356,\n",
      "        1429655941,  591909404,   79785726,  276187193,    6015341,\n",
      "        1428521049, 1423777193,  913846394,  519764078, 1542662291,\n",
      "         674454061,  134484821],\n",
      "       [  51489529,  917276066, 1452884262,  192850296,   67150474,\n",
      "         506003383,   95697216,  297096061,  437979069, 1276357507,\n",
      "        1436563355, 1194889637,  165356172,   21951442, 1085056073,\n",
      "         733828569,  246860459, 1332440504,  623617088, 1546033885,\n",
      "         942876998, 1447531249,  961203633,  909982613,  129084667,\n",
      "         333778402,  749000148,  617127584, 1350554197, 1173334979,\n",
      "         459805127,  430723474],\n",
      "       [ 856879395,   81766331, 1447337286, 1186461376,  576870090,\n",
      "         221612081,   80871764,  489472870, 1180610092,  559797855,\n",
      "         636604827,  537518820,  309084039, 1398877758,  632740371,\n",
      "        1416910957,  334628966, 1393689473, 1391524029, 1527524920,\n",
      "         479487480,  628875110, 1523383959,  785145622,  484850938,\n",
      "         110835514,  115482633, 1406324912,  916387382,  758122291,\n",
      "        1394179464,  329051287],\n",
      "       [1302756208,  382742212,  515408698,  511657837, 1020051682,\n",
      "         582715407,  595982811,  927171766, 1354014132,  496610949,\n",
      "         867218273,  688867654,  390419576,   25457392, 1541859593,\n",
      "        1095484495,  122830166,  924057514, 1479861795,   27152160,\n",
      "         602689479,  952784592,   63092553,    2003315, 1075015789,\n",
      "          35644406,  608111330,  204375030,  796682319,   41727489,\n",
      "         972854339, 1095344530],\n",
      "       [1517187686,  577507277, 1080513915, 1072589529,  949975963,\n",
      "         743759912,  106567847,   11324429,  227760895, 1192952917,\n",
      "        1213297836, 1414662256,  604272484,  121620623,  881908432,\n",
      "         925104528,  744317781,  642381009,  634968877,  918188777,\n",
      "         814374275,  448684750,  925151569,  119247763, 1503910359,\n",
      "         205807098, 1365509312,  521333582, 1102882501,  252506540,\n",
      "         488690550, 1455678117],\n",
      "       [1209297291,  138014478,   54670319, 1109733999,  250972274,\n",
      "        1292547559, 1525922797,  648595677,  780333831, 1148452614,\n",
      "         403019178,  667608280, 1398345112,  533318029, 1111251315,\n",
      "         564281401,  792820845,  886897411, 1455078354, 1446049582,\n",
      "          66559771, 1260986791,   15377635, 1541254296,  202380046,\n",
      "         272866439,  778088848,  673451969, 1152494036,  137124317,\n",
      "         130040864,  270955394],\n",
      "       [ 296842901, 1554787699, 1127965868, 1265090604,  101912921,\n",
      "         598549210,  250013689, 1452589291, 1014677287, 1444483378,\n",
      "          89607475, 1080877491,  453352738,  521235254, 1423923341,\n",
      "         527962733,  411619156,  290252858, 1447863887,  898393362,\n",
      "            738139,  580855874, 1004265448, 1463462379,  542201835,\n",
      "        1572104619, 1346882269,  439654733, 1331126120,  151851102,\n",
      "        1575684346,  962881134],\n",
      "       [ 348890212, 1032755120, 1461084786,  579388306, 1518212626,\n",
      "        1202728139,  119296046, 1536325725, 1424147274, 1542207825,\n",
      "         788725562,  190599996, 1103113365,  731301897, 1165113865,\n",
      "         700967289, 1253836346, 1202283203, 1136548764,  914616776,\n",
      "          56604522,  334332718,  286274112, 1024633155,  321524477,\n",
      "        1041784174,  129331145,  354317919,  915923070,  217880569,\n",
      "        1081983991,  297593290],\n",
      "       [1200297771,  259705904, 1451793324,  890183013,  505784412,\n",
      "         431210179, 1395026202,  430554242,  874879825,   50926075,\n",
      "         474130358, 1154355040,  958860585,  781577768,  822610932,\n",
      "         981982043, 1275156535,  535305509, 1489410341, 1567925912,\n",
      "         447255256,  116511274,  796974212,  360128469, 1416448843,\n",
      "        1551847176,  723481550,  307081448, 1147115237, 1124134044,\n",
      "         667030551,  760659769],\n",
      "       [ 784919085, 1544069202,  798378157,  961695966, 1417371997,\n",
      "         879227817, 1419316318, 1139408841,  216041986,  293065004,\n",
      "        1486500886,   21104838, 1326454087,  341040189,  903502016,\n",
      "        1226644707, 1414817419,  670533527, 1295815252, 1219155383,\n",
      "         875477560, 1148503635,  311355344,   59331145,  570275307,\n",
      "         738933622,  357707143, 1422130438,  538268566,  516614248,\n",
      "        1038991943,  498697172],\n",
      "       [ 432840776,  716560529,  913706052,  565128829, 1311036529,\n",
      "        1057036396,  668446660,  388522476,  382589379, 1569299173,\n",
      "        1046314466,  771678540,   92959729,  430357492, 1264308573,\n",
      "         848211331,  289476492,  423794709, 1099419093, 1135167175,\n",
      "         817368837,  969661664,  503162609, 1074139994, 1365200590,\n",
      "         821747170,  418282032,    6060961,  871389761,  102882952,\n",
      "          32966142,  306136494],\n",
      "       [ 765850099, 1154767234,  994124265,  814763144,  963240956,\n",
      "         530942154,  161975290,  930828870, 1380872839,  565341633,\n",
      "        1376612827,  168004866,  772016122,  939792956,  190661723,\n",
      "        1114664590,  223436693,  424082196,  251938887, 1569451281,\n",
      "         235774194, 1260341696,  925209567,   62760952,  355696158,\n",
      "         682764586, 1249828966,  430641881,  311909384, 1321143531,\n",
      "        1325381874,  399392764],\n",
      "       [ 736978419,   72067802,  422749924,  371359448,  692429513,\n",
      "          26822842,  667671310,  717206198, 1473456308,  808795490,\n",
      "        1345580326, 1345338356,  261622879,  579993523,  847039476,\n",
      "        1217640345,  542595450,  150327925,   77133998,  384687913,\n",
      "         928335961,   42634719,  432432213, 1177356369,  319789387,\n",
      "         742931195, 1467835367, 1224327258,  688624483,  984813053,\n",
      "         646793728, 1021262925],\n",
      "       [1352303126, 1385914362,  317166262,  926973296,  239441862,\n",
      "         461135865, 1227742881, 1413326546,  778351976,  850271567,\n",
      "          13683916,  564677111,  743093973,  764046021, 1216994727,\n",
      "        1304602149, 1492406459,  374587034, 1191269213, 1439554298,\n",
      "        1530710379,  435902809,  768924891,  814264461,  254764545,\n",
      "        1581256670, 1361108220, 1567900607,  585303120, 1329027613,\n",
      "        1126275551,  712492276],\n",
      "       [1110038465,  923919327,  986338600,  810321997,  527667776,\n",
      "           3772950, 1567655512,  577767554, 1232622096, 1577597645,\n",
      "        1324548851,  464493673,  509774228, 1560059408,  127255427,\n",
      "          42214235, 1090559670, 1573402617,  127328116,  934089416,\n",
      "         622616459,  800877580, 1055683366, 1161757226,  861257166,\n",
      "        1476153478,   39850688, 1397142533, 1355158794,  355819026,\n",
      "         968853284, 1258228542]], dtype=uint32)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[265]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m total_bytes = \u001b[32m0\u001b[39m\n\u001b[32m     13\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m slices = [\u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/zarr/core/array.py:2806\u001b[39m, in \u001b[36mArray.__getitem__\u001b[39m\u001b[34m(self, selection)\u001b[39m\n\u001b[32m   2804\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vindex[cast(\u001b[33m\"\u001b[39m\u001b[33mCoordinateSelection | MaskSelection\u001b[39m\u001b[33m\"\u001b[39m, selection)]\n\u001b[32m   2805\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_pure_orthogonal_indexing(pure_selection, \u001b[38;5;28mself\u001b[39m.ndim):\n\u001b[32m-> \u001b[39m\u001b[32m2806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_orthogonal_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpure_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2807\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2808\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_basic_selection(cast(\u001b[33m\"\u001b[39m\u001b[33mBasicSelection\u001b[39m\u001b[33m\"\u001b[39m, pure_selection), fields=fields)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/zarr/core/array.py:3244\u001b[39m, in \u001b[36mArray.get_orthogonal_selection\u001b[39m\u001b[34m(self, selection, out, fields, prototype)\u001b[39m\n\u001b[32m   3242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prototype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3243\u001b[39m     prototype = default_buffer_prototype()\n\u001b[32m-> \u001b[39m\u001b[32m3244\u001b[39m indexer = \u001b[43mOrthogonalIndexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchunk_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3245\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sync(\n\u001b[32m   3246\u001b[39m     \u001b[38;5;28mself\u001b[39m._async_array._get_selection(\n\u001b[32m   3247\u001b[39m         indexer=indexer, out=out, fields=fields, prototype=prototype\n\u001b[32m   3248\u001b[39m     )\n\u001b[32m   3249\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/zarr/core/indexing.py:916\u001b[39m, in \u001b[36mOrthogonalIndexer.__init__\u001b[39m\u001b[34m(self, selection, shape, chunk_grid)\u001b[39m\n\u001b[32m    913\u001b[39m     dim_indexer = IntDimIndexer(dim_sel, dim_len, dim_chunk_len)\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dim_sel, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m     dim_indexer = \u001b[43mSliceDimIndexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_sel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_chunk_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_integer_array(dim_sel):\n\u001b[32m    919\u001b[39m     dim_indexer = IntArrayDimIndexer(dim_sel, dim_len, dim_chunk_len)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/zarr/core/indexing.py:406\u001b[39m, in \u001b[36mSliceDimIndexer.__init__\u001b[39m\u001b[34m(self, dim_sel, dim_len, dim_chunk_len)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dim_sel: \u001b[38;5;28mslice\u001b[39m, dim_len: \u001b[38;5;28mint\u001b[39m, dim_chunk_len: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# normalize\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     start, stop, step = \u001b[43mdim_sel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step < \u001b[32m1\u001b[39m:\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NegativeStepError(\u001b[33m\"\u001b[39m\u001b[33monly slices with step >= 1 are supported.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "n_batches = 20\n",
    "batch_size=32\n",
    "path = \"../data/01_processed/ssl_sets/ob007_no_shards.zarr\"\n",
    "group = zarr.open_group(path, mode='r')\n",
    "arr = group['data']\n",
    "indptr = group['indptr']\n",
    "print(arr.shape)\n",
    "\n",
    "batch_indices = np.random.randint(0, indptr.shape[0] - 2, size=(n_batches, batch_size))\n",
    "print(batch_indices.shape)\n",
    "print([indptr[batch_indices]])\n",
    "total_bytes = 0\n",
    "start_time = time.time()\n",
    "\n",
    "slices = [arr[indptr[i]:indptr[i+1]] for i in batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6dbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorstore as ts\n",
    "import numpy as np\n",
    "import asyncio\n",
    "\n",
    "arr_schema = {\n",
    "    'driver': 'zarr3',\n",
    "    'kvstore': {'driver': 'file'},  # Using memory for demo, normally 'file' or 's3'\n",
    "    'metadata': {'shape': [10**8], 'chunks': [10**6], 'dtype': '<i8'}\n",
    "}\n",
    "\n",
    "async def get_slices_parallel(indptr, indices):\n",
    "    # 1. Open the store asynchronously\n",
    "    dataset = await ts.open(arr_schema)\n",
    "\n",
    "    # 2. Calculate starts and stops\n",
    "    # indptrs are likely in memory, so this is fast\n",
    "    starts = indptr[indices]\n",
    "    stops = indptr[indices + 1]\n",
    "\n",
    "    # 3. Dispatch all reads immediately\n",
    "    # We create a list of futures. TensorStore executes these in parallel \n",
    "    # using its internal thread pool.\n",
    "    futures = []\n",
    "    for start, stop in zip(starts, stops):\n",
    "        # We ask for a numpy view of the result\n",
    "        futures.append(dataset[start:stop].read())\n",
    "\n",
    "    # 4. Wait for all I/O to complete\n",
    "    # 'gather' runs them concurrently\n",
    "    arrays = await asyncio.gather(*futures)\n",
    "\n",
    "    # 5. Concatenate final result\n",
    "    return np.concatenate(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c1aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100, 101, 203, 204, 500, 501])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([100, 203, 500])\n",
    "\n",
    "# Stack creates pairs [[100, 101], [203, 204], ...], ravel flattens them\n",
    "result = np.stack((arr, arr + 1), axis=-1).ravel()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f8a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.r_:        0.2898 seconds\n",
      "concatenate:  0.0492 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create a large array (100 million floats ~ 800MB)\n",
    "arr = np.arange(10**8, dtype=np.float64)\n",
    "\n",
    "# Method 1: np.r_ (The \"Slow\" Way)\n",
    "t0 = time.time()\n",
    "# Selects two 10-million element chunks\n",
    "indices = np.r_[0:10**7, 5*10**7:6*10**7] \n",
    "res_r = arr[indices]\n",
    "t1 = time.time()\n",
    "print(f\"np.r_:        {t1 - t0:.4f} seconds\")\n",
    "\n",
    "# Method 2: np.concatenate (The \"Fast\" Way)\n",
    "t0 = time.time()\n",
    "# Creates views first, no intermediate index array\n",
    "res_c = np.concatenate((arr[0:10**7], arr[5*10**7:6*10**7]))\n",
    "t1 = time.time()\n",
    "print(f\"concatenate:  {t1 - t0:.4f} seconds\")\n",
    "\n",
    "del arr, res_r, res_c, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ad8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import tensorstore as ts\n",
    "import numpy as np\n",
    "\n",
    "class TensorStoreDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, zarr_path):\n",
    "        # Open the store. This is lightweight (lazy).\n",
    "        # We specify the context to ensure threads are managed correctly.\n",
    "        self.store = ts.open({\n",
    "            'driver': 'zarr3',\n",
    "            'kvstore': {'driver': 'file', 'path': zarr_path},\n",
    "        }).result()\n",
    "        \n",
    "        self.indptr =self.store['indptr']\n",
    "\n",
    "    def __getitem__(self, indices):\n",
    "        # indices is a LIST of integers (provided by BatchSampler)\n",
    "        \n",
    "        # 1. Resolve starts/ends for the batch\n",
    "        # Assuming indptrs defines variable-length rows\n",
    "        starts = self.indptr[indices]\n",
    "        ends = self.indptr[indices + 1]\n",
    "        \n",
    "        # 2. Fire off async reads immediately\n",
    "        # TensorStore will coalesce reads and run them in its internal C++ thread pool.\n",
    "        futures = []\n",
    "        for start, end in zip(starts, ends):\n",
    "            # Request the slice. This returns a Future immediately.\n",
    "            futures.append(self.store['data'][start:end].read())\n",
    "\n",
    "        # 3. Block and gather results\n",
    "        # This releases the GIL for the heavy I/O and decompression work in C++\n",
    "        arrays = [f.result() for f in futures]\n",
    "        \n",
    "        # 4. Concatenate and return\n",
    "        # This happens in the worker process, so the main process just gets the final tensor\n",
    "        batch_data = np.concatenate(arrays)\n",
    "        return batch_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indptr) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5186f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.uint64(30895)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.diff(ob007['indptr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b49309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.uint32(3592)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(np.diff(da1['indptr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6967605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorstore as ts\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ZarrRaggedDataset(Dataset):\n",
    "    def __init__(self, group_path, data_key='data', indptr_key='indptr', context_spec=None):\n",
    "        ctx = ts.Context(context_spec) if context_spec else None\n",
    "        abs_path = os.path.abspath(group_path)\n",
    "\n",
    "        # 1. Setup indptr spec\n",
    "        spec_indptr = {\n",
    "            'driver': 'zarr', \n",
    "            'kvstore': {\n",
    "                'driver': 'file', \n",
    "                'path': os.path.join(abs_path, indptr_key)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # CORRECTED CHAIN:\n",
    "        # ts.open(...) returns a Future. \n",
    "        # .result() blocks to get the TensorStore object.\n",
    "        # .read() returns a Future.\n",
    "        # .result() blocks to get the numpy array.\n",
    "        self.indptr = ts.open(spec_indptr, context=ctx).result().read().result()\n",
    "        \n",
    "        # 2. Setup data spec\n",
    "        spec_data = {\n",
    "            'driver': 'zarr', \n",
    "            'kvstore': {\n",
    "                'driver': 'file', \n",
    "                'path': os.path.join(abs_path, data_key)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # We only resolve the store here, we don't read the data yet\n",
    "        self.data_store = ts.open(spec_data, context=ctx).result()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indptr) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = self.indptr[idx]\n",
    "        end = self.indptr[idx + 1]\n",
    "        \n",
    "        # Read the specific slice\n",
    "        val = self.data_store[start:end].read().result()\n",
    "        return torch.from_numpy(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414ffdbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ds = ZarrRaggedDataset(\u001b[43mpath\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "ds = ZarrRaggedDataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447d2cfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mds\u001b[49m[\u001b[32m100\u001b[39m,\u001b[32m101\u001b[39m].shape\n",
      "\u001b[31mNameError\u001b[39m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "ds[100,101].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e6781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(path, batch_size=32, num_workers=4):\n",
    "    # Optimize TensorStore context for the H100 environment\n",
    "    # 'cache_pool': controls memory usage for cached chunks\n",
    "    ts_context = {\n",
    "        'cache_pool': {'total_bytes_limit': 4_000_000_000}, # 4GB cache\n",
    "        'data_copy_concurrency': {'limit': num_workers * 2},\n",
    "    }\n",
    "    \n",
    "    dataset = ZarrRaggedDataset(path, context_spec=ts_context)\n",
    "    \n",
    "    # Custom collate required if sequences are variable length\n",
    "    def collate_ragged(batch):\n",
    "        # Example: Padding to longest in batch\n",
    "        return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_ragged,\n",
    "        pin_memory=True, # Essential for transfer to H100\n",
    "        persistent_workers=True # Keep TensorStore handles alive\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7addb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = get_dataloader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623cd665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chcharlton/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[203]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1444\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1441\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1446\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1263\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1278\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1279\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1280\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import tensorstore as ts\n",
    "# import numpy as np\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class GenomicZarrDataset(Dataset):\n",
    "#     def __init__(self, zarr_path, feature_shape=(8,)):\n",
    "#         self.zarr_path = zarr_path\n",
    "#         self.feature_shape = feature_shape\n",
    "        \n",
    "#         # We load the indptr in __init__ because it is safe (pure numpy) \n",
    "#         # and efficient (Copy-On-Write means workers share this RAM for free).\n",
    "#         spec_indptr = {\n",
    "#             'driver': 'zarr3',\n",
    "#             'kvstore': {'driver': 'file', 'path': f\"{zarr_path}/indptr\"}\n",
    "#         }\n",
    "#         self.indptr = ts.open(spec_indptr).result().read().result()\n",
    "#         self.num_reads = len(self.indptr) - 1\n",
    "        \n",
    "#         # DO NOT open self.data_store here. \n",
    "#         # We want each worker to open its own fresh connection.\n",
    "#         self.data_store = None\n",
    "\n",
    "#     def _init_worker_store(self):\n",
    "#         \"\"\"Helper to open store only when needed\"\"\"\n",
    "#         if self.data_store is None:\n",
    "#             spec_data = {\n",
    "#                 'driver': 'zarr3',\n",
    "#                 'kvstore': {\n",
    "#                     'driver': 'file',\n",
    "#                     'path': os.path.join(os.path.abspath(zarr_path), 'indptr')\n",
    "#                 }\n",
    "#             }\n",
    "#             # Open efficiently without reading yet\n",
    "#             self.data_store = ts.open(spec_data).result()\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_reads\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # 1. Ensure this specific worker has a connection\n",
    "#         self._init_worker_store()\n",
    "        \n",
    "#         # 2. Get indices from shared memory\n",
    "#         start = self.indptr[idx]\n",
    "#         end = self.indptr[idx+1]\n",
    "        \n",
    "#         # 3. Read (This blocks ONLY this worker)\n",
    "#         data_np = self.data_store[:, start:end].read().result()\n",
    "        \n",
    "#         # 4. Return\n",
    "#         return torch.from_numpy(data_np).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487fea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class GenomicZarrDataset(Dataset):\n",
    "    def __init__(self, zarr_path, feature_shape=(8,)):\n",
    "        # Ensure path is absolute to be safe\n",
    "        self.zarr_root = os.path.abspath(zarr_path) \n",
    "        self.feature_shape = feature_shape\n",
    "        \n",
    "        # Open indptr by pointing to root + internal path\n",
    "        spec_indptr = {\n",
    "            'driver': 'zarr3',\n",
    "            'kvstore': {\n",
    "                'driver': 'file',\n",
    "                'path': self.zarr_root  # Points to ob007.zarr\n",
    "            },\n",
    "            'path': 'indptr'  # Points to internal array 'indptr'\n",
    "        }\n",
    "        self.indptr = ts.open(spec_indptr).result().read().result()\n",
    "        self.num_reads = len(self.indptr) - 1\n",
    "        self.data_store = None\n",
    "\n",
    "    def _init_worker_store(self):\n",
    "        if self.data_store is None:\n",
    "            spec_data = {\n",
    "                'driver': 'zarr3',\n",
    "                'kvstore': {\n",
    "                    'driver': 'file',\n",
    "                    'path': self.zarr_root # Points to ob007.zarr\n",
    "                },\n",
    "                'path': 'data'  # Points to internal array 'data'\n",
    "            }\n",
    "            self.data_store = ts.open(spec_data).result()\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.num_reads\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._init_worker_store()\n",
    "        \n",
    "        # 2. Get indices from shared memory\n",
    "        start = self.indptr[idx]\n",
    "        end = self.indptr[idx+1]\n",
    "        \n",
    "        # 3. Read (This blocks ONLY this worker)\n",
    "        data_np = self.data_store[:, start:end].read().result()\n",
    "        \n",
    "        # 4. Return\n",
    "        return torch.from_numpy(data_np).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf6ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = GenomicZarrDataset('../data/01_processed/ssl_sets/ob007.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "996a4397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[90000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fba76b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_ragged(batch):\n",
    "    \"\"\"\n",
    "    Pads sequences to the longest in the batch.\n",
    "    \"\"\"\n",
    "    # batch is a list of Tensors [(L1, F), (L2, F), ...]\n",
    "    \n",
    "    # Get lengths for masking later if needed\n",
    "    lengths = torch.tensor([x.size(0) for x in batch], dtype=torch.long)\n",
    "    \n",
    "    # Pad sequences (batch_first=True -> [B, MaxLen, F])\n",
    "    # padding_value=0 is standard, adjust if your padding token is different\n",
    "    padded_batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=4)\n",
    "    \n",
    "    return padded_batch, lengths\n",
    "dl = DataLoader(\n",
    "        ds, \n",
    "        batch_size=32, \n",
    "        shuffle=False, # Sequential access as planned\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_ragged,\n",
    "        pin_memory=False, # Critical for H100 throughput\n",
    "        prefetch_factor=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82673ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6141 [00:45<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1444\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1441\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1446\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1263\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1278\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1279\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1280\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for batch in tqdm(dl):\n",
    "    x = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e012b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorstore as ts\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from itertools import islice\n",
    "\n",
    "class StreamGenomicDataset(IterableDataset):\n",
    "    def __init__(self, zarr_path, feature_shape=(8,), buffer_size_reads=10000):\n",
    "        self.zarr_root = os.path.abspath(zarr_path)\n",
    "        self.feature_shape = feature_shape\n",
    "        self.buffer_size_reads = buffer_size_reads # How many reads to fetch per I/O call\n",
    "\n",
    "        # Load indptr completely into RAM (Shared across workers via COW)\n",
    "        spec_indptr = {\n",
    "            'driver': 'zarr3',\n",
    "            'kvstore': {'driver': 'file', 'path': self.zarr_root},\n",
    "            'path': 'indptr'\n",
    "        }\n",
    "        self.indptr = ts.open(spec_indptr).result().read().result()\n",
    "        self.total_reads = len(self.indptr) - 1\n",
    "        \n",
    "    def _get_worker_info(self):\n",
    "        \"\"\"Calculates which range of reads this specific worker is responsible for.\"\"\"\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            # Single process loading\n",
    "            return 0, self.total_reads\n",
    "        else:\n",
    "            # Split total reads equally among workers\n",
    "            per_worker = int(np.ceil(self.total_reads / worker_info.num_workers))\n",
    "            worker_id = worker_info.id\n",
    "            start = worker_id * per_worker\n",
    "            end = min(start + per_worker, self.total_reads)\n",
    "            return start, end\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 1. Setup Worker Store\n",
    "        spec_data = {\n",
    "            'driver': 'zarr3',\n",
    "            'kvstore': {'driver': 'file', 'path': self.zarr_root},\n",
    "            'path': 'data'\n",
    "        }\n",
    "        data_store = ts.open(spec_data).result()\n",
    "        \n",
    "        # 2. Determine Range\n",
    "        start_read_idx, end_read_idx = self._get_worker_info()\n",
    "        \n",
    "        # 3. Iterate in huge blocks (Vectorized I/O)\n",
    "        current_idx = start_read_idx\n",
    "        \n",
    "        while current_idx < end_read_idx:\n",
    "            # Define the batch of READS we want to load\n",
    "            batch_end = min(current_idx + self.buffer_size_reads, end_read_idx)\n",
    "            \n",
    "            # Lookup the BYTE positions\n",
    "            # We grab the relevant slice of indptr to avoid repeated lookups\n",
    "            # shape: (buffer_size + 1,)\n",
    "            ptr_batch = self.indptr[current_idx : batch_end + 1]\n",
    "            \n",
    "            byte_start = ptr_batch[0]\n",
    "            byte_end = ptr_batch[-1]\n",
    "            \n",
    "            # --- THE OPTIMIZATION ---\n",
    "            # Single massive read/decompression from TensorStore\n",
    "            # This aligns well with your 20MB chunks\n",
    "            buffer_data = data_store[:, byte_start:byte_end].read().result()\n",
    "            \n",
    "            # Normalize pointers to be relative to the buffer (starts at 0)\n",
    "            local_ptr = ptr_batch - byte_start\n",
    "            \n",
    "            # Yield individual samples from the RAM buffer\n",
    "            for i in range(len(local_ptr) - 1):\n",
    "                p_start = local_ptr[i]\n",
    "                p_end = local_ptr[i+1]\n",
    "                \n",
    "                # Zero-copy view into the buffer\n",
    "                sample = buffer_data[:, p_start:p_end]\n",
    "                \n",
    "                # Transpose to (Seq_Len, Features)\n",
    "                yield torch.from_numpy(sample).T\n",
    "            \n",
    "            current_idx = batch_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280eb695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[304]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      8\u001b[39m dl = DataLoader(\n\u001b[32m      9\u001b[39m     ds, \n\u001b[32m     10\u001b[39m     batch_size=\u001b[32m32\u001b[39m,    \u001b[38;5;66;03m# PyTorch batches up the yielded items\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     pin_memory=\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Now safe to use\u001b[39;00m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Benchmark\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1444\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1441\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1446\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1263\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1278\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1279\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1280\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "ds = StreamGenomicDataset(\n",
    "    '../data/01_processed/ssl_sets/ob007.zarr', \n",
    "    buffer_size_reads=5000 # Adjust this to match roughly 1 or 2 chunks (20-40MB)\n",
    ")\n",
    "\n",
    "# Create Loader\n",
    "dl = DataLoader(\n",
    "    ds, \n",
    "    batch_size=32,    # PyTorch batches up the yielded items\n",
    "    num_workers=4,    # Each worker streams a different part of the file\n",
    "    collate_fn=collate_ragged,\n",
    "    pin_memory=False,  # Now safe to use\n",
    ")\n",
    "\n",
    "# Benchmark\n",
    "for batch in tqdm(dl):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35b43223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorstore as ts\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class VectorizedGenomicDataset(Dataset):\n",
    "    def __init__(self, zarr_path, block_size=1024):\n",
    "        \"\"\"\n",
    "        block_size: Number of reads to fetch in a single I/O call.\n",
    "                    Set this so that block_size * avg_read_len approx equals your Zarr Chunk size.\n",
    "                    E.g., 1000 reads * 15kb = 15MB.\n",
    "        \"\"\"\n",
    "        self.zarr_root = os.path.abspath(zarr_path)\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Load indptr\n",
    "        spec_indptr = {\n",
    "            'driver': 'zarr3',\n",
    "            'kvstore': {'driver': 'file', 'path': self.zarr_root},\n",
    "            'path': 'indptr'\n",
    "        }\n",
    "        self.indptr = ts.open(spec_indptr).result().read().result()\n",
    "        self.total_reads = len(self.indptr) - 1\n",
    "        \n",
    "        # Calculate how many \"blocks\" of data exist\n",
    "        self.num_blocks = int(np.ceil(self.total_reads / self.block_size))\n",
    "        self.data_store = None\n",
    "\n",
    "    def _init_worker_store(self):\n",
    "        if self.data_store is None:\n",
    "            spec_data = {\n",
    "                'driver': 'zarr3',\n",
    "                'kvstore': {'driver': 'file', 'path': self.zarr_root},\n",
    "                'path': 'data'\n",
    "            }\n",
    "            self.data_store = ts.open(spec_data).result()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_blocks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._init_worker_store()\n",
    "        \n",
    "        # 1. Calculate the range of READS for this block\n",
    "        read_start_idx = idx * self.block_size\n",
    "        read_end_idx = min((idx + 1) * self.block_size, self.total_reads)\n",
    "        \n",
    "        # 2. Get the range of BYTES from indptr\n",
    "        # We grab the slice of indptr corresponding to these reads\n",
    "        # ptrs shape: (n_reads_in_block + 1)\n",
    "        ptrs = self.indptr[read_start_idx : read_end_idx + 1]\n",
    "        \n",
    "        byte_start = ptrs[0]\n",
    "        byte_end = ptrs[-1]\n",
    "        \n",
    "        # 3. The \"Big Read\"\n",
    "        # One single C++ call to fetch ~15MB of data\n",
    "        # This matches your original Zarr chunking strategy perfectly\n",
    "        raw_block = self.data_store[:, byte_start:byte_end].read().result()\n",
    "        \n",
    "        # 4. Slice in RAM (CPU is fast here)\n",
    "        # Normalize pointers to be relative to the block\n",
    "        local_ptrs = ptrs - byte_start\n",
    "        \n",
    "        reads = []\n",
    "        for i in range(len(local_ptrs) - 1):\n",
    "            p0, p1 = local_ptrs[i], local_ptrs[i+1]\n",
    "            # Zero-copy slice\n",
    "            sample = raw_block[:, p0:p1]\n",
    "            # Transpose to (Seq, Feat) for PyTorch\n",
    "            reads.append(torch.from_numpy(sample).T)\n",
    "            \n",
    "        return reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80f28ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_flattened(batch_of_blocks):\n",
    "    \"\"\"\n",
    "    batch_of_blocks: List[List[Tensor]]\n",
    "    \n",
    "    Structure:\n",
    "    [\n",
    "      [Read_0, Read_1, ... Read_1023], # From Worker 1\n",
    "      [Read_1024, ..., Read_2047],     # From Worker 2\n",
    "      ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # 1. Flatten the list of lists into one giant list of reads\n",
    "    flat_reads = [read for block in batch_of_blocks for read in block]\n",
    "    \n",
    "    # 2. Pad and Stack\n",
    "    lengths = torch.tensor([x.size(0) for x in flat_reads], dtype=torch.long)\n",
    "    padded_batch = torch.nn.utils.rnn.pad_sequence(flat_reads, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded_batch, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b5e3a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]/home/chcharlton/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "  0%|          | 0/25 [00:49<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     10\u001b[39m dl = DataLoader(\n\u001b[32m     11\u001b[39m     ds, \n\u001b[32m     12\u001b[39m     batch_size=\u001b[32m4\u001b[39m,       \u001b[38;5;66;03m# Small number here, because each item is HUGE\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     prefetch_factor=\u001b[32m2\u001b[39m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Benchmark\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# x shape: [8000, Max_Len, Features]\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1444\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1441\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1446\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1263\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1278\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1279\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1280\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/smrt-foundation/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1. Use the original Zarr file (Large Chunks)\n",
    "ds = VectorizedGenomicDataset(\n",
    "    '../data/01_processed/ssl_sets/ob007.zarr', \n",
    "    block_size=2000 # Adjust this so block_size * avg_len ~= 20MB\n",
    ")\n",
    "\n",
    "# 2. DataLoader Configuration\n",
    "# effective_batch_size = batch_size * block_size\n",
    "# If batch_size=4 and block_size=2000, you get 8000 reads per step.\n",
    "dl = DataLoader(\n",
    "    ds, \n",
    "    batch_size=4,       # Small number here, because each item is HUGE\n",
    "    num_workers=4,      # 4 workers reading in parallel\n",
    "    collate_fn=collate_flattened,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2\n",
    ")\n",
    "\n",
    "# Benchmark\n",
    "for x, lengths in tqdm(dl):\n",
    "    # x shape: [8000, Max_Len, Features]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976c347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smrt-foundation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
